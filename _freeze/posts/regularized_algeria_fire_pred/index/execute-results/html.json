{
  "hash": "238bd5f044303ddd568dde7d08b4d370",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Prediction Algeria's Forest Fire Weather Index (FWI)\"\ndate: \"2024-09-10\"\ncategories: [Machine Learning, Logistic Regression, Regularization]\ncode-fold: true\ncode-copy: hover\ncode-summary: \"Show the code\"\nimage: image.jpg\ndraft: false\n---\n\n\n# Introduction\nThis project is a recycle of the [prediction of fire occurrence in Algeria's forest](https://blog.olamideadu.com/posts/algeria-fwi-prediction/) with a little twist. Instead of predicting fire occurrence, this project will predict the forest Fire Weather Index (FWI). Also different from the former project is the algorithm used. The former project used logistic regression with a principal component analysis preprocessing to reduce multicollinearity between the features. This project uses a regularized regression to predict the FWI of the forest. \n\n## Objective\nThe objective of this project involve:\n\n-   Developing a FWI model. \n-   Evaluating which of the regularized regression will preform best. (significant difference in performance is not the goal but rather getting the best performance).\n\n## Data\nData used for this project is exactly the same as the data used in the prediction of fire occurrence project, check [here](https://blog.olamideadu.com/posts/algeria-fwi-prediction/) for the data definition. To get more understanding of the data, and the correlation between the different variables, check the posts, as I will dive in to model development for this project.\n\nThe processed and clean data is already made available and will be imported. Prior that, we have to load the necessary library for this analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(nanoparquet, tidymodels, knitr, ggthemr)\nggthemr(palette = \"earth\", layout = \"minimal\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nalgeria_ff <- read_parquet(\"data/algeria.parquet\")\n```\n:::\n\n\n# Modelling\nWe will dive to model development straight away. We begin with sharing the data, creating resamples and setting the model specification before feature engineering and finally model development.\n## Data Sharing\nThe data will be splitted to a 70-30 proportion. 70% for training and 30% for testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nalgeria_split <-initial_split(algeria_ff, prop = .7, strata = fwi)\nalgeria_train <- training(algeria_split)\n```\n:::\n\n\nDue to the size of the data 243 rows, a bootstrap resampling technique will be employed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nalgeria_bstrap <- bootstraps(algeria_train, strata = fwi)\n```\n:::\n\n\n## Model Specification\n\nWe do not know the penalty for to use for our regularized regression, so we tune this parameter. The best value for the elastic-net is also unknown and will also be tuned.\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_spec <- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) |> \n  set_engine(\"glmnet\")\n\nridge_spec <- linear_reg(\n  penalty = tune(),\n  mixture = 0\n) |> \n  set_engine(\"glmnet\")\n\nelastic_spec <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) |> \n  set_engine(\"glmnet\")\n```\n:::\n\n\n## Feature engineering\nPreprocessing steps carried before except using pca will be employed here.\n\n::: {.cell}\n\n```{.r .cell-code}\nalgeria_rec <- recipe(fwi ~ ., data = algeria_train) |> \n  step_zv(all_numeric_predictors()) |> \n  step_nzv(all_numeric_predictors()) |> \n  step_YeoJohnson(all_numeric_predictors()) |> \n  step_scale(all_numeric_predictors()) |> \n  step_dummy(all_factor_predictors())\n\nalgeria_rec\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:    1\npredictor: 14\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Zero variance filter on: all_numeric_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Sparse, unbalanced variable filter on: all_numeric_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Yeo-Johnson transformation on: all_numeric_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Scaling for: all_numeric_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_factor_predictors()\n```\n\n\n:::\n:::\n\n\nThe data after undergoing feature engineering is shown in @tbl-preproc:\n\n::: {#tbl-preproc .cell tbl-cap='Data preview after preprocessing'}\n\n```{.r .cell-code}\nset.seed(123)\nalgeria_rec |> \n  prep() |> \n  juice() |> \n  car::some() |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|       day|    month| temperature|       rh|       ws|      rain|      ffmc|      dmc|       dc|      isi|      bui|  fwi| region_Sidi.Bel.Abbes| classes_fire|\n|---------:|--------:|-----------:|--------:|--------:|---------:|---------:|--------:|--------:|--------:|--------:|----:|---------------------:|------------:|\n| 1.9099765| 6.257817|    4.469877| 5.018083| 16.04237| 1.5146337| 0.6112248| 1.971387| 3.257687| 1.004836| 2.219341|  0.5|                     0|            0|\n| 3.5100906| 8.045765|    2.975905| 3.822778| 14.61728| 0.9683845| 0.9364749| 1.666566| 3.227303| 1.068358| 2.007790|  0.5|                     1|            0|\n| 1.7969550| 6.257817|    5.622383| 3.605656| 13.78430| 1.8495797| 1.3793852| 2.404867| 3.562018| 1.187334| 2.602155|  0.8|                     0|            0|\n| 3.0974426| 5.363843|    4.746365| 3.822778| 15.71108| 0.0000000| 2.6879614| 3.413360| 4.732499| 2.802260| 3.732665| 10.6|                     0|            1|\n| 0.7144441| 6.257817|    5.322644| 3.968818| 14.21285| 0.0000000| 2.5741665| 2.375816| 3.764466| 2.406109| 2.692231|  4.9|                     0|            1|\n| 1.0917983| 7.151791|    6.567699| 3.249173| 12.84074| 0.0000000| 2.7659263| 2.826989| 3.433657| 2.499136| 2.910182|  5.9|                     1|            1|\n| 3.6120029| 6.257817|    6.567699| 3.178716| 14.61728| 0.0000000| 3.0242252| 4.049493| 4.665000| 3.067498| 4.080263| 14.5|                     1|            1|\n| 1.7969550| 7.151791|    6.567699| 2.159044| 13.78430| 0.0000000| 3.4069902| 3.716532| 4.469620| 3.403472| 3.773189| 15.7|                     1|            1|\n| 2.1330093| 8.045765|    5.622383| 2.423705| 13.32829| 0.0000000| 3.5003297| 3.885782| 4.603991| 3.450648| 3.980076| 17.5|                     1|            1|\n| 2.5687041| 8.045765|    5.929822| 1.773226| 15.36427| 0.0000000| 3.4534197| 3.794553| 5.041329| 3.724684| 4.130098| 21.6|                     1|            1|\n\n\n:::\n:::\n\n\n## Tune Grid\nIn the models specified earlier we have one to two parameters we have to tune. These are the parameters with `tune()` in front of them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ntune_elastic <- extract_parameter_set_dials(elastic_spec) |> \n  grid_regular(\n    levels = 25\n  )\n\ntune_lasso <- extract_parameter_set_dials(lasso_spec) |> \n  grid_regular(levels =  20)\n\ntune_ridge <- extract_parameter_set_dials(ridge_spec) |> \n  grid_random(size = 20)\n```\n:::\n\n\nWe set control to save prediction and the workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_control <-control_grid(\n  save_pred = TRUE,\n  save_workflow = TRUE\n)\n```\n:::\n\n\n## Workflow {#sec-workflow}\nA workflow object for each model specification will be made\n\n\n::: {.cell}\n\n```{.r .cell-code}\nelastic_wf <- workflow() |> \n  add_recipe(algeria_rec) |> \n  add_model(elastic_spec)\n\nlasso_wf <- workflow() |> \n  add_recipe(algeria_rec) |> \n  add_model(lasso_spec)\n\nridge_wf <- workflow() |> \n  add_recipe(algeria_rec) |> \n  add_model(ridge_spec)\n```\n:::\n\n\nBelow is a breakdown of the process from model specification to feature engineering tied together.\n\n::: {.cell}\n\n```{.r .cell-code}\nelastic_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_zv()\n• step_nzv()\n• step_YeoJohnson()\n• step_scale()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\n## Tuning\nNow we tune the parameter(s) of each models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nelastic_tune <- tune_grid(\n  elastic_wf,\n  resamples = algeria_bstrap,\n  grid = tune_elastic,\n  control = grid_control\n)\n\nlasso_tune <- tune_grid(\n  lasso_wf,\n  resamples = algeria_bstrap,\n  grid = tune_lasso,\n  control = grid_control\n)\n\nridge_tune <- tune_grid(\n  ridge_wf,\n  resamples = algeria_bstrap,\n  grid = tune_ridge,\n  control = grid_control\n)\n```\n:::\n\n\n### Tune Performance\nLet's see the performance of the regularized parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nelastic_tune |> \n  collect_metrics() |>\n  mutate(\n    model = \"elastic\"\n  ) |> \n  bind_rows(\n    lasso_tune |> \n      collect_metrics() |> \n      mutate(\n        model = \"lasso\"\n      )\n  ) |> \n  bind_rows(\n    ridge_tune |> \n      collect_metrics() |> \n      mutate(\n        model = \"ridge\"\n      )\n  ) |> \n  ggplot(aes(penalty, mean, color = model)) +\n  geom_point() +\n  geom_smooth(\n    se = FALSE,\n    method = \"loess\",\n    formula = \"y ~ x\"\n  ) + \n  facet_wrap(~.metric, nrow = 2, scales = \"free\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Regularization Performance](index_files/figure-html/fig-performance-1.png){#fig-performance fig-align='center' width=672}\n:::\n:::\n\n\n### Final workflow {#set-fw}\nAs shown in @fig-performance, the elastic-net regularized model performed the best. We can pick the best model from this and get the final model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_tune <- elastic_tune |> \n  select_best(metric = \"rmse\")\n\nfinal_wf <- finalize_workflow(\n  x = elastic_wf,\n  parameters = best_tune\n)  \n\nfinal_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_zv()\n• step_nzv()\n• step_YeoJohnson()\n• step_scale()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.0562341325190349\n  mixture = 0.0895833333333333\n\nComputational engine: glmnet \n```\n\n\n:::\n:::\n\n\n## Feature Importance\n\n\nAbove @final-fw we parameters fitted accordingly and can note that the tune parameter in @sec-workflow has been replace accordingly. Before We finally fit the model to the whole of the data. we can investigate to see the most important variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'vip'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:utils':\n\n    vi\n```\n\n\n:::\n\n```{.r .cell-code}\nvip(final_wf |> \n      fit(algeria_train))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## Final Fit\n\n::: {.cell}\n\n```{.r .cell-code}\nlast_fit <- final_wf |> \n  last_fit(split = algeria_split)\n```\n:::\n\n\n@fig-elastic shows how the penalty, aka 𝜆 from 0 to infinity.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nadditional_colors <- c(\"#af4242\", \"#535364\", \"#FFC300\", \"#e09263\", \"#123367\")\nset_swatch(c(unique(swatch()), additional_colors))\n\n\nextract_fit_parsnip(last_fit) |> \n  autoplot() +\n  labs(\n    x = \"Lambda\",\n    y = \"Coefficients\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Coeﬃcients for our elastic-net regression model as 𝜆 grows from 0 → ∞](index_files/figure-html/fig-elastic-1.png){#fig-elastic fig-align='center' width=672}\n:::\n:::\n\n### Model Eval\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlast_fit |> \n  collect_metrics() |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|.config              |\n|:-------|:----------|---------:|:--------------------|\n|rmse    |standard   | 3.3764246|Preprocessor1_Model1 |\n|rsq     |standard   | 0.8563439|Preprocessor1_Model1 |\n\n\n:::\n:::\n\n\n\nWith a RMSE of 3.38 the model prediction of the forest fire weather index is reliable as the model can also explain about 86% of the whole data.\n\n## Conclusion\nThis project compared three regularized models, and used it as an alternative of creating a model without a pca preprocessing step, as regularized models penalize the coefficient of features pushing them towards zero or making them exactly zero (lasso regression). The model helped minimize the impact of multicollinearity existing within the data.\n\n\n## Reflection\nI tried using `workflow_map()` to tune the three models together combined in a `workflow_set()`. When metrics where collected to evaluate the models, the tuning parameters were absent and instead only results where returned. You can run the code below to confirm this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalgerian_wf_set <- workflow_set(\n  preproc = list(rec = algeria_rec),\n  models = list(\n    lasso = lasso_spec,\n    ridge = ridge_spec,\n    elastic = elastic_spec\n  ),\n  cross = TRUE\n ) |>\n  option_add(\n     id = \"rec_elastic\",\n     grid = tune_elastic\n  ) |> \n  option_add(\n    id = \"rec_lasso\",\n    grid = tune_lasso\n  ) |> \n  option_add(\n    id = \"rec_ridge\",\n    grid = tune_ridge\n  )\n\ntune_res <- workflow_map(\n  algerian_wf_set,\n  resamples = algeria_bstrap,\n  verbose = TRUE,\n  seed = 123,\n  fn = \"tune_grid\",\n  grid = 20,\n  control = grid_control\n)\n\ntune_res |> \n  collect_metrics()\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}