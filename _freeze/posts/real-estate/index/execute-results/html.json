{
  "hash": "45e79aba94b14c98c43479a0481b2bc1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Real Estate Prediction Using Boosting Tree (XGBoost)\"\ndate: \"2024-02-10\"\ncategories: [Machine Learning, XGBoost]\ncode-fold: true\ncode-copy: hover\ncode-summary: \"Show the code\"\nimage: image.jpg\ndraft: false\n---\n\n\n![](https://devtraco.com/wp-content/uploads/2022/10/Commercial-real-estate.jpg)\n\n\n\n\n\n# Introduction\n\nThe market historical data set of real estate valuation are collected from Xindian Dist., New Taipei City, Taiwan. This project aims to predict price of houses in Xindian, New Taipei given some characteristics of buildings.\n\n![Xindian, New Taipei City,Taiwan](https://newtaipei.travel/content/images/travelpurpose/39468/travelpurpose-image-yvttusgjueag-s79qftr3g.jpg)\n\n## The Data\n\nThis data is available in the public and is collected from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set), for more data to practice machine learning visit [UCirvine](https://archive.ics.uci.edu/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal_estate <- readxl::read_excel(\"Real estate valuation data set.xlsx\") |> \n  clean_names()\n\nhead(real_estate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 8\n     no x1_transaction_date x2_house_age x3_distance_to_the_nearest_mrt_station\n  <dbl>               <dbl>        <dbl>                                  <dbl>\n1     1               2013.         32                                     84.9\n2     2               2013.         19.5                                  307. \n3     3               2014.         13.3                                  562. \n4     4               2014.         13.3                                  562. \n5     5               2013.          5                                    391. \n6     6               2013.          7.1                                 2175. \n# ℹ 4 more variables: x4_number_of_convenience_stores <dbl>, x5_latitude <dbl>,\n#   x6_longitude <dbl>, y_house_price_of_unit_area <dbl>\n```\n\n\n:::\n:::\n\n\n## Data Definition\n\n| Variable Name              | Role                                | Type       | Description                                                                          | Units                                                      | Missing Values |\n|:---------------------------|-------------------------------------|------------|--------------------------------------------------------------------------------------|------------------------------------------------------------|---------------:|\n| No                         | ID                                  | Integer    |                                                                                      |                                                            |             no |\n| X1                         | transaction date                    | Feature    | Continuous                                                                           | for example, 2013.250=2013 March, 2013.500=2013 June, etc. |             no |\n| X2                         | house age                           | Feature    | Continuous                                                                           |                                                            |           year |\n| X3                         | distance to the nearest MRT station | Feature    | Continuous                                                                           |                                                            |          meter |\n| X4                         | number of convenience stores        | Feature    | Integer                                                                              | number of convenience stores in the living circle on foot  |        integer |\n| X5                         | latitude                            | Feature    | Continuous                                                                           | geographic coordinate, latitude                            |         degree |\n| X6                         | longitude                           | Feature    | Continuous                                                                           | geographic coordinate, longitude                           |         degree |\n| Y house price of unit area | Target                              | Continuous | 10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared | 10000 New Taiwan Dollar/Ping                               |             no |\n\n## Data Preparation\n\nFirst, we will split the date from the Taiwan system to year and month.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal_estate <- real_estate |> \n  mutate(\n    year = x1_transaction_date %/% 1,\n    month = round((x1_transaction_date %% 1) * 12), # to get month from taiwanese date\n    .before = x2_house_age\n  )\n\n\nreal_estate <- real_estate |> \n  mutate(month = case_when(month == 0 ~ 1, TRUE ~ month)) |> \n  select(!c(1, 2))\n```\n:::\n\n\nThe names of the variables are a bit long and unclear so we will rename them to make coding easy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal_estate <- real_estate |> \n  rename(\n    age = x2_house_age,\n    distance_to_station = x3_distance_to_the_nearest_mrt_station,\n    number_convenience_stores = x4_number_of_convenience_stores,\n    latitude = x5_latitude,\n    longitude = x6_longitude,\n    price = y_house_price_of_unit_area\n  )\n\nreal_estate <- real_estate |> \n  mutate(\n    age = ceiling(age),\n    sale_date = make_date(year = as.integer(year), month = month),\n    .before = age\n  ) |> \n  select(-c(year, month))\n\nnames(real_estate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"sale_date\"                 \"age\"                      \n[3] \"distance_to_station\"       \"number_convenience_stores\"\n[5] \"latitude\"                  \"longitude\"                \n[7] \"price\"                    \n```\n\n\n:::\n:::\n\n\nTo get a better grasp of the pricing, the US Dollar will be used, and the size of the houses in square meter will be calculated to give an idea of how big the properties are\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal_estate <- real_estate |> \n  mutate(\n    size_m2 = (price * 10000) / 3.9,\n    price_usd = (price * 10000) * 0.032,\n    .before = price\n  )\n```\n:::\n\n\n## Investigating missing values\n\nEven if the data is having no missing value when imported, it's not a bad idea to look for missing data after the preparation which we have made.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(real_estate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nWe can also check for duplicate data point\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(duplicated(real_estate))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nThere are no duplicate data point. We can proceed with our analysis after this.\n\n# Exploratory Data Analysis\n\n## Target Variable\n\n### Univariate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprice_median <- \n  tibble(\n    med = median(real_estate$price_usd),\n    label = paste0(\"$\", med)\n  )\n\nggplot(real_estate, aes(price_usd)) +\n  geom_histogram(binwidth = 500, alpha =0.7, fill = \"wheat3\") +\n  geom_density(stat = \"bin\", binwidth = 500, col = \"brown\") +\n  geom_vline(aes(xintercept = median(price_usd)), col = \"violetred3\") +\n  geom_text(\n    data = price_median,\n    aes(x = med, y = 30, label = label),\n    hjust = -0.3,\n    col = \"red\"\n  ) +\n  labs(\n    x = \"Price\",\n    y = \"count\",\n    title = \"Long-tailed Price distribution\"\n  ) +\n  theme_igray() +\n  scale_x_continuous(label = label_dollar())\n```\n\n::: {.cell-output-display}\n![House price distribution](index_files/figure-html/fig-price-distribution-1.png){#fig-price-distribution width=672}\n:::\n:::\n\n\nThe most house price ranges between 11000 to 14000 dollars @fig-price-distribution. The distribution shows there seems to be an outlier in our data. fig-outlier shows the outlier\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutlier <- \n  tibble(\n    x = 1,\n    max_price = max(real_estate$price_usd),\n  )\n\n    \nggplot(real_estate, aes(price_usd, x = 1)) +\n  ggbeeswarm::geom_quasirandom(\n    col = \"darkgreen\",\n    shape = \"circle\"\n  ) + \n  geom_point(\n    data = outlier, \n    aes(x, max_price),\n    shape = \"circle filled\", stroke = 1.2, size = 3,\n    fill = \"red\",  col = \"orange\",\n  ) +\n  geom_text(\n    data = outlier,\n    aes(y = max_price, label = \"Outlier\"),\n    vjust = 1.7\n  ) +\n  scale_y_continuous(\n    label = label_dollar(),\n    breaks = seq(0, 40000, 5000)\n  ) +\n  labs(\n    x = \"\",\n    y = \"Price\",\n    title = \"Red dot shows house out that is overprized\"\n  ) +\n  coord_flip() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank()\n  ) +\n  theme_pander()\n```\n\n::: {.cell-output-display}\n![Outlier point significantly overprized above 30000 usd](index_files/figure-html/fig-outlier-1.png){#fig-outlier width=672}\n:::\n:::\n\n\nWe need to remove the overprized house\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal_estate <- real_estate |> filter(!price_usd > 30000)\n\nrange(real_estate$price_usd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  2432 25056\n```\n\n\n:::\n:::\n\n\nWe will continue our EDA now that the outlier has been removed\n\n### Multivariate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(real_estate, aes(factor(sale_date), price_usd)) +\n  geom_violin(fill = \"olivedrab3\") +\n  geom_jitter(aes(y = price_usd), size = 0.5, alpha = 0.5, col = \"red\") +\n  theme(axis.text.x = element_text(angle = 20)) +\n  labs(x = \"Sale Date\", y = \"Price\", \n       title = \"January and November shows large volume of sales\",\n       subtitle = \"Mid year (May/June) shows increase in house purchase, as sales in other months declines\"\n  ) +\n  scale_y_continuous(label = label_dollar()) +\n  theme_pander()\n```\n\n::: {.cell-output-display}\n![Monthly price distribution of houses, there are some traces of seasonality](index_files/figure-html/price-distribution-month-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(real_estate, aes(fct_reorder(cut_number(age, 10), price_usd, .fun = sum), price_usd)) +\n  geom_col(fill = \"springgreen3\") +\n  labs(\n    x = \"Age\",\n    y = \"Price\",\n    title = str_wrap(\"New houses age 0 to 4 years fetch made more sales in dollar\n                     in general than old houses\", width = 60)\n  ) +\n  scale_y_continuous(label = label_dollar()) +\n  coord_flip() +\n  theme_igray()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/age-price-relationship-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrelation <- cor(real_estate$price_usd, real_estate$age)\n\nggplot(real_estate, aes(price_usd, age)) +\n  geom_smooth(method = \"lm\", se = F, col = \"tomato2\") +\n  expand_limits(y = c(0, 45)) +\n  labs(\n    x = \"Price\",\n    y = \"Age\",\n    title = \"House price reduces as age increases\"\n  )+\n  annotate(\n    geom = \"label\",\n    label = paste(\"correlation:\", round(correlation, 2), sep = \" \"),\n    x = 15000, y = 25, col = \"red\"\n  ) +\n  theme_clean()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Correlation between  age and price](index_files/figure-html/fig-correlation-age-price-1.png){#fig-correlation-age-price width=672}\n:::\n:::\n\n\n@fig-correlation-age-price shows the relationship between house price and the age of houses\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(real_estate, aes(price_usd, distance_to_station)) +\n  geom_point() +\n  scale_y_log10(label = label_number()) +\n  labs(\n    x = \"Price\",\n    y = \"Distance to Station (m)\",\n    title = \"Negative relationship between Price and Distance to Station\",\n    subtitle = \"Houses closer to the station are costlier\"\n  ) +\n  theme_pander()\n```\n\n::: {.cell-output-display}\n![Correlation between price and distance to station](index_files/figure-html/fig-cor-1.png){#fig-cor width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(real_estate, aes(longitude, latitude, col = price_usd)) +\n  geom_jitter() +\n  labs(\n    col = \"Price (USD)\",\n    x = \"Longitude\",\n    y = \"Latitude\",\n    title = \"The prices of houses increases as we move North East\",\n    subtitle = str_wrap(\"Prices of houses increases where there are clusters\\ of house, this\n                        may be due to the proximity to the MRT station\", width = 55)\n  ) +\n  scale_colour_gradient(low = \"gray\", high = \"red\") +\n  theme_pander() +\n  theme(legend.position = \"top\") +\n  guides(\n    color = guide_colorbar(barwidth = 15, barheight = 1/2, ticks.colour = \"black\", title.position = \"left\", title.theme = element_text(size = 8)))\n```\n\n::: {.cell-output-display}\n![Houses get expensive as we move in a northeast direction,](index_files/figure-html/location-of-houses-1.png){width=672}\n:::\n:::\n\n\n### Correlation with other variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggcorr(real_estate |> select(!c(sale_date, price)))\n```\n\n::: {.cell-output-display}\n![All the factors shows strong relationship with the price of the building](index_files/figure-html/correlation-plot-1.png){width=672}\n:::\n:::\n\n\nWhile size, number of convenience store close to the building and the position of the building, i.e., longitude and latitude are positively correlated to the price of a building, the older a building, and the farther it is from the MRT station the more likely it reduces in price.\n\n# Model Development\n\nBefore we begin modeling, we need to remove some variables that might not be a big influence, this include:\n\n-   sales_date, as there is just a year span of data, it is better we extract just the month use that\n\n-   price, we have price in US Dollar, already, we do not need the price in Taiwanese dollars.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal_estate <- real_estate |> \n  mutate(\n    month = month(sale_date),\n    .before = age\n  ) |> \n  select(-c(sale_date, price))\n\nhead(real_estate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 8\n  month   age distance_to_station number_convenience_stores latitude longitude\n  <dbl> <dbl>               <dbl>                     <dbl>    <dbl>     <dbl>\n1    11    32                84.9                        10     25.0      122.\n2    11    20               307.                          9     25.0      122.\n3     7    14               562.                          5     25.0      122.\n4     6    14               562.                          5     25.0      122.\n5    10     5               391.                          5     25.0      122.\n6     8     8              2175.                          3     25.0      122.\n# ℹ 2 more variables: size_m2 <dbl>, price_usd <dbl>\n```\n\n\n:::\n:::\n\n\nFor this analysis, we will use:\n\n-   XGboost\n\n## Data Splitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(333)\n\n\nreal_estate_split <- initial_split(real_estate, prop = .8, strata = price_usd)\n\nreal_estate_train <- training(real_estate_split)\nreal_estate_test <- testing(real_estate_split)\n\nreal_estate_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<328/85/413>\n```\n\n\n:::\n:::\n\n\n## Model Specification\n\nGiven our choice of model, XGBoost, a tree-based model, a lot of preprocessing is not required, we can going to dive right into our model specification, and tune a lot of the model hyperparameter to reduce the chances of over-fitting and under-fitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxg_model <- \n  boost_tree(\n    mtry = tune(), min_n = tune(),\n    tree_depth = tune(), trees = 1000,\n    loss_reduction = tune(),\n    sample_size = tune(),\n    learn_rate = tune(),\n  ) |> \n  set_engine(\"xgboost\") |> \n  set_mode(\"regression\")\n\nxg_model |>  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n\nModel fit template:\nparsnip::xgb_train(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    colsample_bynode = tune(), nrounds = 1000, min_child_weight = tune(), \n    max_depth = tune(), eta = tune(), gamma = tune(), subsample = tune(), \n    nthread = 1, verbose = 0)\n```\n\n\n:::\n:::\n\n\n## Workflow Process\n\nTo improve efficiency and streamline processes, we start a modelling workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxg_wf <- workflow() |> \n  add_formula(price_usd ~ .) |> \n  add_model(xg_model)\n\nxg_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nprice_usd ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n```\n\n\n:::\n:::\n\n\n## Cross Validation\n\nNext, we create resamples for tuning the model, @tbl-cross-validation-resamples.\n\n\n::: {#tbl-cross-validation-resamples .cell .tbl-cap-location-top tbl-cap='10 Cross Fold Resamples'}\n\n```{.r .cell-code}\nset.seed(222)\n\nreal_estate_folds <- vfold_cv(real_estate_train, strata = price_usd)\n```\n:::\n\n\n## Tune Grid\nNext, we have to set up some values for our hyperparameter, we don't want to exhaust our computing resource, and face the risk of overfitting. We will use the **Latin Hypercube** grid as this approach can be more computationally efficient than a regular grid, especially when there are many hyperparameters to tune. Random selection can also introduce diversity into the search process.\n\n\n::: {#tbl-boost-tree-grid .cell .tbl-cap-location-top tbl-cap='XGBoost Tune Grid'}\n\n```{.r .cell-code}\nset.seed(3434)\n\n\nxgb_grid <- grid_latin_hypercube(\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), real_estate_train),\n  learn_rate(),\n  size = 30\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nxgb_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30 × 6\n   tree_depth min_n loss_reduction sample_size  mtry learn_rate\n        <int> <int>          <dbl>       <dbl> <int>      <dbl>\n 1          5    32       1.17e- 6       0.529     4   8.06e- 3\n 2          6    16       3.05e- 1       0.446     3   5.03e- 7\n 3         15    16       3.58e- 9       0.566     2   2.56e- 6\n 4         13    36       5.57e- 1       0.637     3   7.82e- 6\n 5         10     7       2.58e- 4       0.668     5   7.03e-10\n 6          5    20       1.11e- 2       0.491     7   5.75e- 3\n 7          8    13       3.64e-10       0.339     6   1.05e- 7\n 8         13    25       2.87e+ 0       0.937     1   2.94e-10\n 9          4    31       1.55e+ 0       0.827     2   8.75e- 7\n10          5    14       7.17e- 4       0.771     5   1.64e-10\n# ℹ 20 more rows\n```\n\n\n:::\n:::\n\n\nSince `mtry` depends on the number of predictors, it had to be tuned differently @tbl-boost-tree-grid.\n\n**NOW WE TUNE**. We will use our resamples, the tuneable workflow, and the Latin grid of parameters which we have to try get the best value. To also speed up the process, we will enable parallel computing\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nset.seed(222)\n\nxg_tune_res <- tune_grid(\n  xg_wf,\n  resamples = real_estate_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = T)\n)\n```\n:::\n\n\n## Exploring Tune Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxg_tune_res |> \n  collect_metrics() |> \n  filter(.metric == \"rmse\") |> \n  select(mean, mtry:sample_size) |> \n  pivot_longer(\n    mtry:sample_size,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) |> \n  ggplot(aes(value, mean, color = parameter)) +\n  geom_jitter(show.legend = F, width = .4) +\n  facet_wrap(~parameter, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![Tuning result](index_files/figure-html/fig-tune-res-1.png){#fig-tune-res width=672}\n:::\n:::\n\n\nThe lower the rmse, the better the model, a simplification, but this is not always the case. We will stick to that for now.\n\nLet's show the best performing set of parameter \\\n\n## Best Tune\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xg_tune_res, metric = \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 12\n   mtry min_n tree_depth learn_rate loss_reduction sample_size .metric\n  <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>  \n1     4     3         13    0.0145     0.0000176         0.704 rmse   \n2     8    18          6    0.0578     0.0000239         0.966 rmse   \n3     3     8          3    0.0422     0.00411           0.688 rmse   \n4     7    20          5    0.00575    0.0111            0.491 rmse   \n5     6     5         10    0.00269    0.000000326       0.893 rmse   \n# ℹ 5 more variables: .estimator <chr>, mean <dbl>, n <int>, std_err <dbl>,\n#   .config <chr>\n```\n\n\n:::\n:::\n\n\n# Finalize Model Workflow\n\nLet's select the best and use it to finalize our model.\n\n## Select Best Parameter\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rmse <- select_best(xg_tune_res, metric = \"rmse\")\nbest_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 7\n   mtry min_n tree_depth learn_rate loss_reduction sample_size .config          \n  <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>            \n1     4     3         13     0.0145      0.0000176       0.704 Preprocessor1_Mo…\n```\n\n\n:::\n:::\n\n\nNow we can finalize the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_boost_tree <- finalize_workflow(\n  xg_wf,\n  best_rmse\n)\n\nfinal_boost_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nprice_usd ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = 4\n  trees = 1000\n  min_n = 3\n  tree_depth = 13\n  learn_rate = 0.0145039211746767\n  loss_reduction = 1.76136801549921e-05\n  sample_size = 0.70429474228993\n\nComputational engine: xgboost \n```\n\n\n:::\n:::\n\n\n## Variable Importance\n\nLet's see the most important variables in the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'vip'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:utils':\n\n    vi\n```\n\n\n:::\n\n```{.r .cell-code}\nfinal_boost_tree |> \n  fit(data = real_estate_train) |> \n  pull_workflow_fit() |> \n  vip(\n    geom = \"col\",\n    aesthetics = list(fill = \"springgreen3\")\n  ) +\n  theme_pander()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nℹ Please use `extract_fit_parsnip()` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Feature importance](index_files/figure-html/variable-importance-1.png){width=672}\n:::\n:::\n\n\nThe most important predictor of the price of a house are the:\n\n-   Size\n-   Distance to the station,\n-   The latitude of the buildings, and\n-   The number of convenience stores.\n\n# Model Evaluation\n\nLet's test how good the model is with the test data.\n\n\n::: {#tbl-model-eval .cell tbl-cap='Model evaluation'}\n\n```{.r .cell-code}\nfinal_result <- last_fit(final_boost_tree, real_estate_split)\n\ncollect_metrics(final_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard     177.    Preprocessor1_Model1\n2 rsq     standard       0.998 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nThat's a high Rsquared, close to 1, and the RMSE have a very low error of ± 225.4 dollars. Let's plot prediction vs actual values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_result |> \n  collect_predictions() |> \n  select(\"actual\" = price_usd, \"prediction\" = .pred) |> \n  ggplot(aes(actual, prediction)) +\n  geom_point(col = \"orange2\") +\n  geom_label(\n    aes(x = 10500, y = 15000, label = \"R-square: 0.9974\"),\n    col = \"blue\"\n  ) +\n  geom_abline(col = \"red\") +\n  theme_few()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_label(aes(x = 10500, y = 15000, label = \"R-square: 0.9974\"), : All aesthetics have length 1, but the data has 85 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Model performance](index_files/figure-html/fig-mod-par-1.png){#fig-mod-par width=672}\n:::\n:::\n\n\n@fig-mod-par shows a good performance of the model. For future prediction on a similar data in the region we extract the model and save it for later use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal_estate_boost_tree_model <- final_result |> \n  extract_fit_parsnip()\n```\n:::\n\n\n\n# Conclusion\n\nThis project shows the capabilities of R, and the XGBoost algorithm in real estate use. While the model was built to predict price, it could be made better if a time component is give. Given the data used for this project, a time component is ill-advised as seasonality, and other time related components will not be properly studied by the algorithm.\n\n\nCover photo by <a href=\"https://unsplash.com/@basglaap?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Bas Glaap</a> on <a href=\"https://unsplash.com/photos/white-truck-passing-parked-motorcycles-during-daytime-6WGBumlmwHM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n  \n\n[Back to homepage](https://olamideadu.com)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}