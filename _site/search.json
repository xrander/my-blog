[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "",
    "section": "",
    "text": "My personal blog content"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Olamide Adu is the CEO and Founder of EU StudyAssist. He is a Data Scientist with background in Sustainable Forest and Nature Management, a recipient and graduate of the EMJMD SUFONAMA program from the Swedish University of Agricultural Sciences in 2023. A passionate Data Science with love for growth and learning new things and a big lover of R programming Language."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nSwedish University of Agricultural Sciences | Alnarp, Skåne, Sweden  MSc in Forest Sciences | August 2022 - Sept 2023\nUniversity of Copenhagen | Frederiksberg, Copenhagen, Denmark  MSc in Forest Sciences | August 2021 - July 2022\nThe Federal University of Technology, Akure | Akure South, Ondo, Nigeria  B. Agric. Tech. in Forestry and Wood Technology | January 2015 - December 2019"
  },
  {
    "objectID": "index.html#experiences",
    "href": "index.html#experiences",
    "title": "About Me",
    "section": "Experiences",
    "text": "Experiences\n\nSwedish University of Agricultural Sciences | Data Analyst | November 2022 - January 2023\nHP Tech Venture Group LLC | Summer Research Analyst (Externship) | January 2022 - July 2022\nFederal College of Education, Pankshin | Teaching Assistant | March 2020 - February 2021"
  },
  {
    "objectID": "index.html#languages",
    "href": "index.html#languages",
    "title": "About Me",
    "section": "Languages",
    "text": "Languages\n\nR\nPython\nJavascript"
  },
  {
    "objectID": "index.html#honors-and-awards",
    "href": "index.html#honors-and-awards",
    "title": "About Me",
    "section": "Honors and Awards",
    "text": "Honors and Awards\n\nErasmus Mundus Joint Masters Degree Scholarship (€49000) | 2021 - 2023\nDean’s List Award of Excellence | 2015 - 2019"
  },
  {
    "objectID": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#getting-our-data",
    "href": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#getting-our-data",
    "title": "Webscraping and Visualizing the Top CryptoCurrencies",
    "section": "Getting Our data",
    "text": "Getting Our data\n\nFirst, I scraped data from CoinMarketCap using the URL https://coinmarketcap.com/all/views/all/. The code extracts a specific table and selects relevant columns like name, symbol, market cap, and price.\n\n\n\nShow the code\nlibrary(pacman)\np_load(rvest, tidyverse, magick, ggimage)\n\nurl &lt;- \"https://coinmarketcap.com/all/views/all/\"\n\ncrypto &lt;- read_html(url) |&gt; \n  html_nodes(\"table\") |&gt; \n  html_table()\n\ncrypto &lt;- crypto[[3]]\n\nhead(crypto)\n\n\n# A tibble: 6 × 1,001\n   Rank Name        Symbol `Market Cap` Price `Circulating Supply` `Volume(24h)`\n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;        \n1     1 BTCBitcoin  BTC    $1.07T$1,07… $54,… 19,719,475 BTC       $52,500,564,…\n2     2 ETHEthereum ETH    $345.06B$34… $2,8… 120,196,848 ETH *    $29,468,896,…\n3     3 USDTTether… USDT   $112.44B$11… $0.9… 112,484,875,383 USD… $96,103,976,…\n4     4 BNBBNB      BNB    $69.21B$69,… $469… 147,582,870 BNB *    $2,632,825,3…\n5     5 SOLSolana   SOL    $58.11B$58,… $125… 462,886,047 SOL *    $4,203,105,4…\n6     6 USDCUSDC    USDC   $33.09B$33,… $0.9… 33,099,458,015 USDC… $9,313,268,3…\n# ℹ 994 more variables: `% 1h` &lt;chr&gt;, `% 24h` &lt;chr&gt;, `% 7d` &lt;chr&gt;, `` &lt;lgl&gt;,\n#   `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;,\n#   `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;,\n#   `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;,\n#   `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;,\n#   `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;,\n#   `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, `` &lt;lgl&gt;, …"
  },
  {
    "objectID": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#data-exploration-and-cleaning",
    "href": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#data-exploration-and-cleaning",
    "title": "Webscraping and Visualizing the Top CryptoCurrencies",
    "section": "Data Exploration and Cleaning",
    "text": "Data Exploration and Cleaning\n\nI started by cleaning the column names using janitor::clean_names() and selecting the columns I needed. Then, I ensured data types were appropriate by converting market_cap and price to numeric values.\n\n\n\nShow the code\ncrypto &lt;- crypto |&gt; \n  janitor::clean_names() |&gt; \n  select(name, symbol, market_cap, price)\n\nglimpse(crypto)\n\n\nRows: 200\nColumns: 4\n$ name       &lt;chr&gt; \"BTCBitcoin\", \"ETHEthereum\", \"USDTTether USDt\", \"BNBBNB\", \"…\n$ symbol     &lt;chr&gt; \"BTC\", \"ETH\", \"USDT\", \"BNB\", \"SOL\", \"USDC\", \"XRP\", \"TON\", \"…\n$ market_cap &lt;chr&gt; \"$1.07T$1,071,480,397,941\", \"$345.06B$345,058,510,685\", \"$1…\n$ price      &lt;chr&gt; \"$54,373.45\", \"$2,871.28\", \"$0.9996\", \"$469.29\", \"$125.54\",…\n\n\n\nIs the data structure as expected? From the data which we have above, there are some columns that needs their data types changed. The market_cap and price column should be numeric/double data type and not character.\n\n\n\nShow the code\ncrypto &lt;- crypto |&gt; \n  mutate(\n    market_cap = str_remove_all(market_cap, r\"--[\\$[\\d.]+[TB]]--\"),\n    market_cap = parse_number(market_cap),\n    price = parse_number(price)\n  )\n\nstr(crypto)\n\n\ntibble [200 × 4] (S3: tbl_df/tbl/data.frame)\n $ name      : chr [1:200] \"BTCBitcoin\" \"ETHEthereum\" \"USDTTether USDt\" \"BNBBNB\" ...\n $ symbol    : chr [1:200] \"BTC\" \"ETH\" \"USDT\" \"BNB\" ...\n $ market_cap: num [1:200] 1.07e+12 3.45e+11 1.12e+11 6.92e+10 5.81e+10 ...\n $ price     : num [1:200] 54373 2871 1 469 126 ..."
  },
  {
    "objectID": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#handling-missing-data",
    "href": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#handling-missing-data",
    "title": "Webscraping and Visualizing the Top CryptoCurrencies",
    "section": "Handling missing Data",
    "text": "Handling missing Data\n\nI used the skimr package to identify missing data. The code then filtered the crypto data frame to keep only complete rows with values in all columns.\n\n\n\nShow the code\nskimr::n_missing(crypto)\n\n\n[1] 360\n\n\n\n\nShow the code\nskimr::skim(crypto)\n\n\n\nData summary\n\n\nName\ncrypto\n\n\nNumber of rows\n200\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n3\n37\n0\n200\n0\n\n\nsymbol\n0\n1\n0\n4\n180\n21\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmarket_cap\n180\n0.1\n9.107187e+10\n2.433057e+11\n4359885722\n6.659552e+09\n1.119664e+10\n3.93483e+10\n1.071480e+12\n▇▁▁▁▁\n\n\nprice\n180\n0.1\n2.912640e+03\n1.212951e+04\n0\n8.500000e-01\n5.540000e+00\n7.51500e+01\n5.437345e+04\n▇▁▁▁▁\n\n\n\n\n\n\n\nShow the code\ncrypto &lt;- crypto[complete.cases(crypto), ]\nskimr::skim_without_charts(crypto)\n\n\n\nData summary\n\n\nName\ncrypto\n\n\nNumber of rows\n20\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n6\n17\n0\n20\n0\n\n\nsymbol\n0\n1\n3\n4\n0\n20\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\nmarket_cap\n0\n1\n9.107187e+10\n2.433057e+11\n4359885722\n6.659552e+09\n1.119664e+10\n3.93483e+10\n1.071480e+12\n\n\nprice\n0\n1\n2.912640e+03\n1.212951e+04\n0\n8.500000e-01\n5.540000e+00\n7.51500e+01\n5.437345e+04"
  },
  {
    "objectID": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#data-visualization",
    "href": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#data-visualization",
    "title": "Webscraping and Visualizing the Top CryptoCurrencies",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nNow, let’s visualize the data! I created a donut chart to represent the market cap distribution of the top six cryptocurrencies. The remaining currencies are grouped into an “Other” category.\nThe code calculates the market cap share for each currency, along with cumulative values and labels for the chart.\n\n\nShow the code\ncrypto &lt;- crypto |&gt; \n  mutate(\n    new_sym = fct_lump(\n      symbol, n = 6, w = market_cap\n    )\n  )\n\ncrypto |&gt; \n  summarize(\n    .by = new_sym,\n    market_cap = sum(market_cap),\n    count = n()\n  )\n\n\n\n\nTable 1: Top Six Cryptocurrencies according to Market Capitalization\n\n\n\n# A tibble: 7 × 3\n  new_sym    market_cap count\n  &lt;fct&gt;           &lt;dbl&gt; &lt;int&gt;\n1 BTC     1071480397941     1\n2 ETH      345058510685     1\n3 USDT     112440083915     1\n4 BNB       69212448352     1\n5 SOL       58108556585     1\n6 USDC      33094885127     1\n7 Other    132042521446    14\n\n\n\n\n\nTable 1 shows that the market cap has been compressed into 7, 6 for the top cryptocurrency and 14 lumped together into a new category, Other.\n\n\n\nShow the code\ncrypto_summary &lt;- crypto |&gt; \n  summarize(\n    .by = new_sym,\n    market_cap = sum(market_cap)\n  ) |&gt; \n  mutate(\n    prop = market_cap/sum(market_cap) * 100,\n    market_cap = round(market_cap/1e9, 2),\n    market_cap = paste0(market_cap, \" B\"),\n    ymax = cumsum(prop),\n    ymin = c(0, head(ymax, n = -1)),\n    lab_pos = (ymax + ymin)/2,\n    label = paste0(new_sym, \"\\nValue: \",round(prop, 2), \"%\")\n  )"
  },
  {
    "objectID": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#crytocurrencies-by-market-capitalization",
    "href": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#crytocurrencies-by-market-capitalization",
    "title": "Webscraping and Visualizing the Top CryptoCurrencies",
    "section": "Crytocurrencies by Market Capitalization",
    "text": "Crytocurrencies by Market Capitalization\n\n\nShow the code\nggplot(\n  crypto_summary,\n  aes(xmin = 3, xmax = 4,ymin = ymin, ymax = ymax, fill = new_sym)\n) +\n  geom_rect() +\n  expand_limits(x = c(1.5, 4)) +\n  coord_polar(theta = \"y\", start = 1) +\n  scale_fill_brewer(palette =\"YlOrRd\") +\n  theme_void() +\n  ggrepel::geom_label_repel(\n    x = 3,\n    aes(y = lab_pos, label = label),\n    size = 2,\n    col = \"gray3\"\n  ) +\n  theme_void() +\n  ggtitle(\"Market Cap of Top Cryptocurrencies\") +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\nCrypto Asset Market Cap\n\n\n\n\nAs shown in ?@fig-market-cap, Bitcoin, and ETH are clearly dominating the crypto space in market capitalization. Assets such as BNB, SOL and USDT are slowly increasing their dominance ranging from 3 - 6%."
  },
  {
    "objectID": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#top-20-cryptocurrencies-price",
    "href": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#top-20-cryptocurrencies-price",
    "title": "Webscraping and Visualizing the Top CryptoCurrencies",
    "section": "Top 20 Cryptocurrencies Price",
    "text": "Top 20 Cryptocurrencies Price\n\nI downloaded logos for the top 20 cryptocurrencies and added them as an “images” column to the data frame.\nThe code then creates a bar chart to visualize individual cryptocurrency prices, with labels indicating the price for each currency.\n\n\n\nShow the code\nimages &lt;- list.files(path = \"images\", full.names = TRUE)\n\ncrypto &lt;- crypto |&gt; \n  arrange(symbol) |&gt; \n  bind_cols(\"images\" = images)\n\ncrypto_img &lt;- crypto |&gt; \n  mutate(\n    images = paste0(\"&lt;img src='\", images, \"' width='15'/&gt;\")\n  )\n\n\nNow we can visualize the prices of each asset.\n\n\nShow the code\ncrypto_img |&gt; \n  ggplot(aes(price, fct_reorder(images, price))) +\n  geom_col(\n    width = .1,\n    fill = \"#FBD25B\"\n  ) +\n  geom_label(\n    aes(label = round(price, 2)),\n    col = \"white\",\n    fill = \"#AE1D0E\",\n    size = 2.5\n  ) +\n  labs(\n    title = \"Price of the Top 20 Cryptocurrencies\"\n  ) +\n  scale_x_log10(label = scales::label_number()) +\n  theme_minimal() +\n  theme(\n    axis.text.y = ggtext::element_markdown(),\n    axis.text.x = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title = element_blank(),\n    plot.title = element_text(hjust = .5, color = \"#AE1D0E\")\n  )"
  },
  {
    "objectID": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#conclusion",
    "href": "posts/webscraping-and-visualizing-the-top-crypto-currencies/index.html#conclusion",
    "title": "Webscraping and Visualizing the Top CryptoCurrencies",
    "section": "Conclusion",
    "text": "Conclusion\n\nIn this project, I successfully scraped cryptocurrency data, cleaned it for analysis, and created visualizations to explore market cap distribution and individual cryptocurrency prices. This process demonstrates the power of web scraping and data visualization in R."
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html",
    "href": "posts/algeria-fwi-prediction/index.html",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "",
    "text": "According to the World Bank, Algeria’s forest is about 0.82% of the total country’s land mass in 2021 (tradingeconomics.com). Algeria is one of the Maghreb countries affected by wildfire. Given the troubles associated with wildfire, it is important to assess and predict the potential for wildfire activity. One of the ways to predict the potential for wildfire is associating its occurrence with weather conditions. This is called the Fire Weather Index (FWI). FWI was developed by the Canadian Forest Service and it is a key component of the Canadian Forest Fire Weather Index System. FWI is also used internationally to assess fire danger and predict wildfire behavior based on weather conditions. \n\n\nFor this project, I will predict the occurrence of fire (fire or nor fire) given a set of parameters related to FWI in two regions of Algeria, the Bejaia region located in the northeast of Algeria and Sidi Bel-abbes region located in the northwest of Algeria.\n\n\n\nThe data for this analysis is collected from UCI machine learning data repository and provided by (Abid and Izeboudjen 2020). The variables include:\n\n\n\n\n\n\n\n\nVariable name\nData type\nDefinition\n\n\n\n\nregion\ncategorical\narea in Algeria, either of Sidi Bel-abbes or Bejaia\n\n\nday\ndate\nthe day in number\n\n\nmonth\ndate\nmonth of the year, from June to September\n\n\nyear\ndate\nsingle year of when data as observed\n\n\ntemp\nnumeric\nmax noon temperature in \\(^{\\circ}C\\)\n\n\nrh\nnumeric\nrelative humidity in percentage\n\n\nws\nnumeric\nwind speed in km/h\n\n\nrain\nnumeric\ntotal rain in a day in mm\n\n\nffmc\nnumeric\nfine fuel moisture code\n\n\ndmc\nnumeric\nDuff moisture code\n\n\ndc\nnumeric\ndrought code\n\n\nisi\nnumeric\nInitial spread index\n\n\nbui\nnumeric\nbuildup index\n\n\nfwi\nnumeric\nfire weather index\n\n\nclasses\nbinary\nclass of fire occurrence. This is the target variable\n\n\n\nTo get started I load all necessary packages. Tidyverse for all forms of data manipulation, visualization and data importation and tidymodels for our model workflow.\n\n\nShow the code\nlibrary(pacman)\np_load(tidyverse, tidymodels, knitr, ggthemes, hrbrthemes)\ntheme_set(theme_ipsum_ps(base_size = 12))\n\n\n\n\n\n\n\n\nI like using the pacman package instead of using base R’s library() function because it simplifies library management and integration. Another packagem management system is pkgdown.\n\n\n\nNext I import the data\n\n\nShow the code\nalgeria_ff &lt;- read_csv(\"data/Algerian_forest_fires_dataset_UPDATE.csv\", skip = 1) |&gt; \n  janitor::clean_names()\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 246 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (14): day, month, year, Temperature, RH, Ws, Rain, FFMC, DMC, DC, ISI, B...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html#objective",
    "href": "posts/algeria-fwi-prediction/index.html#objective",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "",
    "text": "For this project, I will predict the occurrence of fire (fire or nor fire) given a set of parameters related to FWI in two regions of Algeria, the Bejaia region located in the northeast of Algeria and Sidi Bel-abbes region located in the northwest of Algeria."
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html#data",
    "href": "posts/algeria-fwi-prediction/index.html#data",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "",
    "text": "The data for this analysis is collected from UCI machine learning data repository and provided by (Abid and Izeboudjen 2020). The variables include:\n\n\n\n\n\n\n\n\nVariable name\nData type\nDefinition\n\n\n\n\nregion\ncategorical\narea in Algeria, either of Sidi Bel-abbes or Bejaia\n\n\nday\ndate\nthe day in number\n\n\nmonth\ndate\nmonth of the year, from June to September\n\n\nyear\ndate\nsingle year of when data as observed\n\n\ntemp\nnumeric\nmax noon temperature in \\(^{\\circ}C\\)\n\n\nrh\nnumeric\nrelative humidity in percentage\n\n\nws\nnumeric\nwind speed in km/h\n\n\nrain\nnumeric\ntotal rain in a day in mm\n\n\nffmc\nnumeric\nfine fuel moisture code\n\n\ndmc\nnumeric\nDuff moisture code\n\n\ndc\nnumeric\ndrought code\n\n\nisi\nnumeric\nInitial spread index\n\n\nbui\nnumeric\nbuildup index\n\n\nfwi\nnumeric\nfire weather index\n\n\nclasses\nbinary\nclass of fire occurrence. This is the target variable\n\n\n\nTo get started I load all necessary packages. Tidyverse for all forms of data manipulation, visualization and data importation and tidymodels for our model workflow.\n\n\nShow the code\nlibrary(pacman)\np_load(tidyverse, tidymodels, knitr, ggthemes, hrbrthemes)\ntheme_set(theme_ipsum_ps(base_size = 12))\n\n\n\n\n\n\n\n\nI like using the pacman package instead of using base R’s library() function because it simplifies library management and integration. Another packagem management system is pkgdown.\n\n\n\nNext I import the data\n\n\nShow the code\nalgeria_ff &lt;- read_csv(\"data/Algerian_forest_fires_dataset_UPDATE.csv\", skip = 1) |&gt; \n  janitor::clean_names()\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 246 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (14): day, month, year, Temperature, RH, Ws, Rain, FFMC, DMC, DC, ISI, B...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html#data-preview",
    "href": "posts/algeria-fwi-prediction/index.html#data-preview",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "Data Preview",
    "text": "Data Preview\nA quick preview of the data is the first step:\n\n\nShow the code\nalgeria_ff |&gt; \n  head() |&gt; \n  kable()\nalgeria_ff |&gt; \n  tail() |&gt; \n  kable()\nalgeria_ff |&gt; \n  car::some() |&gt; \n  kable()\n\n\n\n\nTable 1: Data preview\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nday\nmonth\nyear\ntemperature\nrh\nws\nrain\nffmc\ndmc\ndc\nisi\nbui\nfwi\nclasses\n\n\n\n\n01\n06\n2012\n29\n57\n18\n0\n65.7\n3.4\n7.6\n1.3\n3.4\n0.5\nnot fire\n\n\n02\n06\n2012\n29\n61\n13\n1.3\n64.4\n4.1\n7.6\n1\n3.9\n0.4\nnot fire\n\n\n03\n06\n2012\n26\n82\n22\n13.1\n47.1\n2.5\n7.1\n0.3\n2.7\n0.1\nnot fire\n\n\n04\n06\n2012\n25\n89\n13\n2.5\n28.6\n1.3\n6.9\n0\n1.7\n0\nnot fire\n\n\n05\n06\n2012\n27\n77\n16\n0\n64.8\n3\n14.2\n1.2\n3.9\n0.5\nnot fire\n\n\n06\n06\n2012\n31\n67\n14\n0\n82.6\n5.8\n22.2\n3.1\n7\n2.5\nfire\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nday\nmonth\nyear\ntemperature\nrh\nws\nrain\nffmc\ndmc\ndc\nisi\nbui\nfwi\nclasses\n\n\n\n\n25\n09\n2012\n28\n70\n15\n0\n79.9\n13.8\n36.1\n2.4\n14.1\n3\nnot fire\n\n\n26\n09\n2012\n30\n65\n14\n0\n85.4\n16\n44.5\n4.5\n16.9\n6.5\nfire\n\n\n27\n09\n2012\n28\n87\n15\n4.4\n41.1\n6.5\n8\n0.1\n6.2\n0\nnot fire\n\n\n28\n09\n2012\n27\n87\n29\n0.5\n45.9\n3.5\n7.9\n0.4\n3.4\n0.2\nnot fire\n\n\n29\n09\n2012\n24\n54\n18\n0.1\n79.7\n4.3\n15.2\n1.7\n5.1\n0.7\nnot fire\n\n\n30\n09\n2012\n24\n64\n15\n0.2\n67.3\n3.8\n16.5\n1.2\n4.8\n0.5\nnot fire\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nday\nmonth\nyear\ntemperature\nrh\nws\nrain\nffmc\ndmc\ndc\nisi\nbui\nfwi\nclasses\n\n\n\n\n25\n07\n2012\n31\n65\n18\n0\n84.3\n12.5\n88.7\n4.8\n18.5\n7.3\nfire\n\n\n11\n06\n2012\n31\n42\n21\n0\n90.6\n18.2\n30.5\n13.4\n18\n16.7\nfire\n\n\n22\n06\n2012\n33\n46\n14\n1.1\n78.3\n8.1\n8.3\n1.9\n7.7\n1.2\nnot fire\n\n\n03\n07\n2012\n34\n56\n17\n0.1\n84.7\n9.7\n27.3\n4.7\n10.3\n5.2\nfire\n\n\n06\n07\n2012\n35\n42\n15\n0.3\n84.7\n15.5\n45.1\n4.3\n16.7\n6.3\nfire\n\n\n16\n07\n2012\n31\n83\n17\n0\n84.5\n19.4\n33.1\n4.7\n19.2\n7.3\nfire\n\n\n29\n07\n2012\n34\n59\n16\n0\n88.1\n19.5\n47.2\n7.4\n19.5\n10.9\nfire\n\n\n31\n07\n2012\n37\n55\n15\n0\n89.3\n28.3\n67.2\n8.3\n28.3\n14.5\nfire\n\n\n03\n09\n2012\n28\n75\n16\n0\n82.2\n4.4\n24.3\n3.3\n6\n2.5\nfire\n\n\n23\n09\n2012\n35\n56\n14\n0\n89\n29.4\n115.6\n7.5\n36\n15.2\nfire\n\n\n\n\n\n\n\n\n\n\n\nTable 1 shows the first six observations, Table 1 (a), the last six observations, Table 1 (b), and 10 random observations from the data, Table 1 (c).\nFrom Table 2, all the variables are character when they should be majorly numeric and one or two categorical variable. I can also see that the regions are not indicated in the data. The variables present are day, month, year, temperature, rh, ws, rain, ffmc, dmc, dc, isi, bui, fwi, classes.\n\n\nShow the code\nskimr::skim(algeria_ff)\n\n\n\n\nTable 2: Data Properties\n\n\n\n\n\n\n(a)\n\n\n\n\n\nName\nalgeria_ff\n\n\nNumber of rows\n246\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\n\nVariable type: character\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nday\n0\n1.00\n2\n29\n0\n33\n0\n\n\nmonth\n1\n1.00\n2\n5\n0\n5\n0\n\n\nyear\n1\n1.00\n4\n4\n0\n2\n0\n\n\ntemperature\n1\n1.00\n2\n11\n0\n20\n0\n\n\nrh\n1\n1.00\n2\n2\n0\n63\n0\n\n\nws\n1\n1.00\n1\n2\n0\n19\n0\n\n\nrain\n1\n1.00\n1\n4\n0\n40\n0\n\n\nffmc\n1\n1.00\n2\n4\n0\n174\n0\n\n\ndmc\n1\n1.00\n1\n4\n0\n167\n0\n\n\ndc\n1\n1.00\n1\n6\n0\n199\n0\n\n\nisi\n1\n1.00\n1\n4\n0\n107\n0\n\n\nbui\n1\n1.00\n1\n4\n0\n175\n0\n\n\nfwi\n1\n1.00\n1\n4\n0\n127\n0\n\n\nclasses\n2\n0.99\n4\n8\n0\n3\n0"
  },
  {
    "objectID": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html",
    "href": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html",
    "title": "Managing Workflowset Models",
    "section": "",
    "text": "The tidymodels package is a game-changer for the R ecosystem, providing a streamlined and intuitive approach to modeling. Built on the tidyverse foundation, it offers a cohesive framework that simplifies the journey from data wrangling to robust models. What makes tidymodels stand out is its consistent workflow, reducing the learning curve for data scientists and ensuring compatibility across different modeling packages【Kuhn and Silge (2022)】.\n\n\nThe workflows package is one of the standout components of tidymodels, making the iterative machine learning process in R more manageable. By bundling model fitting and data preprocessing steps into a single coherent object, workflows simplifies the complexities of the machine learning pipeline, ensuring each step is clearly defined and reproducible. This iterative machine learning process, as covered in “Tidy Modeling with R”【Kuhn and Silge (2022)】, is illustrated below:\n\n\n\nSource: Tidy Modeling with R\n\n\n\n\n\nThe focus of this post, the workflowsets package, builds on the workflows package by extending its capabilities to handle multiple machine learning models. Since the best model for any given task is not predetermined, it’s crucial to test multiple models and compare their performances. workflowsets is designed to manage multiple workflows, making it easier to compare different modeling approaches and preprocessing strategies.\nThis blog post introduces the option_add function of the workflowsets package, which is used to control options for evaluating workflow set functions such as fit_resamples and tune_grid. For more information on this function, refer to the documentation with ?option_add.\nWe start by loading the packages we will be using for this post\n\n\nShow the code\nlibrary(pacman)\np_load(tidyverse, tidymodels, gt, finetune, bonsai)\n\n\nFor this post we’ll use the heart disease dataset from kaggle.com. A preview of the data is given Table 1\n\n\nShow the code\nheart_disease &lt;- read_csv(\"heart_disease_dataset.csv\")\n\nhead(heart_disease) |&gt; \n  gt() |&gt; \n  tab_header(\n    title = \"Heart Diseases\"\n  ) |&gt; \n  opt_stylize(\n    style = 2, \n    color = \"cyan\"\n  ) |&gt; \n  as_raw_html()\n\n\n\n\nTable 1: Data Preview\n\n\n\n\n  \n  \n\n\n\nHeart Diseases\n\n\nAge\nGender\nCholesterol\nBlood Pressure\nHeart Rate\nSmoking\nAlcohol Intake\nExercise Hours\nFamily History\nDiabetes\nObesity\nStress Level\nBlood Sugar\nExercise Induced Angina\nChest Pain Type\nHeart Disease\n\n\n\n\n75\nFemale\n228\n119\n66\nCurrent\nHeavy\n1\nNo\nNo\nYes\n8\n119\nYes\nAtypical Angina\n1\n\n\n48\nMale\n204\n165\n62\nCurrent\nNone\n5\nNo\nNo\nNo\n9\n70\nYes\nTypical Angina\n0\n\n\n53\nMale\n234\n91\n67\nNever\nHeavy\n3\nYes\nNo\nYes\n5\n196\nYes\nAtypical Angina\n1\n\n\n69\nFemale\n192\n90\n72\nCurrent\nNone\n4\nNo\nYes\nNo\n7\n107\nYes\nNon-anginal Pain\n0\n\n\n62\nFemale\n172\n163\n93\nNever\nNone\n6\nNo\nYes\nNo\n2\n183\nYes\nAsymptomatic\n0\n\n\n77\nMale\n309\n110\n73\nNever\nNone\n0\nNo\nYes\nYes\n4\n122\nYes\nAsymptomatic\n1"
  },
  {
    "objectID": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#introduction",
    "href": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#introduction",
    "title": "Managing Workflowset Models",
    "section": "",
    "text": "The tidymodels package is a game-changer for the R ecosystem, providing a streamlined and intuitive approach to modeling. Built on the tidyverse foundation, it offers a cohesive framework that simplifies the journey from data wrangling to robust models. What makes tidymodels stand out is its consistent workflow, reducing the learning curve for data scientists and ensuring compatibility across different modeling packages【Kuhn and Silge (2022)】.\n\n\nThe workflows package is one of the standout components of tidymodels, making the iterative machine learning process in R more manageable. By bundling model fitting and data preprocessing steps into a single coherent object, workflows simplifies the complexities of the machine learning pipeline, ensuring each step is clearly defined and reproducible. This iterative machine learning process, as covered in “Tidy Modeling with R”【Kuhn and Silge (2022)】, is illustrated below:\n\n\n\nSource: Tidy Modeling with R\n\n\n\n\n\nThe focus of this post, the workflowsets package, builds on the workflows package by extending its capabilities to handle multiple machine learning models. Since the best model for any given task is not predetermined, it’s crucial to test multiple models and compare their performances. workflowsets is designed to manage multiple workflows, making it easier to compare different modeling approaches and preprocessing strategies.\nThis blog post introduces the option_add function of the workflowsets package, which is used to control options for evaluating workflow set functions such as fit_resamples and tune_grid. For more information on this function, refer to the documentation with ?option_add.\nWe start by loading the packages we will be using for this post\n\n\nShow the code\nlibrary(pacman)\np_load(tidyverse, tidymodels, gt, finetune, bonsai)\n\n\nFor this post we’ll use the heart disease dataset from kaggle.com. A preview of the data is given Table 1\n\n\nShow the code\nheart_disease &lt;- read_csv(\"heart_disease_dataset.csv\")\n\nhead(heart_disease) |&gt; \n  gt() |&gt; \n  tab_header(\n    title = \"Heart Diseases\"\n  ) |&gt; \n  opt_stylize(\n    style = 2, \n    color = \"cyan\"\n  ) |&gt; \n  as_raw_html()\n\n\n\n\nTable 1: Data Preview\n\n\n\n\n  \n  \n\n\n\nHeart Diseases\n\n\nAge\nGender\nCholesterol\nBlood Pressure\nHeart Rate\nSmoking\nAlcohol Intake\nExercise Hours\nFamily History\nDiabetes\nObesity\nStress Level\nBlood Sugar\nExercise Induced Angina\nChest Pain Type\nHeart Disease\n\n\n\n\n75\nFemale\n228\n119\n66\nCurrent\nHeavy\n1\nNo\nNo\nYes\n8\n119\nYes\nAtypical Angina\n1\n\n\n48\nMale\n204\n165\n62\nCurrent\nNone\n5\nNo\nNo\nNo\n9\n70\nYes\nTypical Angina\n0\n\n\n53\nMale\n234\n91\n67\nNever\nHeavy\n3\nYes\nNo\nYes\n5\n196\nYes\nAtypical Angina\n1\n\n\n69\nFemale\n192\n90\n72\nCurrent\nNone\n4\nNo\nYes\nNo\n7\n107\nYes\nNon-anginal Pain\n0\n\n\n62\nFemale\n172\n163\n93\nNever\nNone\n6\nNo\nYes\nNo\n2\n183\nYes\nAsymptomatic\n0\n\n\n77\nMale\n309\n110\n73\nNever\nNone\n0\nNo\nYes\nYes\n4\n122\nYes\nAsymptomatic\n1"
  },
  {
    "objectID": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#short-eda",
    "href": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#short-eda",
    "title": "Managing Workflowset Models",
    "section": "Short EDA",
    "text": "Short EDA\n\n\nShow the code\nskimr::skim_without_charts(heart_disease) |&gt; \n  gt() |&gt; \n  tab_spanner(\n    label = \"Character\",\n    columns = character.min:character.whitespace\n  ) |&gt; \n  tab_spanner(\n    label = \"Numeric\",\n    columns = starts_with(\"numeric\")\n  ) |&gt; \n  cols_label(\n    skim_type ~ \"Type\",\n    skim_variable ~\"Variable\",\n    n_missing ~ \"Missing?\",\n    complete_rate ~ \"Complete?\",\n    character.min ~ \"Min\",\n    character.max ~ \"Max\",\n    character.empty ~ \"Empty\",\n    character.n_unique ~ \"Unique\",\n    character.whitespace ~ \"Gap\",\n    numeric.mean ~ \"Mean\",\n    numeric.sd ~ \"SD\",\n    numeric.p0 ~ \"Min\",\n    numeric.p25 ~ \"25%\",\n    numeric.p50 ~ \"Median\",\n    numeric.p75 ~ \"75%\",\n    numeric.p100 ~ \"Max\"\n  ) |&gt; \n  cols_width(\n    skim_type ~ px(80),\n    everything() ~ px(70)\n  ) |&gt; \n  opt_stylize(\n    style = 2,\n    color = \"cyan\",\n  ) |&gt; \n  as_raw_html()\n\n\n\n  \n  \n\n\n\nType\nVariable\nMissing?\nComplete?\nCharacter\nNumeric\n\n\nMin\nMax\nEmpty\nUnique\nGap\nMean\nSD\nMin\n25%\nMedian\n75%\nMax\n\n\n\n\ncharacter\nGender\n0\n1\n4\n6\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nSmoking\n0\n1\n5\n7\n0\n3\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nAlcohol Intake\n0\n1\n4\n8\n0\n3\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nFamily History\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nDiabetes\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nObesity\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nExercise Induced Angina\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nChest Pain Type\n0\n1\n12\n16\n0\n4\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nAge\n0\n1\nNA\nNA\nNA\nNA\nNA\n52.293\n15.727126\n25\n39.00\n52.0\n66\n79\n\n\nnumeric\nCholesterol\n0\n1\nNA\nNA\nNA\nNA\nNA\n249.939\n57.914673\n150\n200.00\n248.0\n299\n349\n\n\nnumeric\nBlood Pressure\n0\n1\nNA\nNA\nNA\nNA\nNA\n135.281\n26.388300\n90\n112.75\n136.0\n159\n179\n\n\nnumeric\nHeart Rate\n0\n1\nNA\nNA\nNA\nNA\nNA\n79.204\n11.486092\n60\n70.00\n79.0\n89\n99\n\n\nnumeric\nExercise Hours\n0\n1\nNA\nNA\nNA\nNA\nNA\n4.529\n2.934241\n0\n2.00\n4.5\n7\n9\n\n\nnumeric\nStress Level\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.646\n2.831024\n1\n3.00\n6.0\n8\n10\n\n\nnumeric\nBlood Sugar\n0\n1\nNA\nNA\nNA\nNA\nNA\n134.941\n36.699624\n70\n104.00\n135.0\n167\n199\n\n\nnumeric\nHeart Disease\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.392\n0.488441\n0\n0.00\n0.0\n1\n1\n\n\n\n\n\n\n\n?@tbl-preview-data shows there are no missing values, so we can proceed with our analysis.\nNext, we will convert all character variables to factor data types\n\n\nShow the code\nheart_diseases &lt;- heart_disease |&gt; \n  janitor::clean_names() |&gt; \n  mutate(\n    across(where(is.character), factor),\n    exercise_hours = factor(exercise_hours),\n    stress_level = factor(stress_level),\n    heart_disease = factor(\n      heart_disease, \n      labels = c(\"No\",\"Yes\"),\n      levels = c(0, 1)\n    )\n  )\n\n\n\n\nShow the code\nGGally::ggscatmat(\n  data = heart_diseases,\n  columns = 1:ncol(heart_diseases),\n  color = \"heart_disease\",\n  alpha = .3\n)\n\n\n\n\n\n\n\n\nFigure 1: Scattered Matrix Plots of variables\n\n\n\n\n\n\n\nShow the code\nGGally::ggcorr(\n  data = heart_diseases,\n  columns = 1:ncol(heart_diseases),\n  name = expression(rho),\n  geom = \"circle\",\n  size = 3,\n  min_size = 5,\n  max_size = 10,\n  angle = -45\n) +\n  ggtitle(\"Correlation Plot of Numeric Variables\")\n\n\n\n\n\n\n\n\nFigure 2: Correlation plot of numeric variables\n\n\n\n\n\n\n\nShow the code\nheart_diseases |&gt; \n  ggplot(aes(heart_disease, fill = gender)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    x = \"Heart disease\",\n    y = \"Frequency\",\n    title = \"Heart disease a bit more prevalent in male than females\"\n  ) +\n  ggthemes::scale_fill_fivethirtyeight()\n\n\n\n\n\n\n\n\nFigure 3: Frequency of Heart Disease Outcome\n\n\n\n\n\nWe won’t spend time on EDA and proceed with our modeling workflow."
  },
  {
    "objectID": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#modeling",
    "href": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#modeling",
    "title": "Managing Workflowset Models",
    "section": "Modeling",
    "text": "Modeling\n\nData Splitting\nwe will split our data to 75% for training and 25% for testing, using the outcome variable (heart_disease) as the strata to ensure a balance split. Additionally, We will create validation folds to evaluate the models.\n\n\nShow the code\nset.seed(832)\nhd_split &lt;- initial_split(heart_diseases, prop = .75, strata = heart_disease)\n\nhd_train &lt;- training(hd_split)\nhd_folds &lt;- vfold_cv(hd_train)\n\nhead(hd_train) |&gt; \n  gt() |&gt; \n  opt_stylize(\n    style = 2,\n    color = \"cyan\"\n  ) |&gt; \n  as_raw_html()\n\n\n\n  \n  \n\n\n\nage\ngender\ncholesterol\nblood_pressure\nheart_rate\nsmoking\nalcohol_intake\nexercise_hours\nfamily_history\ndiabetes\nobesity\nstress_level\nblood_sugar\nexercise_induced_angina\nchest_pain_type\nheart_disease\n\n\n\n\n48\nMale\n204\n165\n62\nCurrent\nNone\n5\nNo\nNo\nNo\n9\n70\nYes\nTypical Angina\nNo\n\n\n62\nFemale\n172\n163\n93\nNever\nNone\n6\nNo\nYes\nNo\n2\n183\nYes\nAsymptomatic\nNo\n\n\n37\nFemale\n317\n137\n66\nCurrent\nHeavy\n3\nNo\nYes\nYes\n5\n114\nNo\nNon-anginal Pain\nNo\n\n\n43\nMale\n155\n169\n82\nCurrent\nHeavy\n8\nYes\nYes\nNo\n2\n163\nNo\nTypical Angina\nNo\n\n\n44\nFemale\n250\n111\n66\nFormer\nNone\n6\nYes\nNo\nYes\n3\n121\nYes\nNon-anginal Pain\nNo\n\n\n43\nFemale\n279\n173\n81\nCurrent\nModerate\n9\nYes\nNo\nNo\n7\n150\nNo\nAsymptomatic\nNo\n\n\n\n\n\n\n\n\n\nModel Specification\nWe will use two models for our analysis:\n\nK-nearest neighbors (KNN) model\nGeneralized linear model (GLM).\n\n\n\nShow the code\nknn_spec &lt;- nearest_neighbor(\n  neighbors = tune(),\n  weight_func = tune(),\n  dist_power = tune()\n) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\")\n\nglm_spec &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\", family = stats::binomial(link = \"logit\")) |&gt; \n  set_mode(\"classification\")\n\n\nBelow is the specification we have set for the KNN model:\n\n\nShow the code\nknn_spec |&gt;  translate()\n\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n  weight_func = tune()\n  dist_power = tune()\n\nComputational engine: kknn \n\nModel fit template:\nkknn::train.kknn(formula = missing_arg(), data = missing_arg(), \n    ks = min_rows(tune(), data, 5), kernel = tune(), distance = tune())\n\n\nThe KNN spec model is having three tuning parameters. For the GLM model we have the following:\n\n\nShow the code\nglm_spec |&gt; translate()\n\n\nLogistic Regression Model Specification (classification)\n\nEngine-Specific Arguments:\n  family = stats::binomial(link = \"logit\")\n\nComputational engine: glm \n\nModel fit template:\nstats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), \n    family = stats::binomial(link = \"logit\"))\n\n\nThe GLM specification is having no tuning parameter.\nAs seen in all the model specification above, the formula is missing. We’ll determine the formula for all models and the necessary preprocessing/feature engineering options we want to include in the next step using the recipe package\n\n\nData Preprocessing\nWe have three preprocessing specification. The first defines the formula which we will use, the second includes normalizing all numeric predictors, and the final preprocessing step involves creating dummy variables for our categorical variables.\n\n\nShow the code\nformula &lt;- recipe(\n  heart_disease ~ .,\n  data = hd_train\n)\n\nnormalize &lt;- formula |&gt; \n  step_normalize(all_numeric_predictors())\n\ndummy &lt;- normalize |&gt; \n  step_dummy(all_factor_predictors())\n\n\n\n\nShow the code\nnormalize |&gt; \n  prep() |&gt; \n  juice() |&gt; \n  head() |&gt; \n  gt() |&gt; \n  opt_stylize(\n    style = 3,\n    color = \"cyan\"\n  )\n\n\n\n\nTable 2: Preview of normalized preprocessed data\n\n\n\n\n\n\n\n\n\nage\ngender\ncholesterol\nblood_pressure\nheart_rate\nsmoking\nalcohol_intake\nexercise_hours\nfamily_history\ndiabetes\nobesity\nstress_level\nblood_sugar\nexercise_induced_angina\nchest_pain_type\nheart_disease\n\n\n\n\n-0.2983420\nMale\n-0.784268085\n1.13748976\n-1.4962849\nCurrent\nNone\n5\nNo\nNo\nNo\n9\n-1.7689353\nYes\nTypical Angina\nNo\n\n\n0.6020865\nFemale\n-1.332244271\n1.06281212\n1.2243394\nNever\nNone\n6\nNo\nYes\nNo\n2\n1.3197224\nYes\nAsymptomatic\nNo\n\n\n-1.0058214\nFemale\n1.150772824\n0.09200285\n-1.1452366\nCurrent\nHeavy\n3\nNo\nYes\nYes\n5\n-0.5662721\nNo\nNon-anginal Pain\nNo\n\n\n-0.6199235\nMale\n-1.623356620\n1.28684503\n0.2589566\nCurrent\nHeavy\n8\nYes\nYes\nNo\n2\n0.7730573\nNo\nTypical Angina\nNo\n\n\n-0.5556072\nFemale\n0.003447684\n-0.87880642\n-1.1452366\nFormer\nNone\n6\nYes\nNo\nYes\n3\n-0.3749394\nYes\nNon-anginal Pain\nNo\n\n\n-0.6199235\nFemale\n0.500051103\n1.43620030\n0.1711946\nCurrent\nModerate\n9\nYes\nNo\nNo\n7\n0.4177250\nNo\nAsymptomatic\nNo\n\n\n\n\n\n\n\n\n\n\nTable 2 previews how the data looks after normalizing, which is the second feature engineering technique. Table 3 shows the data after creating dummy variables categorical variables.\n\n\nShow the code\ndummy |&gt; \n  prep() |&gt; \n  juice() |&gt; \n  head() |&gt; \n  gt() |&gt; \n  opt_stylize(\n    style = 2,\n    color = \"cyan\"\n  ) |&gt; \n  as_raw_html()\n\n\n\n\nTable 3: Preview of dummy + normalized preprocessed data\n\n\n\n\n  \n  \n\n\n\nage\ncholesterol\nblood_pressure\nheart_rate\nblood_sugar\nheart_disease\ngender_Male\nsmoking_Former\nsmoking_Never\nalcohol_intake_Moderate\nalcohol_intake_None\nexercise_hours_X1\nexercise_hours_X2\nexercise_hours_X3\nexercise_hours_X4\nexercise_hours_X5\nexercise_hours_X6\nexercise_hours_X7\nexercise_hours_X8\nexercise_hours_X9\nfamily_history_Yes\ndiabetes_Yes\nobesity_Yes\nstress_level_X2\nstress_level_X3\nstress_level_X4\nstress_level_X5\nstress_level_X6\nstress_level_X7\nstress_level_X8\nstress_level_X9\nstress_level_X10\nexercise_induced_angina_Yes\nchest_pain_type_Atypical.Angina\nchest_pain_type_Non.anginal.Pain\nchest_pain_type_Typical.Angina\n\n\n\n\n-0.2983420\n-0.784268085\n1.13748976\n-1.4962849\n-1.7689353\nNo\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n\n\n0.6020865\n-1.332244271\n1.06281212\n1.2243394\n1.3197224\nNo\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n-1.0058214\n1.150772824\n0.09200285\n-1.1452366\n-0.5662721\nNo\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n-0.6199235\n-1.623356620\n1.28684503\n0.2589566\n0.7730573\nNo\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n-0.5556072\n0.003447684\n-0.87880642\n-1.1452366\n-0.3749394\nNo\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n-0.6199235\n0.500051103\n1.43620030\n0.1711946\n0.4177250\nNo\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\nModel Workflow Set\n\n\nShow the code\nhd_wf_set&lt;- workflow_set(\n  preproc = list(\n    form = formula,\n    norm = normalize,\n    dum = dummy\n  ),\n  models = list(\n    glm = glm_spec,\n    knn = knn_spec\n  )\n)\n\n\n\n\nTuning Parameter\nUsing the workflowset function, we’ve tied three recipe objects to the three different models. The K-nearest neighbor model needs tuning as mentioned earlier.\n\n\nShow the code\nset.seed(34443)\n\nknn_grid &lt;- knn_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_regular(levels = 6)\n\nknn_latin &lt;- knn_spec |&gt; \n  extract_parameter_set_dials() |&gt; \n  grid_latin_hypercube(size = 300)\n\ngrid_control &lt;- control_race(\n  save_pred = TRUE,\n  save_workflow = TRUE\n)\n\nknn_grid |&gt; \n  ggplot(aes(dist_power, neighbors, col = weight_func)) +\n  geom_point() +\n  ggthemes::scale_color_colorblind() +\n  labs(\n    x = \"Minkowski distance\",\n    y = \"Number of Neighbors\",\n    title = \"k-NN Regular Grid\"\n  ) +\n  facet_wrap(~weight_func) +\n  theme(\n    legend.position = \"none\"\n  )\n  \nknn_latin |&gt;\n  ggplot(aes(dist_power, neighbors, col = weight_func)) +\n  geom_point() +\n  ggthemes::scale_color_tableau() +\n  labs(\n    x = \"Minkowski distance\",\n    y = \"Number of Neighbors\",\n    title = \"k-NN Latin Hypercube Grid\"\n  ) +\n  facet_wrap(~weight_func) +\n  theme(\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n(a) - knn regular tune grid - knn latin hypercube tune grid\n\n\n\n\n\n\n\n\n\n\n\n(b) - knn regular tune grid - knn latin hypercube tune grid\n\n\n\n\n\n\nFigure 4: Tuning grids to be used for K-nearest neighbor model specification\n\n\n\n\nWe set the tuning grid for the model and use the option_add function to specify it. We will test two different grid structures as shown in Figure 4."
  },
  {
    "objectID": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#using-option_add-to-specify-model-grids",
    "href": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#using-option_add-to-specify-model-grids",
    "title": "Managing Workflowset Models",
    "section": "Using option_add to Specify Model Grids",
    "text": "Using option_add to Specify Model Grids\nWe can specify the grid to use for each model using the option_add function. Below is an image of hd_wf_set that we defined recently, and we will interpret its output.\n\n\n\nDefined workflowset output\n\n\nThe image above shows that option column is having zero values as well as the results column.\n\n\nShow the code\nhd_tune &lt;- hd_wf_set |&gt; \n  option_add(\n    id = \"norm_knn\",\n    grid = knn_grid,\n    control = grid_control\n  ) |&gt; \n  option_add(\n    id = \"form_knn\",\n    grid = knn_grid,\n    control = grid_control\n  ) |&gt; \n  option_add(\n    id = \"norm_knn\",\n    grid = knn_latin,\n    control = grid_control\n  ) |&gt; \n  option_add(\n    id = \"form_knn\",\n    grid = knn_latin,\n    control = grid_control\n  ) |&gt; \n  option_add(\n    id = \"dum_knn\",\n    grid = knn_grid,\n    control = grid_control\n  ) |&gt; \n  option_add(\n    id = \"dum_knn\",\n    grid = knn_latin,\n    control = grid_control\n  )\n\n\n\n\n\nDefined workflowset output after options are added\n\n\nAfter using the option-add function, we can see that KNN model specification have two options added to it. We can now proceed to tune our model.\n\n\nShow the code\ndoParallel::registerDoParallel(cores = 6)\n\nhd_tune_res &lt;- workflow_map(\n hd_tune ,\n fn = \"tune_race_anova\",\n resamples = hd_folds,\n seed = 3343,\n verbose = TRUE\n)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 1 of 6 resampling: form_glm\n\n\n✔ 1 of 6 resampling: form_glm (510ms)\n\n\ni 2 of 6 tuning:     form_knn\n\n\n✔ 2 of 6 tuning:     form_knn (1m 39.3s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 3 of 6 resampling: norm_glm\n\n\n✔ 3 of 6 resampling: norm_glm (514ms)\n\n\ni 4 of 6 tuning:     norm_knn\n\n\n✔ 4 of 6 tuning:     norm_knn (1m 46.5s)\n\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 5 of 6 resampling: dum_glm\n\n\n✔ 5 of 6 resampling: dum_glm (646ms)\n\n\ni 6 of 6 tuning:     dum_knn\n\n\n✔ 6 of 6 tuning:     dum_knn (2m 30.6s)"
  },
  {
    "objectID": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#tune-result",
    "href": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#tune-result",
    "title": "Managing Workflowset Models",
    "section": "Tune Result",
    "text": "Tune Result\n\n\nShow the code\nautoplot(hd_tune_res)\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\nShow the code\nhd_tune_res |&gt; \n  rank_results(rank_metric = \"accuracy\") |&gt; \n  filter(.metric == \"accuracy\") |&gt; \n  select(-c(.metric,  preprocessor, model, n)) |&gt; \n  gt() |&gt; \n  cols_label(\n    wflow_id = \"Model ID\",\n    .config = \"Model Number\"\n  ) |&gt; \n  opt_stylize(\n    style = 2,\n    color = \"cyan\"\n  ) |&gt; \n  as_raw_html()\n\n\n\n  \n  \n\n\n\nModel ID\nModel Number\nmean\nstd_err\nrank\n\n\n\n\nform_knn\nPreprocessor1_Model236\n0.8653333\n0.013546445\n1\n\n\nnorm_knn\nPreprocessor1_Model236\n0.8653333\n0.013546445\n2\n\n\nform_glm\nPreprocessor1_Model1\n0.8613333\n0.013799266\n3\n\n\nnorm_glm\nPreprocessor1_Model1\n0.8613333\n0.013799266\n4\n\n\ndum_glm\nPreprocessor1_Model1\n0.8613333\n0.013799266\n5\n\n\nnorm_knn\nPreprocessor1_Model134\n0.8520000\n0.012000000\n6\n\n\nform_knn\nPreprocessor1_Model134\n0.8520000\n0.012000000\n7\n\n\ndum_knn\nPreprocessor1_Model134\n0.7426667\n0.008899993\n8\n\n\ndum_knn\nPreprocessor1_Model137\n0.7413333\n0.009573626\n9\n\n\ndum_knn\nPreprocessor1_Model129\n0.7386667\n0.011958777\n10\n\n\ndum_knn\nPreprocessor1_Model103\n0.7360000\n0.010850272\n11\n\n\ndum_knn\nPreprocessor1_Model222\n0.7333333\n0.015267168\n12\n\n\ndum_knn\nPreprocessor1_Model106\n0.7333333\n0.008663817\n13\n\n\ndum_knn\nPreprocessor1_Model221\n0.7226667\n0.013303671\n14\n\n\n\n\n\n\n\nBased on the results, it appears that the KNN model with no preprocessing is the best performing model."
  },
  {
    "objectID": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#conclusion",
    "href": "posts/option-add-use-in-tuning-different-grid-for-different-models/index.html#conclusion",
    "title": "Managing Workflowset Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe success of our KNN model, particularly with preprocessing, underscores the critical role of the option_add function. By utilizing option_add, we efficiently defined and refined our model’s tuning grid, allowing us to systematically explore and optimize hyperparameters. This approach not only enhances model performance but also ensures robustness and reliability in our predictive analytics pipeline."
  },
  {
    "objectID": "posts/gef-projects/index.html",
    "href": "posts/gef-projects/index.html",
    "title": "What about our Climate Funds",
    "section": "",
    "text": "The Global Environment Facility (GEF), a partnership between the World Bank, the United Nations Environment Programme (UNEP), and the United Nations Development Programme (UNDP), plays a critical role in empowering developing countries to tackle pressing environmental issues. By providing both financial resources and technical expertise, the GEF supports these nations in implementing sustainable development practices that benefit the planet."
  },
  {
    "objectID": "posts/gef-projects/index.html#introduction",
    "href": "posts/gef-projects/index.html#introduction",
    "title": "What about our Climate Funds",
    "section": "",
    "text": "The Global Environment Facility (GEF), a partnership between the World Bank, the United Nations Environment Programme (UNEP), and the United Nations Development Programme (UNDP), plays a critical role in empowering developing countries to tackle pressing environmental issues. By providing both financial resources and technical expertise, the GEF supports these nations in implementing sustainable development practices that benefit the planet."
  },
  {
    "objectID": "posts/gef-projects/index.html#aim",
    "href": "posts/gef-projects/index.html#aim",
    "title": "What about our Climate Funds",
    "section": "Aim",
    "text": "Aim\nThis analysis delves into the GEF’s work, aiming to uncover insights through exploratory data analysis. We’ll explore key aspects like:\n\nIdentifying GEF Agencies: This will involve pinpointing the different entities involved within the GEF’s structure.\nFunding Trends: We’ll analyze trends in the total funds associated with the GEF, revealing how resources have evolved over time.\nGEF Agency Contributions: This analysis will investigate the financial contributions of each agency to the GEF’s mission.\nFocus Area Spending: We’ll assess how much funding has been allocated to the GEF’s core areas of focus (e.g., climate change, biodiversity).\nTop Recipient Countries: This exploration will identify the countries receiving the highest total project funding.\nContinental Funding Distribution: We’ll examine the top 3 funded countries within each continent, providing a more granular perspective.\nCapacity Building Investment: We’ll estimate the resources invested in building the capacity of developing countries to address environmental challenges.\nProject Status: This analysis will categorize projects based on their completion status (cancelled, approved, completed), revealing project success rates.\nProject Spending by Size and Stage: We’ll investigate how funding is distributed across projects of different sizes and stages (e.g., enabling activity and so on).\nTotal Funds Per GEF Replenishment Period: This analysis will explore how much funding was available during each GEF replenishment cycle. The GEF operates on a cycle where donor countries pledge contributions every four years. Examining trends in total funds across these periods can reveal changes in donor commitment and resource availability for the GEF’s work.\nPinpoint the Single Most Funded Project: This investigation will identify the individual project that has garnered the highest total funding from the GEF."
  },
  {
    "objectID": "posts/gef-projects/index.html#explore-data",
    "href": "posts/gef-projects/index.html#explore-data",
    "title": "What about our Climate Funds",
    "section": "Explore Data",
    "text": "Explore Data\nLet’s start by loading the data.\n\n\nShow the code\nlibrary(pacman)\np_load(\n  tidyverse, janitor, gt, countrycode, scales, ggimage, ggthemes,\n  ggtext, magick, ggtextures, gtExtras\n)\n\ntheme_set(theme_hc() +\n  theme(\n    axis.title.y = element_text(angle = 90),\n    plot.title = element_text(face = \"bold\", size = 15),\n  )\n)\n\n\n\n\nShow the code\ngef &lt;- read_csv(\"projects.csv\") |&gt; clean_names() |&gt; \n  filter(approval_fy &gt;= 1991)\n\ncol_names &lt;- str_to_upper(str_replace_all(names(gef), \"_\", \" \"))\n\nhead(gef, n = 1) |&gt; \n  gt() |&gt; \n  cols_label(\n    title = col_names[1],\n    id = col_names[2],\n    countries = col_names[3],\n    focal_areas = col_names[4],\n    type = col_names[5],\n    agencies = col_names[6],\n    gef_grant = col_names[7],\n    cofinancing = col_names[8],\n    status = col_names[9],\n    approval_fy = col_names[10],\n    funding_source_indexed_field = col_names[11],\n    non_grant_instrument_indexed_field = col_names[12],\n    capacity_building_initiative_for_transparency = col_names[13],\n    gef_period = col_names[14]\n  ) |&gt; \n  tab_header(md(\"**GEF Data Preview**\")) |&gt; \n  fmt_currency(columns = gef_grant, currency = \"USD\") |&gt; \n  tab_style(\n    style = cell_text(size = px(12)),\n    locations = cells_body(columns = everything())\n  ) |&gt; \n  tab_style(\n    style = cell_text(size = px(14)),\n    locations = cells_column_labels(columns = everything())\n  ) |&gt; \n  cols_width(\n    title ~ px(250),\n    agencies ~ px(260),\n    everything() ~ px(50)\n  ) |&gt; \n  gt_theme_538()\n\n\n\n\n\n\n\n\nGEF Data Preview\n\n\nTITLE\nID\nCOUNTRIES\nFOCAL AREAS\nTYPE\nAGENCIES\nGEF GRANT\nCOFINANCING\nSTATUS\nAPPROVAL FY\nFUNDING SOURCE INDEXED FIELD\nNON GRANT INSTRUMENT INDEXED FIELD\nCAPACITY BUILDING INITIATIVE FOR TRANSPARENCY\nGEF PERIOD\n\n\n\n\nFirst and Second Biennial Transparency Report and Fifth National Communication (1BTR + 5NC & 2BTR)\n11649\nTogo\nClimate Change\nEnabling Activity\nUnited Nations Development Programme\n$1,233,000.00\nNA\nProject Approved\n2024\nGEF Trust Fund\nNo\nNo\nGEF - 8"
  },
  {
    "objectID": "posts/gef-projects/index.html#agencies-supporting-the-gef",
    "href": "posts/gef-projects/index.html#agencies-supporting-the-gef",
    "title": "What about our Climate Funds",
    "section": "Agencies Supporting the GEF",
    "text": "Agencies Supporting the GEF\n\n\nShow the code\ngef &lt;- gef |&gt; \n  select(-c(id, non_grant_instrument_indexed_field))\n\nicons &lt;- list.files(\"agencies\", full.names = TRUE)\nagencies_abr &lt;- c(\n  \"AFDB\", \"ADB\", \"BBF\", \"Con Int\", \"DBLA\", \"DBSA\",\n  \"EBRD\", \"FAO\",\"FECO\", \"GEF\", \"IADB\", \"IFC\", \"IFAD\",\n  \"IUCN\", \"MEPC\", \"WB\", \"UNDP\", \"UNEP\", \"UNIDO\",\n  \"WADB\", \"WWF\"\n)\n\nagencies &lt;- gef |&gt; \n  select(agencies) |&gt; \n  separate_longer_delim(agencies, delim = \",\") |&gt; \n  mutate(\n    agencies = str_trim(agencies)\n  ) |&gt; \n  distinct() |&gt; \n  arrange(agencies) \n\nagencies &lt;- agencies |&gt;  \n  bind_cols(list(agencies_abr, icons)) |&gt; \n  set_names(c(\"agencies\", \"abbr\", \"logo\"))\n  \nagencies |&gt; \n  relocate(logo) |&gt; \n  gt() |&gt; \n  cols_label(\n    agencies = \"Partners\",\n    abbr = \"Abbreviation\",\n    logo = \"\"\n  ) |&gt; \n  tab_header(\n    title = \"AGENCIES SUPPORTING GEF\"\n  ) |&gt; \n  text_transform(\n    fn = function(x){\n      local_image(\n        filename = icons,\n        height = 50\n      )\n    },\n    locations = cells_body(\n      columns = logo\n    )\n  ) |&gt; \n  cols_align(\n    columns = logo,\n    align = \"center\"\n  ) |&gt; \n  gt_theme_538()\n\n\n\n\nTable 1: Partner organizations (Agencies) of the Global Environment Facility\n\n\n\n\n\n\n\n\n\nAGENCIES SUPPORTING GEF\n\n\n\nPartners\nAbbreviation\n\n\n\n\n\nAfrican Development Bank\nAFDB\n\n\n\nAsian Development Bank\nADB\n\n\n\nBrazilian Biodiversity Fund\nBBF\n\n\n\nConservation International\nCon Int\n\n\n\nDevelopment Bank of Latin America\nDBLA\n\n\n\nDevelopment Bank of Southern Africa\nDBSA\n\n\n\nEuropean Bank for Reconstruction and Development\nEBRD\n\n\n\nFood and Agriculture Organization\nFAO\n\n\n\nForeign Economic Cooperation Office\nFECO\n\n\n\nGEF Secretariat\nGEF\n\n\n\nInter-American Development Bank\nIADB\n\n\n\nInternational Finance Corporation\nIFC\n\n\n\nInternational Fund for Agricultural Development\nIFAD\n\n\n\nInternational Union for Conservation of Nature\nIUCN\n\n\n\nMinistry of Environmental Protection of China\nMEPC\n\n\n\nThe World Bank\nWB\n\n\n\nUnited Nations Development Programme\nUNDP\n\n\n\nUnited Nations Environment Programme\nUNEP\n\n\n\nUnited Nations Industrial Development Organization\nUNIDO\n\n\n\nWest African Development Bank\nWADB\n\n\n\nWorld Wildlife Fund - US Chapter\nWWF"
  },
  {
    "objectID": "posts/gef-projects/index.html#trend-of-gef-funds-since-establishment-in-1991",
    "href": "posts/gef-projects/index.html#trend-of-gef-funds-since-establishment-in-1991",
    "title": "What about our Climate Funds",
    "section": "Trend of GEF Funds since establishment in 1991",
    "text": "Trend of GEF Funds since establishment in 1991\n\n\nShow the code\ngef |&gt; \n  summarize(\n    .by = c(countries, approval_fy),\n    gef_grant = sum(gef_grant, na.rm = TRUE),\n    cofinancing = sum(cofinancing, na.rm = TRUE)\n  ) |&gt; \n  pivot_longer(\n    cols = gef_grant:cofinancing,\n    names_to = \"fund_type\",\n    values_to = \"amount\"\n  ) |&gt; \n  filter(amount &gt; 0) |&gt; \n  summarize(\n     .by = c(fund_type, approval_fy),\n     amount = sum(amount)\n  ) |&gt; \n  mutate(\n    fund_type = case_when(\n      fund_type == \"gef_grant\" ~ \"GEF\",\n      fund_type == \"cofinancing\" ~ \"Other Institutions\"\n    )\n  ) |&gt; \n  ggplot(aes(approval_fy, amount/1e6, col = fund_type, fill = fund_type)) +\n  geom_line(width = .5) +\n    geom_rect(\n    aes(xmin = 2019, xmax = 2020, ymin = 0, ymax = 11e3),\n    fill = \"gray\", size = .01, alpha = .2\n  ) +\n  geom_area(\n    position = \"dodge\",\n    alpha = .5\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Funding Amount in Million Dollars\",\n    fill = \"Funded By\",\n    col = \"Funded By\",\n    title = \"Funding Contribution from GEF and Other Bodies (1991 - 2024)\",\n    caption = \"By: Olamide Michael Adu\"\n  ) +\n  geom_vline(\n    xintercept = 1991,\n    linewidth = .3,\n    col = \"orange1\"\n  ) +\n  geom_hline(\n    yintercept = 6e3,\n    linewidth = .3,\n    col = \"orange1\"\n  ) +\n  geom_label(\n    aes(x = 1995, y = 6e3, label = \"GEF Established\"),\n    col = \"white\",\n    fill = \"tomato1\"\n  ) +\n  geom_label(\n    aes(x = 2019, y = 9e3, label = \"Covid 19\"),\n    col = \"white\",\n    fill = \"gray\"\n  ) +\n  scale_x_continuous(breaks = seq(1991, 2024, 4)) +\n  scale_y_continuous(labels = label_dollar()) +\n  scale_fill_economist() +\n  scale_color_economist() +\n  theme(\n    axis.title.y= element_text(\n      vjust = 7,\n      size = 12,\n      margin = margin(t = 0, r = 0, l = 10, b = 0)\n    ),\n    plot.margin = unit(c(.5, .5, .5, .5), \"cm\")\n  )\n\n\n\n\n\n\n\n\nFigure 1: countries with the highest fundings\n\n\n\n\n\n\nWho is responsible for financing most projects?\n\n\nShow the code\nmy_icons &lt;- tibble(\n  icons = icons,\n  agencies_abr = agencies_abr\n)\n\nmy_icons &lt;- my_icons |&gt; \n  mutate(\n    icons = paste0(\"&lt;img src =\", icons, \" width = '25'/&gt;\\ &lt;br&gt;**\", agencies_abr, \"**\")\n  )\n\ngef |&gt; \n  mutate(\n    agencies = case_when(\n      str_detect(agencies, \",\") ~ \"Multiple Organizations\",\n      .default = agencies\n    )\n  ) |&gt; \n  summarize(\n    .by = agencies,\n    fund_amount = sum(cofinancing, na.rm = TRUE)\n  ) |&gt; \n  mutate(\n    fund_amount = round(fund_amount/1e9, 1)\n  ) |&gt; \n  filter(agencies != \"Multiple Organizations\") |&gt; \n  arrange(agencies) |&gt; \n  bind_cols(my_icons[-c(9, 11, 15), ]) |&gt; # agencies not among the list removed\n  slice_max(fund_amount, n = 10) |&gt; \n  ggplot(aes(fund_amount, fct_reorder(icons, fund_amount))) +\n  geom_col(fill = \"springgreen4\") +\n  geom_label(\n    aes(label = paste0(\"$\", fund_amount, \" B\")),\n    fill = \"burlywood1\", \n    col = \"gray2\"\n  ) +\n  geom_image(aes(x = 33, y = 3, image = \"money.jpg\"), size = .7) +\n  labs(\n    title =\"Green Giants: Top Funders Backing the GEF\",\n    subtitle = \"The Key Player Behind Global Environmental Action (amount in Billions)\",\n    caption = \"By: Olamide Michael Adu\"\n  ) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text.y = element_markdown(size = 9, color = \"black\"),\n    plot.subtitle = element_text(size = 11, color = \"burlywood\"),\n    plot.title = element_text(vjust = 1, hjust = -.1),\n    plot.margin = unit(c(.5, 1, .5, 1), \"cm\")\n  ) \n\n\n\n\n\n\n\n\nFigure 2: Top Environmental Players\n\n\n\n\n\nFigure 2 shows that the World Bank Group and UNDP has been the top cofinancer of GEF projects."
  },
  {
    "objectID": "posts/gef-projects/index.html#what-is-the-main-areas-that-gef-funds-go-to",
    "href": "posts/gef-projects/index.html#what-is-the-main-areas-that-gef-funds-go-to",
    "title": "What about our Climate Funds",
    "section": "What is the Main Areas That GEF Funds go to?",
    "text": "What is the Main Areas That GEF Funds go to?\n\n\nShow the code\nimage &lt;- tibble(\n  image = list(\n  image_read_svg(\"focal_areas/biodiversity.svg\"),\n  image_read_svg(\"focal_areas/chemicals_and_waste.svg\"),\n  image_read_svg(\"focal_areas/climate_change.svg\"),\n  image_read_svg(\"focal_areas/international_waters.svg\"),\n  image_read_svg(\"focal_areas/land_degradation.svg\")\n  )\n)\n\n\ngef |&gt;\n  select(gef_grant, cofinancing, focal_areas, approval_fy) |&gt; \n  replace_na(\n    list(\n      gef_grant = 0,\n      cofinancing = 0\n    )\n  ) |&gt; \n  drop_na(focal_areas) |&gt; \n  mutate(\n    total_amount = gef_grant + cofinancing,\n    .keep = \"unused\"\n  ) |&gt; \n  summarize(\n    .by = focal_areas,\n    fund_amount = sum(total_amount)\n  ) |&gt; \n  separate_longer_delim(\n    focal_areas,\n    delim = \",\"\n  ) |&gt; \n  mutate(\n    focal_areas = str_trim(focal_areas),\n    .by = fund_amount,\n    total_amount = fund_amount/n(),\n    total_amount = total_amount/1e9 # Change to billions\n  ) |&gt; \n  summarize(\n   .by = focal_areas,\n   total_amount = mean(total_amount)\n  ) |&gt; \n  arrange(focal_areas) |&gt; \n  bind_cols(image) |&gt; \n  ggplot(aes(fct_reorder(focal_areas, total_amount), total_amount, ,image = image)) +\n  geom_col(fill = \"springgreen\", alpha = .5, col = \"black\") +\n  geom_isotype_col(\n    img_height = grid::unit(1.2, \"cm\"),\n    img_width = grid::unit(1, \"cm\"),\n    ncol = 1, nrow = 1,\n    hjust = 1, vjust = .5\n  ) +\n  scale_y_continuous(breaks = seq(0, 6, 1)) +\n  labs(\n    x = \"Key Areas\",\n    y = \"Amount Invested ($ Billions)\",\n    title = \"Shifting Focus: How GEF Investment\",\n    subtitle = \"GEF Strategic Prioritization in Different Environmental Areas\",\n    caption = \"By: Olamide Michael Adu\"\n  ) +\n  coord_flip() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = -.7),\n    plot.subtitle = element_text(hjust = -1.12),\n  )\n\n\n\n\n\n\n\n\nFigure 3: The GEF focus areas spending"
  },
  {
    "objectID": "posts/gef-projects/index.html#top-funded-countries",
    "href": "posts/gef-projects/index.html#top-funded-countries",
    "title": "What about our Climate Funds",
    "section": "Top funded countries",
    "text": "Top funded countries\n\n\nShow the code\nall_funds &lt;- gef |&gt; \n  select(gef_grant, countries, cofinancing) |&gt; \n  replace_na(\n    list(\n      gef_grant = 0,\n      cofinancing = 0\n    )\n  ) |&gt; \n  mutate(\n    total_amount = gef_grant + cofinancing,\n    .keep = \"unused\"\n  ) |&gt; \n  summarize(\n    .by = c(countries),\n    total_amount = sum(total_amount)\n  ) |&gt; \n  separate_longer_delim(\n    cols = countries,\n    delim = \",\"\n  ) |&gt; \n  mutate( # This block of code is needed to find the average for countries which have been\n    countries = str_trim(countries), # grouped together during a funding round\n    .by = total_amount,\n    fund_amount = total_amount/n(),\n  ) |&gt; \n  summarize(\n    .by = c(countries),\n    total_amount = sum(fund_amount)\n  ) |&gt; \n  arrange(countries)\n\nfund_countries &lt;- all_funds |&gt; \n  filter(\n    !countries %in% c(\"Global\", \"Africa\", \"Asia/Pacific\",\n                      \"Europe and Central Asia\", \"Latin America and Caribbean\",\n                      \"Regional\"\n                      )\n  )\n\nfund_region &lt;- all_funds |&gt; \n  filter(\n    countries %in% c(\"Global\", \"Africa\", \"Asia/Pacific\",\n                      \"Europe and Central Asia\", \"Latin America and Caribbean\"\n                     )\n  )\n\n\n\n\nShow the code\ncountries &lt;- list.files(path = \"countries/svg\", full.names = TRUE)\ncountry_logo = tibble(logo = countries[str_detect(countries, \"ch|in|me|br|ph|vn|id|za|/ng|pe\")])\n\n\n\n\nShow the code\nfund_countries |&gt; \n  slice_max(total_amount, n = 10) |&gt;  \n  arrange(countries) |&gt; \n  bind_cols(country_logo) |&gt; \n  mutate( # This block ensures countries matches their logo by replacing them with abbr\n    logo = case_when(\n      str_detect(logo, \"id\") ~ str_replace(logo, \"id\", \"in\"),\n      str_detect(logo, \"in\") ~ str_replace(logo, \"in\", \"id\"),\n      str_detect(logo, \"vn\") ~ str_replace(logo, \"vn\", \"za\"),\n      str_detect(logo, \"za\") ~ str_replace(logo, \"za\", \"vn\"),\n      .default = logo\n    )\n  ) |&gt; \n  relocate(logo, .before = countries) |&gt; \n  arrange(desc(total_amount)) |&gt; \n  mutate(total_amount = round(total_amount/1e9, 2)) |&gt; \n  gt() |&gt; \n  cols_label(\n    logo = \"\",\n    countries = \"Country\",\n    total_amount = \"Funds (Billion)\"\n  ) |&gt; \n  fmt_image(\n    columns = logo, width = 30, height = 30\n  ) |&gt; \n  fmt_currency(\n    columns = total_amount\n  ) |&gt; \n  tab_header(\n    title = \"Top Funded countries with involving the GEF\",\n    subtitle = \"Funds can be by GEF, National Government and other interested parties\"\n  ) |&gt; \n  gt_theme_538()\n\n\n\n\nTable 2: Top 10 most funded countries\n\n\n\n\n\n\n\n\n\nTop Funded countries with involving the GEF\n\n\nFunds can be by GEF, National Government and other interested parties\n\n\n\nCountry\nFunds (Billion)\n\n\n\n\n\nChina\n$18.70\n\n\n\nIndia\n$7.23\n\n\n\nMexico\n$4.58\n\n\n\nBrazil\n$4.15\n\n\n\nPhilippines\n$3.85\n\n\n\nViet Nam\n$2.87\n\n\n\nIndonesia\n$2.83\n\n\n\nSouth Africa\n$2.43\n\n\n\nNigeria\n$2.19\n\n\n\nPeru\n$2.08"
  },
  {
    "objectID": "posts/gef-projects/index.html#most-funded-countries-in-each-continent",
    "href": "posts/gef-projects/index.html#most-funded-countries-in-each-continent",
    "title": "What about our Climate Funds",
    "section": "Most Funded Countries in Each Continent",
    "text": "Most Funded Countries in Each Continent\n\n\nShow the code\ncontinent &lt;- codelist |&gt; \n  select(continent, country.name.en)\n\nfund_countries &lt;- fund_countries |&gt; \n  left_join(continent, join_by(countries == country.name.en))\n  \neurope &lt;- c(\"Bosnia-Herzegovina\", \"Czech Republic\", \"Kosovo\",\n            \"Russian Federation\", \"Slovak Republic\", \"Türkiye\")\nafrica &lt;- c(\"Cabo Verde\", \"Congo\", \"Congo DR\", \"Cote d'Ivoire\",\n            \"Sao Tome and Principe\")\nasia &lt;- c(\"Korea DPR\", \"Kyrgyz Republic\", \"Lao PDR\", \"Myanmar\", \n          \"Palestinian Authority\", \"Republic Of Korea\",\n          \"Republic Of Korea\", \"Viet Nam\")\namericas &lt;- c(\"Antigua And Barbuda\", \"St. Kitts And Nevis\",\n              \"St. Vincent and Grenadines\", \"Trinidad and Tobago\")\noceania &lt;- c(\"Timor Leste\", \"Micronesia\")\n\nfund_countries &lt;- fund_countries |&gt; \n  filter(countries != \"Yugoslavia\") |&gt; \n  mutate(\n    continent = case_when(\n      countries %in% europe ~ \"Europe\",\n      countries %in% africa ~ \"Africa\",\n      countries %in% asia ~ \"Asia\",\n      countries %in% americas ~ \"Americas\",\n      countries %in% oceania ~ \"Oceania\",\n      .default = continent\n    )\n  )\n\n\n\n\nShow the code\nfund_countries |&gt; \n  group_by(continent) |&gt; \n  slice_max(total_amount, n = 3) |&gt; \n  ungroup() |&gt; \n  mutate(\n    continent = str_to_upper(continent),\n    total_amount = round(total_amount/1e6, 2)\n  ) |&gt; \n  gt(groupname_col = \"continent\") |&gt; \n  tab_header(\n    title = \"Top Funded GEF (Co)Financed Countries per Continent\"\n  ) |&gt; \n   cols_label(\n     countries = \"Country\",\n     total_amount = \" Funds Received (millions)\"\n  ) |&gt; \n  fmt_currency(\n    columns = total_amount\n  ) |&gt; \n  gt_theme_538()\n\n\n\n\nTable 3: Top Funded Countries Per Continent\n\n\n\n\n\n\n\n\n\nTop Funded GEF (Co)Financed Countries per Continent\n\n\nCountry\nFunds Received (millions)\n\n\n\n\nAFRICA\n\n\nSouth Africa\n$2,427.83\n\n\nNigeria\n$2,192.91\n\n\nEgypt\n$1,989.42\n\n\nAMERICAS\n\n\nMexico\n$4,582.48\n\n\nBrazil\n$4,146.98\n\n\nPeru\n$2,080.64\n\n\nASIA\n\n\nChina\n$18,700.05\n\n\nIndia\n$7,231.04\n\n\nPhilippines\n$3,851.19\n\n\nEUROPE\n\n\nRussian Federation\n$1,807.13\n\n\nTürkiye\n$1,505.62\n\n\nUkraine\n$937.76\n\n\nOCEANIA\n\n\nTimor Leste\n$527.64\n\n\nPapua New Guinea\n$473.85\n\n\nSolomon Islands\n$424.47"
  },
  {
    "objectID": "posts/gef-projects/index.html#what-about-capacity-building",
    "href": "posts/gef-projects/index.html#what-about-capacity-building",
    "title": "What about our Climate Funds",
    "section": "What about Capacity Building",
    "text": "What about Capacity Building\n\n\nShow the code\ngef |&gt; \n  filter(capacity_building_initiative_for_transparency != \"No\") |&gt; \n  select(capacity_building_initiative_for_transparency, approval_fy,\n         \"funding_source\" = funding_source_indexed_field, cofinancing, gef_grant) |&gt; \n  replace_na(\n    list(\n      gef_grant = 0,\n      cofinancing = 0\n    )\n  ) |&gt; \n  mutate(\n    total_amount = cofinancing + gef_grant,\n    .keep = \"unused\"\n  ) |&gt; \n  select(-capacity_building_initiative_for_transparency) |&gt; \n  ggplot(aes(approval_fy, total_amount/1e6)) +\n  geom_col(aes(fill = funding_source)) +\n  scale_fill_tableau() +\n  labs(\n    x = \"Year\",\n    y = \"Amount (Million)\",\n    title = \"Investment for Capacity Building\",\n    caption = \"By: Olamide Michael Adu\"\n  ) +\n  scale_y_continuous(labels = label_dollar()) +\n  scale_x_continuous(breaks = seq(2017, 2024, 1)) +\n  facet_wrap(~funding_source, scales = \"free_x\") +\n  theme(\n    legend.position = \"none\",\n    axis.title.y = element_text(\n      vjust = 8,\n      margin = margin(t = 0, r = 0, b = 0, l = 1, unit = \"cm\")\n    ),\n    plot.margin = unit(c(.5, 1, .5, 1), \"cm\")\n  )\n\n\n\n\n\n\n\n\nFigure 4: Amount Spent on Capacity Building"
  },
  {
    "objectID": "posts/gef-projects/index.html#project-status",
    "href": "posts/gef-projects/index.html#project-status",
    "title": "What about our Climate Funds",
    "section": "Project Status",
    "text": "Project Status\n\n\nShow the code\ngef |&gt; \n  mutate(\n    status = str_remove_all(status, \"Project \")\n  ) |&gt; \n  summarize(\n    .by = c(approval_fy, status),\n    count = n()\n  ) |&gt; \n  ggplot(aes(approval_fy, count, fill = fct_reorder(status, count))) +\n  geom_col(position = \"fill\") +\n  labs(\n    x = \"Year\",\n    y = \"Proportion\",\n    fill = \"Project Status:\",\n    title = \"State of GEF Projects\",\n    subtitle = \"Higher proportion of projects are either completed or approved.\\n2020 is having 100% approval with no completed project yet\",\n    caption = \"By: Olamide Michael Adu\"\n  ) +\n  scale_y_continuous(\n    breaks = seq(0, 1, .2),\n    labels = label_percent(scale = 100)\n  ) +\n  scale_fill_wsj() + \n  theme(\n    plot.subtitle = element_text(size = 10, hjust = -.1)\n  )\n\n\n\n\n\n\n\n\nFigure 5: Project status since 1991"
  },
  {
    "objectID": "posts/gef-projects/index.html#finance-involved-in-different-project-stage-per-year",
    "href": "posts/gef-projects/index.html#finance-involved-in-different-project-stage-per-year",
    "title": "What about our Climate Funds",
    "section": "Finance involved in Different Project Stage Per Year",
    "text": "Finance involved in Different Project Stage Per Year\n\n\nShow the code\ngef |&gt; \n  summarize(\n    .by = c(approval_fy, type),\n    gef_grant = sum(gef_grant, na.rm = TRUE),\n    cofinancing = sum(cofinancing, na.rm = TRUE)\n  ) |&gt; \n  mutate(\n    total_amount = gef_grant + cofinancing,\n    .keep = \"unused\"\n  ) |&gt;\n  ggplot(aes(approval_fy, total_amount/1e6, col = type, fill = type)) +\n  geom_area(alpha = .3) +\n  labs(\n    x = \"\",\n    y = \"Funds (millions)\",\n    title = \"Financial trend of different project stage and size\",\n    caption = \"By: Olamide Michael Adu\"\n  ) +\n  scale_y_continuous(labels = label_dollar()) +\n  facet_wrap(~type, scales = \"free_y\")\n\n\n\n\n\n\n\n\nFigure 6: Trend of Funds for different project stage or type according to size\n\n\n\n\n\n\n\nShow the code\ngef |&gt; \n  replace_na(\n    list(\n      gef_grant = 0,\n      cofinancing = 0\n    )\n  ) |&gt; \n  filter(!is.na(gef_period)) |&gt; \n  mutate(\n    total_amount = gef_grant + cofinancing,\n    .keep = \"unused\"\n  ) |&gt; \n  summarize(\n    .by = c(gef_period),\n    total_amount = sum(total_amount)\n  ) |&gt; \n  mutate(\n    xmin = case_when(\n      gef_period == \"Pilot Phase\" ~ 1991,\n      str_match(gef_period, \"\\\\d\") == 1 ~ 1994,\n      str_match(gef_period, \"\\\\d\") == 2 ~ 1998,\n      str_match(gef_period, \"\\\\d\") == 3 ~ 2002,\n      str_match(gef_period, \"\\\\d\") == 4 ~ 2006,\n      str_match(gef_period, \"\\\\d\") == 5 ~ 2010,\n      str_match(gef_period, \"\\\\d\") == 6 ~ 2014,\n      str_match(gef_period, \"\\\\d\") == 7 ~ 2018,\n      str_match(gef_period, \"\\\\d\") == 8 ~ 2022,\n    ),\n    xmax = case_when(\n      gef_period == \"Pilot Phase\" ~ 1994 ,\n      str_match(gef_period, \"\\\\d\") == 1 ~ 1998,\n      str_match(gef_period, \"\\\\d\") == 2 ~ 2002,\n      str_match(gef_period, \"\\\\d\") == 3 ~ 2006,\n      str_match(gef_period, \"\\\\d\") == 4 ~ 2010,\n      str_match(gef_period, \"\\\\d\") == 5 ~ 2014,\n      str_match(gef_period, \"\\\\d\") == 6 ~ 2018,\n      str_match(gef_period, \"\\\\d\") == 7 ~ 2022,\n      str_match(gef_period, \"\\\\d\") == 8 ~ 2026,\n    ),\n    gef_period = case_when(\n      gef_period == \"Pilot Phase\" ~ \"PP\",\n      .default = gef_period\n    )\n  ) |&gt; \n  arrange(desc(gef_period)) |&gt;\n  mutate(\n    year = c(1993, 2024, 2020, 2016, 2012, 2008, 2004, 2000, 1996)\n  ) |&gt; \n  ggplot(aes(year, total_amount/1e6)) +\n  geom_pointrange(aes(xmin = xmin, xmax = xmax), col = \"coral2\",) +\n  geom_text(\n    aes(x = xmin, y = total_amount/1e6, label = gef_period),\n    col = \"gray17\",\n    vjust = -.5,\n    hjust = -.2\n  ) +\n  labs(\n    x = \"Years\",\n    y = \"Amount (Millions)\",\n    title = \"Donation + Expenses + Turnover of the Replenishment Periods\",\n    caption = \"By: Olamide Michael Adu\"\n  ) +\n  scale_y_continuous(labels = label_dollar()) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(vjust = 2)\n  )\n\n\n\n\n\n\n\n\nFigure 7: Total amount of GEF’s replenishment periods"
  },
  {
    "objectID": "posts/gef-projects/index.html#most-funded-project-by-the-gef",
    "href": "posts/gef-projects/index.html#most-funded-project-by-the-gef",
    "title": "What about our Climate Funds",
    "section": "Most Funded Project by the GEF",
    "text": "Most Funded Project by the GEF\n\n\nShow the code\ngef |&gt; \n  replace_na(\n    list(\n      gef_grant = 0,\n      cofinancing = 0\n    )\n  ) |&gt; \n  mutate(total_amount = gef_grant + cofinancing) |&gt; \n  summarize(\n    .by = c(title, countries, approval_fy, status),\n    total_amount = round(sum(total_amount)/1e6, 2)\n  ) |&gt; \n  slice_max(total_amount, n = 10) |&gt;\n  mutate(\n    countries = case_when(\n      str_detect(countries, \",\") ~ \"Some African Countries\",\n      .default = countries\n    )\n  ) |&gt; \n  ggplot(aes(total_amount, fct_reorder(countries, total_amount))) +\n  geom_col(\n    aes(fill = status),\n    position = position_dodge(width = .91),\n    alpha = .7\n  ) +\n  ggrepel::geom_label_repel(\n    aes(label = str_wrap(title, width = 60), fill = status),\n    size = 2.5,\n    col = \"white\",\n    nudge_x= 7\n  ) +\n  scale_fill_stata() +\n  scale_x_continuous(labels = label_dollar()) +\n  labs(\n    x = \"Project Amount (in Millions)\",\n    y = \"Regions\",\n    title = \"Most Funded Project and their Status\",\n    caption = \"By: Olamide Michael Adu\"\n  )\n\n\n\n\n\n\n\n\nFigure 8: The GEF’s most funded project"
  },
  {
    "objectID": "posts/gef-projects/index.html#conclusion",
    "href": "posts/gef-projects/index.html#conclusion",
    "title": "What about our Climate Funds",
    "section": "Conclusion:",
    "text": "Conclusion:\nThis exploratory data analysis has provided valuable insights into the Global Environment Facility’s (GEF) work and impact. By examining various aspects like funding trends, project focus areas, and recipient countries, we’ve gained a deeper understanding of how the GEF tackles global environmental challenges. The findings presented here is limited to the data provided. Limitations included is not limited to:\n\ndifficulty stating who the specific donor nations are\ndifficulty showing how funds were transferred from each replenishment period\nProject investment income\nSuccess of projects, especially for the completed ones.\nTracking projects"
  },
  {
    "objectID": "posts/possum regression/index.html",
    "href": "posts/possum regression/index.html",
    "title": "How old is that Possum?",
    "section": "",
    "text": "This project aims to predict the age of possums collected from three different sites in Australia using linear regression.. The sites are Victoria, New South Wales and Queensland. New South Wales and Queensland are compressed into a single category “Others”. The data is available in the DAAG R package developed by Maindonald, Braun, and Braun (2015).\n\n\n\nCute Possum\n\n\nThe data is having the following properties:\n\n\n\n\n\n\n\n\nS/N\nVariable name\nDefinition\n\n\n\n\n1.\ncase\nObservation number\n\n\n2.\nsite\nThe site number where the possum was trapped.\n\n\n3.\nPop\nThe site as Vic (Victoria) or Other (New South Wales and Queensland)\n\n\n4.\nsex\nGender, either m (male) or f (female).\n\n\n5.\nage\nAge of possum\n\n\n6.\nhdlngth\nHead length, in mm.\n\n\n7.\nskullw\nSkull width, in mm.\n\n\n8.\ntotlngth\nTotal length, in cm.\n\n\n9.\ntaill\nTail length, in cm.\n\n\n10.\nfootlgth\nFoot length in mm\n\n\n11.\nearconch\nEar conch length in mm\n\n\n12.\neye\ndistance from medial canthus to lateral canthus of right eye\n\n\n13.\nchest\nchest girth (in cm)\n\n\n14.\nbelly\nbelly girth (in cm)\n\n\n\n\n\nShow the code\npacman::p_load(tidyverse, GGally, knitr, ggthemr, tidymodels, themis)\nggthemr(\"grape\")\n\n\n\n\n\n\nShow the code\npsm &lt;- DAAG::possum |&gt; janitor::clean_names() |&gt; \n  remove_rownames() |&gt; as_tibble()\n\n\nAfter getting any data, the first thing to do is trying to understand the data\n\n\nShow the code\nskimr::skim_without_charts(psm)\n\n\n\n\nTable 1: Summary statistics of possum data\n\n\n\n\n\n\n(a)\n\n\n\n\n\nName\npsm\n\n\nNumber of rows\n104\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\n\nVariable type: factor\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\npop\n0\n1\nFALSE\n2\noth: 58, Vic: 46\n\n\nsex\n0\n1\nFALSE\n2\nm: 61, f: 43\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\ncase\n0\n1.00\n52.50\n30.17\n1.0\n26.75\n52.50\n78.25\n104.0\n\n\nsite\n0\n1.00\n3.62\n2.35\n1.0\n1.00\n3.00\n6.00\n7.0\n\n\nage\n2\n0.98\n3.83\n1.91\n1.0\n2.25\n3.00\n5.00\n9.0\n\n\nhdlngth\n0\n1.00\n92.60\n3.57\n82.5\n90.68\n92.80\n94.73\n103.1\n\n\nskullw\n0\n1.00\n56.88\n3.11\n50.0\n54.98\n56.35\n58.10\n68.6\n\n\ntotlngth\n0\n1.00\n87.09\n4.31\n75.0\n84.00\n88.00\n90.00\n96.5\n\n\ntaill\n0\n1.00\n37.01\n1.96\n32.0\n35.88\n37.00\n38.00\n43.0\n\n\nfootlgth\n1\n0.99\n68.46\n4.40\n60.3\n64.60\n68.00\n72.50\n77.9\n\n\nearconch\n0\n1.00\n48.13\n4.11\n40.3\n44.80\n46.80\n52.00\n56.2\n\n\neye\n0\n1.00\n15.05\n1.05\n12.8\n14.40\n14.90\n15.72\n17.8\n\n\nchest\n0\n1.00\n27.00\n2.05\n22.0\n25.50\n27.00\n28.00\n32.0\n\n\nbelly\n0\n1.00\n32.59\n2.76\n25.0\n31.00\n32.50\n34.12\n40.0\n\n\n\n\n\n\n\n\n\n\n\nTable 1 (a) shows that data was collected on 104 possum’s. There are 2 categorical variables, pop and sex, but this should be three as site should also be a categorical variable (check below to see transformation of this variable). The case variable is not needed and can be removed. There are missing data in age and footlgth variables, Table 1 (b). We can remove this missing data points as it’s not a lot, Figure 1.\n\n\nShow the code\nvisdat::vis_miss(psm)\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\nShow the code\npsm &lt;- psm |&gt; \n  drop_na() |&gt; \n  select(-case) |&gt; \n  mutate(\n    site = factor(\n      site, \n      levels= 1:7,\n      labels = c(\"Cambarville\", \"Bellbird\", \"Whian Whian\",\n                \"Byrangery\", \"Conondale\", \"Allyn River\", \"Bulburin\")\n    )\n  )"
  },
  {
    "objectID": "posts/possum regression/index.html#loading-the-data",
    "href": "posts/possum regression/index.html#loading-the-data",
    "title": "How old is that Possum?",
    "section": "",
    "text": "Show the code\npsm &lt;- DAAG::possum |&gt; janitor::clean_names() |&gt; \n  remove_rownames() |&gt; as_tibble()\n\n\nAfter getting any data, the first thing to do is trying to understand the data\n\n\nShow the code\nskimr::skim_without_charts(psm)\n\n\n\n\nTable 1: Summary statistics of possum data\n\n\n\n\n\n\n(a)\n\n\n\n\n\nName\npsm\n\n\nNumber of rows\n104\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\n\nVariable type: factor\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\npop\n0\n1\nFALSE\n2\noth: 58, Vic: 46\n\n\nsex\n0\n1\nFALSE\n2\nm: 61, f: 43\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\ncase\n0\n1.00\n52.50\n30.17\n1.0\n26.75\n52.50\n78.25\n104.0\n\n\nsite\n0\n1.00\n3.62\n2.35\n1.0\n1.00\n3.00\n6.00\n7.0\n\n\nage\n2\n0.98\n3.83\n1.91\n1.0\n2.25\n3.00\n5.00\n9.0\n\n\nhdlngth\n0\n1.00\n92.60\n3.57\n82.5\n90.68\n92.80\n94.73\n103.1\n\n\nskullw\n0\n1.00\n56.88\n3.11\n50.0\n54.98\n56.35\n58.10\n68.6\n\n\ntotlngth\n0\n1.00\n87.09\n4.31\n75.0\n84.00\n88.00\n90.00\n96.5\n\n\ntaill\n0\n1.00\n37.01\n1.96\n32.0\n35.88\n37.00\n38.00\n43.0\n\n\nfootlgth\n1\n0.99\n68.46\n4.40\n60.3\n64.60\n68.00\n72.50\n77.9\n\n\nearconch\n0\n1.00\n48.13\n4.11\n40.3\n44.80\n46.80\n52.00\n56.2\n\n\neye\n0\n1.00\n15.05\n1.05\n12.8\n14.40\n14.90\n15.72\n17.8\n\n\nchest\n0\n1.00\n27.00\n2.05\n22.0\n25.50\n27.00\n28.00\n32.0\n\n\nbelly\n0\n1.00\n32.59\n2.76\n25.0\n31.00\n32.50\n34.12\n40.0\n\n\n\n\n\n\n\n\n\n\n\nTable 1 (a) shows that data was collected on 104 possum’s. There are 2 categorical variables, pop and sex, but this should be three as site should also be a categorical variable (check below to see transformation of this variable). The case variable is not needed and can be removed. There are missing data in age and footlgth variables, Table 1 (b). We can remove this missing data points as it’s not a lot, Figure 1.\n\n\nShow the code\nvisdat::vis_miss(psm)\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\nShow the code\npsm &lt;- psm |&gt; \n  drop_na() |&gt; \n  select(-case) |&gt; \n  mutate(\n    site = factor(\n      site, \n      levels= 1:7,\n      labels = c(\"Cambarville\", \"Bellbird\", \"Whian Whian\",\n                \"Byrangery\", \"Conondale\", \"Allyn River\", \"Bulburin\")\n    )\n  )"
  },
  {
    "objectID": "posts/possum regression/index.html#univariate-analysis---target",
    "href": "posts/possum regression/index.html#univariate-analysis---target",
    "title": "How old is that Possum?",
    "section": "Univariate Analysis - Target",
    "text": "Univariate Analysis - Target\n\n\nShow the code\npsm |&gt; \n  summarize(\n    median_age = median(age),\n    mean_age = round(mean(age), 2),\n    minimum_age = min(age),\n    maximum_age = max(age)\n  ) |&gt; kable(\n    col.names = c(\"Median\", \"Mean\", \"Min\", \"Max\"),\n    align = \"lccr\",\n    caption = \"Measure of Central Tendency for Age\"\n  )\n\n\n\n\nTable 2: Descriptive Statistics for Possum Age\n\n\n\n\nMeasure of Central Tendency for Age\n\n\nMedian\nMean\nMin\nMax\n\n\n\n\n3\n3.82\n1\n9\n\n\n\n\n\n\n\n\nTable 2 shows a spread mean and median value for age which might indicates that the distribution is skewed or bimodal, see Figure 2.\n\n\nShow the code\npsm |&gt; \n  ggplot(aes(age)) +\n  geom_density(col = \"#ab2493\") +\n  labs(\n    x = \"Age\",\n    y = \"Density\",\n    title = \"Age variable showing a bimodal distribution\"\n  )\n\n\n\n\n\n\n\n\nFigure 2: Distribution of Age variable"
  },
  {
    "objectID": "posts/possum regression/index.html#univariate-analysis-of-features",
    "href": "posts/possum regression/index.html#univariate-analysis-of-features",
    "title": "How old is that Possum?",
    "section": "Univariate Analysis of Features",
    "text": "Univariate Analysis of Features\n\nFactors\n\nRegions and Sites (Trap Locations)\n\nShow the code\npsm |&gt; \n  mutate(\n    pop = case_when(\n      pop == \"Vic\" ~ \"Victoria\",\n      .default = \"Others\"\n    )\n  ) |&gt; \n  ggplot(aes(pop)) +\n  geom_bar(fill = \"cadetblue4\") +\n  expand_limits(y = c(0, 70)) +\n  labs(\n    x = \"Population\",\n    y = \"Frequency\",\n    title = \"Population of Registered Possums According to Regions\"\n  ) +\n  theme(plot.title = element_text(face = \"bold\", hjust = .5))\n\npsm |&gt; \n  count(site) |&gt; \n  arrange(n) |&gt; \n  ggplot(aes(n, fct_reorder(site, n))) +\n  geom_bar(\n    stat = \"identity\",\n    fill = \"coral2\"\n  ) +\n  labs(\n    y = \"Sites\",\n    x = \"Count\",\n    title = \"Population of Registered Possums According to Sites\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = .5)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 3: Registered Possum Population Record\n\n\n\nMore possums were recorded at the region labelled Other Figure 3. We should recall that Other is the combination of records from New South Wales and Queensland. For the sites where trap where placed within the regions, Cambarville have the highest record of possums with more than half the second site, Bulburin.\n\n\n\nNumerical Variables\n\n\nShow the code\npsm_long &lt;- psm |&gt; \n  pivot_longer(\n    cols = hdlngth:belly,\n    names_to = \"variables\",\n    values_to = \"values\"\n  )\n\npsm_long |&gt; \n  summarize(\n    .by = variables,\n    mean = mean(values),\n    median = median(values),\n    minimum = min(values),\n    maximum = max(values)\n  ) |&gt; \n  kable(\n    col.names = c(\"Variable\", \"Mean\", \"Median\", \"Minimum\", \"Maximum\"),\n    align = \"lcccr\"\n  )\n\n\n\n\nTable 3: Measure of Central Tendency for Numerical Variable\n\n\n\n\n\n\nVariable\nMean\nMedian\nMinimum\nMaximum\n\n\n\n\nhdlngth\n92.73069\n92.9\n82.5\n103.1\n\n\nskullw\n56.96040\n56.4\n50.0\n68.6\n\n\ntotlngth\n87.26931\n88.0\n75.0\n96.5\n\n\ntaill\n37.04951\n37.0\n32.0\n43.0\n\n\nfootlgth\n68.39802\n67.9\n60.3\n77.9\n\n\nearconch\n48.13366\n46.8\n41.3\n56.2\n\n\neye\n15.05049\n14.9\n12.8\n17.8\n\n\nchest\n27.06436\n27.0\n22.0\n32.0\n\n\nbelly\n32.63861\n32.5\n25.0\n40.0\n\n\n\n\n\n\n\n\nTable 3 shows the measure of central tendency. The difference between median and mean is minimal indicating a normal distribution.\n\n\nShow the code\nadditional_colors &lt;- c(\"#af4242\", \"#535364\", \"#FFC300\")\nset_swatch(c(unique(swatch()), additional_colors))\n\npsm_long |&gt; \n  ggplot(aes(values, col = variables)) +\n  geom_density() +\n  scale_color_discrete() +\n  facet_wrap(~variables, scales = \"free\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 4: Numerical variable distribution\n\n\n\n\n\nAll the numerical variables are normally distributed Figure 4. earconch, footlgth, taill, and totlngth are bimodals."
  },
  {
    "objectID": "posts/possum regression/index.html#multivariate-analysis",
    "href": "posts/possum regression/index.html#multivariate-analysis",
    "title": "How old is that Possum?",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\n\n\nShow the code\npsm |&gt;\n  mutate(\n    pop = case_when(\n      pop == \"Vic\" ~ \"Victoria\",\n      .default = \"Others\"\n    )\n  ) |&gt; \n  ggplot(aes(site, age, fill = pop, col = \"#000\")) +\n  geom_violin(inherit.aes = FALSE, aes(site, age)) +\n  geom_boxplot(position = \"dodge\", width = .2) +\n  geom_jitter(size = .5) +\n  facet_wrap(~pop, scales = \"free\") +\n  coord_flip() +\n  theme(legend.position = \"none\")\n\n\nWarning: The `scale_name` argument of `discrete_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix\n\n\nShow the code\nggcorr(\n  psm,\n  geom = \"text\",\n  low = \"#219123\",\n  mid = \"#e09263\",\n  high = \"#8f0123\"\n)\n\n\n\n\n\n\n\n\nFigure 5: Correlation Matrix\n\n\n\n\n\nThe correlation of the variables to age is low with the maximum correlation being with belly, Figure 5. However, there’s high correlation between the predictors and to prevent collinearity we could consider employing Principal Component Analysis to transform correlated predictors to uncorrelated predictor. More information on the relationships existing between the variables can be seen in Figure 6.\n\n\nShow the code\nggpairs(psm)\n\n\n\n\n\n\n\n\nFigure 6: Generalized pairs plot"
  },
  {
    "objectID": "posts/possum regression/index.html#data-sharing",
    "href": "posts/possum regression/index.html#data-sharing",
    "title": "How old is that Possum?",
    "section": "Data Sharing",
    "text": "Data Sharing\nFigure 2 shows how older possums from age 8 to 9 are not well represented in the data. A stratified data sharing technique will be employed to account for less represented age data point.\n\n\nShow the code\nset.seed(124)\npsm_split &lt;- initial_split(psm, prop = .75, strata = age)\npsm_train &lt;- training(psm_split)\n\n\nIf we check Figure 7, possums that are 8 and 9 years old are represented.\n\n\nShow the code\npsm_train |&gt; \n  count(age) |&gt; \n  ggplot(aes(factor(age), n)) +\n  geom_col() +\n  geom_text(\n    aes(label = n),\n    nudge_y = .5,\n    col = \"red\"\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"Count\",\n    title = \"Age Frequency in Training Data\"\n  )\n\n\n\n\n\n\n\n\nFigure 7: Age data point frequency in training data\n\n\n\n\n\nTo prevent data leaking, the data will be resampled to have a validation-training data in 10 folds using the k-folds resampling.\n\n\nShow the code\nset.seed(124)\npsm_folds &lt;- vfold_cv(psm_train, v = 10)"
  },
  {
    "objectID": "posts/possum regression/index.html#model-specification",
    "href": "posts/possum regression/index.html#model-specification",
    "title": "How old is that Possum?",
    "section": "Model Specification",
    "text": "Model Specification\nWe will use linear regression to predict the age of possums\n\n\nShow the code\nlm_spec &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\nlm_spec |&gt; translate()\n\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModel fit template:\nstats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())"
  },
  {
    "objectID": "posts/possum regression/index.html#feature-engineering",
    "href": "posts/possum regression/index.html#feature-engineering",
    "title": "How old is that Possum?",
    "section": "Feature engineering",
    "text": "Feature engineering\nThree preprocessing will be added to formular specification. These are:\n\nNormalizing, centering and scaling numerical variables\nPCA to reduce collinearity between numeric variables\nCreating dummy variables for categorical data.\n\nThese preprocesses are added as presented down to the last step which is creating dummy variables for categorical data.\n\n\nShow the code\npsm_rec_1 &lt;- recipe(age ~ ., data = psm_train)\n\npsm_rec_2 &lt;- psm_rec_1 |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_center(all_numeric_predictors()) |&gt; \n  step_scale(all_numeric_predictors())\n\npsm_rec_3 &lt;- psm_rec_2 |&gt; \n  step_pca(all_numeric_predictors())\n\npsm_rec_4 &lt;- psm_rec_3 |&gt;\n  step_dummy(all_factor_predictors()) \n\n\nThe result from applying the whole steps is shown in Table 4\n\n\nShow the code\npsm_rec_4 |&gt; \n  prep() |&gt; \n  juice() |&gt; \n  head() |&gt; \n  kable()\n\n\n\n\nTable 4: Data look after preprocessing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nPC1\nPC2\nPC3\nPC4\nPC5\nsite_Bellbird\nsite_Whian.Whian\nsite_Byrangery\nsite_Conondale\nsite_Allyn.River\nsite_Bulburin\npop_other\nsex_m\n\n\n\n\n2\n-0.0683403\n-1.343005\n0.1629385\n-0.3007284\n-0.5391333\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0.5255119\n-2.047798\n-0.8395109\n0.1766334\n-0.0667790\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0.5932377\n-1.959162\n0.3740650\n0.6887651\n0.0693626\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2\n-0.2120375\n-2.211136\n1.7088223\n-0.3387357\n-0.0931182\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2\n-2.7210452\n-1.389487\n0.2290110\n0.7561382\n1.2385934\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n-1.6707158\n-1.652564\n0.3608677\n-0.3541424\n-0.3539676\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nThe numerical predictors has been reduced to 5 variables."
  },
  {
    "objectID": "posts/possum regression/index.html#model-workflow",
    "href": "posts/possum regression/index.html#model-workflow",
    "title": "How old is that Possum?",
    "section": "Model Workflow",
    "text": "Model Workflow\n\n\nShow the code\npsm_wf_set &lt;- workflow_set(\n  preproc = list(\n    \"formula\" = psm_rec_1,\n    \"normalized\" = psm_rec_2, \n    \"pca\" = psm_rec_3,\n    \"dummy\" = psm_rec_4\n  ),\n  models = list(\"ols\" = lm_spec),\n  cross = TRUE\n)\n\npsm_wf_set\n\n\n# A workflow set/tibble: 4 × 4\n  wflow_id       info             option    result    \n  &lt;chr&gt;          &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 formula_ols    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 normalized_ols &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 pca_ols        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 dummy_ols      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\n\nFitting the model\n\n\nShow the code\npsm_mod &lt;- psm_wf_set |&gt; \n  workflow_map(\n    \"fit_resamples\",\n    resamples = psm_folds,\n    seed = 124\n)\n\ncollect_metrics(psm_mod) |&gt; kable()\n\n\n\n\nTable 5: Model metric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwflow_id\n.config\npreproc\nmodel\n.metric\n.estimator\nmean\nn\nstd_err\n\n\n\n\nformula_ols\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrmse\nstandard\n1.7863370\n10\n0.1001647\n\n\nformula_ols\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrsq\nstandard\n0.2506679\n10\n0.0642785\n\n\nnormalized_ols\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrmse\nstandard\n1.7863370\n10\n0.1001647\n\n\nnormalized_ols\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrsq\nstandard\n0.2506679\n10\n0.0642785\n\n\npca_ols\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrmse\nstandard\n1.7590867\n10\n0.1122277\n\n\npca_ols\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrsq\nstandard\n0.1970622\n10\n0.0555754\n\n\ndummy_ols\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrmse\nstandard\n1.7590867\n10\n0.1122277\n\n\ndummy_ols\nPreprocessor1_Model1\nrecipe\nlinear_reg\nrsq\nstandard\n0.1970622\n10\n0.0555754\n\n\n\n\n\n\n\n\nTable 5 shows no difference in the model between dummy_ols and pca_ols which is different from using the formula without preprocessing, formula_ols and normalized_ols. This presented visually in Figure 8\n\n\nShow the code\ncollect_metrics(psm_mod) |&gt; \n  filter(.metric == \"rmse\") |&gt; \n  select(wflow_id, mean, std_err) |&gt; \n  mutate(\n    ymax = mean + std_err,\n    ymin = mean - std_err\n  ) |&gt; \n  ggplot(aes(wflow_id, mean, col = wflow_id)) +\n  geom_pointrange(aes(ymin = ymin, ymax =ymax)) +\n  labs(\n    x = \"Preproc\",\n    title = \"RMSE of Possum Linear Regression Model for 3 Preprocessor\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 8: Model evaluation using RMSE for each preprocessor\n\n\n\n\n\nWe can use either of pca_ols or dummy_ols as they have the lowest mean.\n\n\nShow the code\npsm_pca_mod &lt;- psm_mod |&gt; \n  extract_workflow(id = \"pca_ols\")\n\npsm_pca_mod \n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_normalize()\n• step_center()\n• step_scale()\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\nShow the code\npsm_model &lt;- last_fit(\n  psm_pca_mod,\n  split = psm_split\n)\n\n\n\n\nShow the code\npsm_model |&gt; \n  collect_predictions() |&gt; \n  select(\"prediction\" =.pred, age) |&gt; \n  mutate(\n    prediction = ceiling(prediction),\n    residual = age - prediction\n  ) |&gt; \n  ggplot(\n    aes(age, residual)\n  ) +\n  geom_point() +\n  geom_hline(aes(yintercept = 0), col = \"gray3\") +\n  ggtitle(\"Residual\")\n\n\n\n\n\n\n\n\n\nWhile the points are fairly distributed along the y axis and x axis, the effect of having little representative from the population for old-aged possum, age 9 and 8 can be see. The lack of points on the lower-right side of the plot is a good of indication."
  },
  {
    "objectID": "posts/possum regression/index.html#feature-importance",
    "href": "posts/possum regression/index.html#feature-importance",
    "title": "How old is that Possum?",
    "section": "Feature Importance",
    "text": "Feature Importance\nThe features contributing the most to the model are shown in Figure 9\n\n\nShow the code\npsm_model |&gt;\n  extract_fit_parsnip() |&gt; \n  vip::vip(\n    num_features = 10,\n    geom = \"col\"\n  ) +\n  ggtitle(\"Feature Importance\")\n\n\n\n\n\n\n\n\nFigure 9: PC1, and Byrangery sites are the most important variables."
  },
  {
    "objectID": "posts/possum regression/index.html#summary",
    "href": "posts/possum regression/index.html#summary",
    "title": "How old is that Possum?",
    "section": "Summary",
    "text": "Summary\nWhile this a good use of linear regression, the robustness of the model would have been helped if underrepresented data points are available. By the time of writing this blog post, the author with intentions to use only SLR is having no feature engineering method to account for the age variable that has been discretized. A resampling technique such as Monte Carlo or bootstrapping is an approach that could help with the model rather than the use of k-fold resampling method."
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html#missing-data",
    "href": "posts/algeria-fwi-prediction/index.html#missing-data",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "Missing Data",
    "text": "Missing Data\nThere’s a maximum of two missing data, which is in the classes variable, Table 2 (b). I will investigate this:\n\n\nShow the code\nalgeria_ff |&gt; \n  filter(is.na(classes)) |&gt;  kable()\n\n\n\n\nTable 3: Missing data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nday\nmonth\nyear\ntemperature\nrh\nws\nrain\nffmc\ndmc\ndc\nisi\nbui\nfwi\nclasses\n\n\n\n\nSidi-Bel Abbes Region Dataset\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n14\n07\n2012\n37\n37\n18\n0.2\n88.9\n12.9\n14.6 9\n12.5\n10.4\nfire\nNA\n\n\n\n\n\n\n\n\nThere’s an interesting finding in Table 3. The start of Sidi-Bel Abbes region dataset can be seen under the day variable. I will add row numbers and a new column called region and add each region according to the row number where Sidi-Bel Abbes appears in the variable day. All data before Sidi-Bel Abbes are Bejaia region data,\n\n\nShow the code\nalgeria_ff &lt;- algeria_ff |&gt; \n  mutate(\n    id = row_number(),\n    .before = day\n  )\nhead(algeria_ff) |&gt; kable()\n\n\n\n\nTable 4: Row numbers added\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nday\nmonth\nyear\ntemperature\nrh\nws\nrain\nffmc\ndmc\ndc\nisi\nbui\nfwi\nclasses\n\n\n\n\n1\n01\n06\n2012\n29\n57\n18\n0\n65.7\n3.4\n7.6\n1.3\n3.4\n0.5\nnot fire\n\n\n2\n02\n06\n2012\n29\n61\n13\n1.3\n64.4\n4.1\n7.6\n1\n3.9\n0.4\nnot fire\n\n\n3\n03\n06\n2012\n26\n82\n22\n13.1\n47.1\n2.5\n7.1\n0.3\n2.7\n0.1\nnot fire\n\n\n4\n04\n06\n2012\n25\n89\n13\n2.5\n28.6\n1.3\n6.9\n0\n1.7\n0\nnot fire\n\n\n5\n05\n06\n2012\n27\n77\n16\n0\n64.8\n3\n14.2\n1.2\n3.9\n0.5\nnot fire\n\n\n6\n06\n06\n2012\n31\n67\n14\n0\n82.6\n5.8\n22.2\n3.1\n7\n2.5\nfire\n\n\n\n\n\n\n\n\nI will perform the previous filter operation in Table 3 to get the start of the row number for Sidi-Bel Abbes Region.\n\nShow the code\nalgeria_ff |&gt; \n  filter(is.na(classes))\n\n\n\n\nTable 5\n\n\n\n# A tibble: 2 × 15\n     id day    month year  temperature rh    ws    rain  ffmc  dmc   dc    isi  \n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1   123 Sidi-… &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;        &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt; \n2   168 14     07    2012  37          37    18    0.2   88.9  12.9  14.6… 12.5 \n# ℹ 3 more variables: bui &lt;chr&gt;, fwi &lt;chr&gt;, classes &lt;chr&gt;\n\n\n\n\nSidi-Bel Abbes data starts from 124, Table 5. I will add the regions and remove the id variable that contains the row numbers. The number of observations for each region can be seen in Table 6\n\nShow the code\nalgerian_ff &lt;- algeria_ff |&gt; \n  mutate(\n    region = case_when(id &lt;= 122 ~ \"Bejaia\",\n                       .default = \"Sidi-Bel Abbes\"),\n    .before = day,\n    .keep = \"unused\"\n  )\n\nhead(algerian_ff)\n\n\n\n\nTable 6: Number of observations for the regions\n\n\n\n# A tibble: 6 × 15\n  region day   month year  temperature rh    ws    rain  ffmc  dmc   dc    isi  \n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 Bejaia 01    06    2012  29          57    18    0     65.7  3.4   7.6   1.3  \n2 Bejaia 02    06    2012  29          61    13    1.3   64.4  4.1   7.6   1    \n3 Bejaia 03    06    2012  26          82    22    13.1  47.1  2.5   7.1   0.3  \n4 Bejaia 04    06    2012  25          89    13    2.5   28.6  1.3   6.9   0    \n5 Bejaia 05    06    2012  27          77    16    0     64.8  3     14.2  1.2  \n6 Bejaia 06    06    2012  31          67    14    0     82.6  5.8   22.2  3.1  \n# ℹ 3 more variables: bui &lt;chr&gt;, fwi &lt;chr&gt;, classes &lt;chr&gt;\n\n\n\n\nNow we can check for missing data again\n\n\nShow the code\nalgerian_ff |&gt; \n  filter(if_any(everything(), is.na)) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\nday\nmonth\nyear\ntemperature\nrh\nws\nrain\nffmc\ndmc\ndc\nisi\nbui\nfwi\nclasses\n\n\n\n\nSidi-Bel Abbes\nSidi-Bel Abbes Region Dataset\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nSidi-Bel Abbes\n14\n07\n2012\n37\n37\n18\n0.2\n88.9\n12.9\n14.6 9\n12.5\n10.4\nfire\nNA\n\n\n\n\n\nThe missing points are still the same. Here, I will remove this data points and proceed with the analysis.\n\n\nShow the code\nalgerian_ff &lt;- algerian_ff |&gt; \n  drop_na() |&gt; \n  filter(classes %in% c(\"not fire\", \"fire\"))\n\n\nI also need to change the other variables to numeric data types except region and classes which will be changed to factor variable type.\n\n\nShow the code\nalgerian_ff &lt;- algerian_ff |&gt; \n  mutate(\n    region = factor(region),\n    classes = ifelse(classes == \"not fire\", 0, 1),\n    classes = factor(classes, labels = c(\"not fire\", \"fire\"), levels = c(0 ,1)),\n    across(where(is.character), parse_number)\n  )\n\nstr(algerian_ff)\n\n\ntibble [243 × 15] (S3: tbl_df/tbl/data.frame)\n $ region     : Factor w/ 2 levels \"Bejaia\",\"Sidi-Bel Abbes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ day        : num [1:243] 1 2 3 4 5 6 7 8 9 10 ...\n $ month      : num [1:243] 6 6 6 6 6 6 6 6 6 6 ...\n $ year       : num [1:243] 2012 2012 2012 2012 2012 ...\n $ temperature: num [1:243] 29 29 26 25 27 31 33 30 25 28 ...\n $ rh         : num [1:243] 57 61 82 89 77 67 54 73 88 79 ...\n $ ws         : num [1:243] 18 13 22 13 16 14 13 15 13 12 ...\n $ rain       : num [1:243] 0 1.3 13.1 2.5 0 0 0 0 0.2 0 ...\n $ ffmc       : num [1:243] 65.7 64.4 47.1 28.6 64.8 82.6 88.2 86.6 52.9 73.2 ...\n $ dmc        : num [1:243] 3.4 4.1 2.5 1.3 3 5.8 9.9 12.1 7.9 9.5 ...\n $ dc         : num [1:243] 7.6 7.6 7.1 6.9 14.2 22.2 30.5 38.3 38.8 46.3 ...\n $ isi        : num [1:243] 1.3 1 0.3 0 1.2 3.1 6.4 5.6 0.4 1.3 ...\n $ bui        : num [1:243] 3.4 3.9 2.7 1.7 3.9 7 10.9 13.5 10.5 12.6 ...\n $ fwi        : num [1:243] 0.5 0.4 0.1 0 0.5 2.5 7.2 7.1 0.3 0.9 ...\n $ classes    : Factor w/ 2 levels \"not fire\",\"fire\": 1 1 1 1 1 2 2 2 1 1 ..."
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html#data-sharing",
    "href": "posts/algeria-fwi-prediction/index.html#data-sharing",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "Data Sharing",
    "text": "Data Sharing\nLet’s split the data into two portions. The training data will be 70% of the whole data while the test data will be 30%.\n\nShow the code\nset.seed(123)\nalgerian_split &lt;- initial_split(algerian_ff, prop = .7)\nalgerian_train &lt;- training(algerian_split)\nalgerian_test &lt;- testing(algerian_split)\nalgerian_train |&gt; \n  count(region, classes)\n\n\n\n\nTable 7: Distribution of the targer variable across regions\n\n\n\n# A tibble: 4 × 3\n  region         classes      n\n  &lt;fct&gt;          &lt;fct&gt;    &lt;int&gt;\n1 Bejaia         not fire    41\n2 Bejaia         fire        48\n3 Sidi-Bel Abbes not fire    32\n4 Sidi-Bel Abbes fire        49\n\n\n\n\nTable 7 shows how the data is distributed in the training data."
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html#model-specification",
    "href": "posts/algeria-fwi-prediction/index.html#model-specification",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "Model Specification",
    "text": "Model Specification\nNext we create the model specification\n\n\nShow the code\nlr_spec &lt;- logistic_reg() |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"glm\")\n\nlr_spec |&gt; \n  translate()\n\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\nModel fit template:\nstats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), \n    family = stats::binomial)"
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html#feature-engineering",
    "href": "posts/algeria-fwi-prediction/index.html#feature-engineering",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nFor feature engineering, we will remove zero and near-zero variance variables. After, we’ll apply Yeo-Johnson to prevent to handle data values that have zero or negative values. After this, we standardize the results and make all factor variables one-hot coded.\n\n\nShow the code\nlr_rec &lt;- recipe(\n  classes ~ . + 1, data = algerian_train\n) |&gt; \n  step_zv(all_numeric_predictors()) |&gt; \n  step_nzv(all_numeric_predictors()) |&gt; \n  step_YeoJohnson(all_numeric_predictors()) |&gt; \n  step_scale(all_numeric_predictors()) |&gt; \n  step_pca(all_numeric_predictors()) |&gt; \n  step_dummy(region)\n  \nlr_rec\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 14\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_numeric_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_numeric_predictors()\n\n\n• Yeo-Johnson transformation on: all_numeric_predictors()\n\n\n• Scaling for: all_numeric_predictors()\n\n\n• PCA extraction with: all_numeric_predictors()\n\n\n• Dummy variables from: region\n\n\nTo see how the data looks after preprocessing let’s use the prep() and juice() function.\n\n\nShow the code\nlr_rec |&gt; \n  prep() |&gt; \n  juice() |&gt; \n  head() |&gt; \n  kable()\n\n\n\n\nTable 8: Preprocessed data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclasses\nPC1\nPC2\nPC3\nPC4\nPC5\nregion_Sidi.Bel.Abbes\n\n\n\n\nfire\n-12.75870\n1.065189\n1.9127196\n0.1717558\n1.5907799\n1\n\n\nfire\n-15.82715\n3.592703\n-0.6338943\n-0.1082043\n0.1228698\n1\n\n\nfire\n-13.60741\n1.357303\n-0.8886342\n-0.0618020\n0.1017522\n1\n\n\nnot fire\n-11.74371\n-3.347725\n-1.0163208\n1.2834374\n1.1943557\n0\n\n\nfire\n-13.83764\n1.683620\n1.3037796\n0.2674702\n0.5054699\n1\n\n\nfire\n-14.13147\n1.995548\n-0.6331457\n0.3294726\n0.1962187\n1\n\n\n\n\n\n\n\n\nFrom Table 8 we can see that the variable year has been removed.The variables has also been reduced as some of them were related, check Figure 2 ## Workflow\n\n\nShow the code\nlr_wf &lt;- workflow() |&gt; \n  add_model(lr_spec) |&gt; \n  add_recipe(lr_rec) |&gt; \n  fit(algerian_train)"
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html#model-result",
    "href": "posts/algeria-fwi-prediction/index.html#model-result",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "Model Result",
    "text": "Model Result\n\n\nShow the code\nlr_wf |&gt; tidy()\n\n\n# A tibble: 7 × 5\n  term                  estimate std.error statistic   p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             -8.40      7.01     -1.20  0.230    \n2 PC1                     -0.758     0.538    -1.41  0.159    \n3 PC2                      3.43      0.847     4.05  0.0000517\n4 PC3                      0.480     0.459     1.05  0.296    \n5 PC4                      1.77      0.638     2.77  0.00558  \n6 PC5                     -0.999     0.440    -2.27  0.0231   \n7 region_Sidi.Bel.Abbes    0.356     0.959     0.371 0.711    \n\n\nWhen all the factors are zero, the odds of fire occurring is very low, i.e. the exponential of the intercept estimate, 2.2380029^{-4}. For region Sidi-Bel Abbes, the odds of a fire outbreak is 1.5 times higher, ?@tbl-model-sum."
  },
  {
    "objectID": "posts/algeria-fwi-prediction/index.html#model-evaluation",
    "href": "posts/algeria-fwi-prediction/index.html#model-evaluation",
    "title": "Prediction of Fire Occurrence in Algeria’s Forest",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nAccuracy\n\n\nShow the code\nlr_wf |&gt; augment(algerian_test) |&gt; \n  accuracy(classes, .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.918\n\n\nThe accuracy of the model is high at 92%.\n\n\nSensitivity\n\n\nShow the code\nlr_wf |&gt; \n  augment(algerian_test) |&gt; \n  sensitivity(classes, .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.818\n\n\nThe model is 82% sensitive.\n\n\nPrecision\n\n\nShow the code\nlr_wf |&gt; \n  augment(algerian_test) |&gt; \n  precision(classes, .pred_class)\n\n\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision binary             1\n\n\n\n\nROC-AUC\n\n\nShow the code\nlr_wf |&gt; \n  augment(algerian_test) |&gt; \n  roc_auc(classes, `.pred_not fire`)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.992\n\n\n\n\nShow the code\nlr_wf |&gt; \n  augment(algerian_test) |&gt; \n  roc_curve(classes, `.pred_not fire`) |&gt; \n  autoplot()\n\n\n\n\n\n\n\n\nFigure 5: Area under the curve is high\n\n\n\n\n\nThe area under the curve is .99 which is very good Figure 5.\n\n\nConfusion Matrix\n\n\nShow the code\nlr_wf |&gt; \n  augment(algerian_test) |&gt; \n  conf_mat(classes, .pred_class) \n\n\n          Truth\nPrediction not fire fire\n  not fire       27    0\n  fire            6   40\n\n\nThe model performance precision and accuracy is high above 90%, with precision at 100%. There are 6 false positives which signals the likelihood of fire when there is supposed to be non."
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html",
    "href": "posts/regularized_algeria_fire_pred/index.html",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "",
    "text": "This project is a recycle of the prediction of fire occurrence in Algeria’s forest with a little twist. Instead of predicting fire occurrence, this project will predict the forest Fire Weather Index (FWI). Also different from the former project is the algorithm used. The former project used logistic regression with a principal component analysis preprocessing to reduce multicollinearity between the features. This project uses a regularized regression to predict the FWI of the forest.\n\n\nThe objective of this project involve:\n\nDeveloping a FWI model.\nEvaluating which of the regularized regression will preform best. (significant difference in performance is not the goal but rather getting the best performance).\n\n\n\n\nData used for this project is exactly the same as the data used in the prediction of fire occurrence project, check here for the data definition. To get more understanding of the data, and the correlation between the different variables, check the posts, as I will dive in to model development for this project.\nThe processed and clean data is already made available and will be imported. Prior that, we have to load the necessary library for this analysis.\n\n\nShow the code\npacman::p_load(nanoparquet, tidymodels, knitr, ggthemr)\nggthemr(palette = \"earth\", layout = \"minimal\")\n\n\n\n\nShow the code\nalgeria_ff &lt;- read_parquet(\"data/algeria.parquet\")"
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#objective",
    "href": "posts/regularized_algeria_fire_pred/index.html#objective",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "",
    "text": "The objective of this project involve:\n\nDeveloping a FWI model.\nEvaluating which of the regularized regression will preform best. (significant difference in performance is not the goal but rather getting the best performance)."
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#data",
    "href": "posts/regularized_algeria_fire_pred/index.html#data",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "",
    "text": "Data used for this project is exactly the same as the data used in the prediction of fire occurrence project, check here for the data definition. To get more understanding of the data, and the correlation between the different variables, check the posts, as I will dive in to model development for this project.\nThe processed and clean data is already made available and will be imported. Prior that, we have to load the necessary library for this analysis.\n\n\nShow the code\npacman::p_load(nanoparquet, tidymodels, knitr, ggthemr)\nggthemr(palette = \"earth\", layout = \"minimal\")\n\n\n\n\nShow the code\nalgeria_ff &lt;- read_parquet(\"data/algeria.parquet\")"
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#model-specification",
    "href": "posts/regularized_algeria_fire_pred/index.html#model-specification",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "Model Specification",
    "text": "Model Specification\nWe do not know the penalty for to use for our regularized regression, so we tune this parameter. The best value for the elastic-net is also unknown and will also be tuned.\n\n\nShow the code\nlasso_spec &lt;- linear_reg(\n  penalty = tune(),\n  mixture = 1\n) |&gt; \n  set_engine(\"glmnet\")\n\nridge_spec &lt;- linear_reg(\n  penalty = tune(),\n  mixture = 0\n) |&gt; \n  set_engine(\"glmnet\")\n\nelastic_spec &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) |&gt; \n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#feature-engineering",
    "href": "posts/regularized_algeria_fire_pred/index.html#feature-engineering",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "Feature engineering",
    "text": "Feature engineering\nPreprocessing steps carried before except using pca will be employed here.\n\n\nShow the code\nalgeria_rec &lt;- recipe(fwi ~ ., data = algeria_train) |&gt; \n  step_zv(all_numeric_predictors()) |&gt; \n  step_nzv(all_numeric_predictors()) |&gt; \n  step_YeoJohnson(all_numeric_predictors()) |&gt; \n  step_scale(all_numeric_predictors()) |&gt; \n  step_dummy(all_factor_predictors())\n\nalgeria_rec\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 14\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_numeric_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_numeric_predictors()\n\n\n• Yeo-Johnson transformation on: all_numeric_predictors()\n\n\n• Scaling for: all_numeric_predictors()\n\n\n• Dummy variables from: all_factor_predictors()\n\n\nThe data after undergoing feature engineering is shown in Table 1:\n\n\nShow the code\nset.seed(123)\nalgeria_rec |&gt; \n  prep() |&gt; \n  juice() |&gt; \n  car::some() |&gt; \n  kable()\n\n\n\n\nTable 1: Data preview after preprocessing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nday\nmonth\ntemperature\nrh\nws\nrain\nffmc\ndmc\ndc\nisi\nbui\nfwi\nregion_Sidi.Bel.Abbes\nclasses_fire\n\n\n\n\n1.9099765\n6.257817\n4.469877\n5.018083\n16.04237\n1.5146337\n0.6112248\n1.971387\n3.257687\n1.004836\n2.219341\n0.5\n0\n0\n\n\n3.5100906\n8.045765\n2.975905\n3.822778\n14.61728\n0.9683845\n0.9364749\n1.666566\n3.227303\n1.068358\n2.007790\n0.5\n1\n0\n\n\n1.7969550\n6.257817\n5.622383\n3.605656\n13.78430\n1.8495797\n1.3793852\n2.404867\n3.562018\n1.187334\n2.602155\n0.8\n0\n0\n\n\n3.0974426\n5.363843\n4.746365\n3.822778\n15.71108\n0.0000000\n2.6879614\n3.413360\n4.732499\n2.802260\n3.732665\n10.6\n0\n1\n\n\n0.7144441\n6.257817\n5.322644\n3.968818\n14.21285\n0.0000000\n2.5741665\n2.375816\n3.764466\n2.406109\n2.692231\n4.9\n0\n1\n\n\n1.0917983\n7.151791\n6.567699\n3.249173\n12.84074\n0.0000000\n2.7659263\n2.826989\n3.433657\n2.499136\n2.910182\n5.9\n1\n1\n\n\n3.6120029\n6.257817\n6.567699\n3.178716\n14.61728\n0.0000000\n3.0242252\n4.049493\n4.665000\n3.067498\n4.080263\n14.5\n1\n1\n\n\n1.7969550\n7.151791\n6.567699\n2.159044\n13.78430\n0.0000000\n3.4069902\n3.716532\n4.469620\n3.403472\n3.773189\n15.7\n1\n1\n\n\n2.1330093\n8.045765\n5.622383\n2.423705\n13.32829\n0.0000000\n3.5003297\n3.885782\n4.603991\n3.450648\n3.980076\n17.5\n1\n1\n\n\n2.5687041\n8.045765\n5.929822\n1.773226\n15.36427\n0.0000000\n3.4534197\n3.794553\n5.041329\n3.724684\n4.130098\n21.6\n1\n1"
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#tune-grid",
    "href": "posts/regularized_algeria_fire_pred/index.html#tune-grid",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "Tune Grid",
    "text": "Tune Grid\nIn the models specified earlier we have one to two parameters we have to tune. These are the parameters with tune() in front of them.\n\n\nShow the code\nset.seed(123)\n\ntune_elastic &lt;- extract_parameter_set_dials(elastic_spec) |&gt; \n  grid_regular(\n    levels = 25\n  )\n\ntune_lasso &lt;- extract_parameter_set_dials(lasso_spec) |&gt; \n  grid_regular(levels =  20)\n\ntune_ridge &lt;- extract_parameter_set_dials(ridge_spec) |&gt; \n  grid_random(size = 20)\n\n\nWe set control to save prediction and the workflow.\n\n\nShow the code\ngrid_control &lt;-control_grid(\n  save_pred = TRUE,\n  save_workflow = TRUE\n)"
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#sec-workflow",
    "href": "posts/regularized_algeria_fire_pred/index.html#sec-workflow",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "Workflow",
    "text": "Workflow\nA workflow object for each model specification will be made\n\n\nShow the code\nelastic_wf &lt;- workflow() |&gt; \n  add_recipe(algeria_rec) |&gt; \n  add_model(elastic_spec)\n\nlasso_wf &lt;- workflow() |&gt; \n  add_recipe(algeria_rec) |&gt; \n  add_model(lasso_spec)\n\nridge_wf &lt;- workflow() |&gt; \n  add_recipe(algeria_rec) |&gt; \n  add_model(ridge_spec)\n\n\nBelow is a breakdown of the process from model specification to feature engineering tied together.\n\n\nShow the code\nelastic_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_zv()\n• step_nzv()\n• step_YeoJohnson()\n• step_scale()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet"
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#tuning",
    "href": "posts/regularized_algeria_fire_pred/index.html#tuning",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "Tuning",
    "text": "Tuning\nNow we tune the parameter(s) of each models\n\n\nShow the code\nelastic_tune &lt;- tune_grid(\n  elastic_wf,\n  resamples = algeria_bstrap,\n  grid = tune_elastic,\n  control = grid_control\n)\n\nlasso_tune &lt;- tune_grid(\n  lasso_wf,\n  resamples = algeria_bstrap,\n  grid = tune_lasso,\n  control = grid_control\n)\n\nridge_tune &lt;- tune_grid(\n  ridge_wf,\n  resamples = algeria_bstrap,\n  grid = tune_ridge,\n  control = grid_control\n)\n\n\n\nTune Performance\nLet’s see the performance of the regularized parameters.\n\n\nShow the code\nelastic_tune |&gt; \n  collect_metrics() |&gt;\n  mutate(\n    model = \"elastic\"\n  ) |&gt; \n  bind_rows(\n    lasso_tune |&gt; \n      collect_metrics() |&gt; \n      mutate(\n        model = \"lasso\"\n      )\n  ) |&gt; \n  bind_rows(\n    ridge_tune |&gt; \n      collect_metrics() |&gt; \n      mutate(\n        model = \"ridge\"\n      )\n  ) |&gt; \n  ggplot(aes(penalty, mean, color = model)) +\n  geom_point() +\n  geom_smooth(\n    se = FALSE,\n    method = \"loess\",\n    formula = \"y ~ x\"\n  ) + \n  facet_wrap(~.metric, nrow = 2, scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 1: Regularization Performance\n\n\n\n\n\n\n\nFinal workflow\nAs shown in Figure 1, the elastic-net regularized model performed the best. We can pick the best model from this and get the final model.\n\n\nShow the code\nbest_tune &lt;- elastic_tune |&gt; \n  select_best(metric = \"rmse\")\n\nfinal_wf &lt;- finalize_workflow(\n  x = elastic_wf,\n  parameters = best_tune\n)  \n\nfinal_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_zv()\n• step_nzv()\n• step_YeoJohnson()\n• step_scale()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.0562341325190349\n  mixture = 0.0895833333333333\n\nComputational engine: glmnet"
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#feature-importance",
    "href": "posts/regularized_algeria_fire_pred/index.html#feature-importance",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "Feature Importance",
    "text": "Feature Importance\nAbove @final-fw we parameters fitted accordingly and can note that the tune parameter in Section 2.4 has been replace accordingly. Before We finally fit the model to the whole of the data. we can investigate to see the most important variables\n\n\nShow the code\nlibrary(vip)\n\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\nShow the code\nvip(final_wf |&gt; \n      fit(algeria_train))"
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#final-fit",
    "href": "posts/regularized_algeria_fire_pred/index.html#final-fit",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "Final Fit",
    "text": "Final Fit\n\n\nShow the code\nlast_fit &lt;- final_wf |&gt; \n  last_fit(split = algeria_split)\n\n\nFigure 2 shows how the penalty, aka 𝜆 from 0 to infinity.\n\n\nShow the code\nadditional_colors &lt;- c(\"#af4242\", \"#535364\", \"#FFC300\", \"#e09263\", \"#123367\")\nset_swatch(c(unique(swatch()), additional_colors))\n\n\nextract_fit_parsnip(last_fit) |&gt; \n  autoplot() +\n  labs(\n    x = \"Lambda\",\n    y = \"Coefficients\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 2: Coeﬃcients for our elastic-net regression model as 𝜆 grows from 0 → ∞\n\n\n\n\n\n\nModel Eval\n\n\nShow the code\nlast_fit |&gt; \n  collect_metrics() |&gt; \n  kable()\n\n\n\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\n\nrmse\nstandard\n3.3764246\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.8563439\nPreprocessor1_Model1\n\n\n\n\n\nWith a RMSE of 3.38 the model prediction of the forest fire weather index is reliable as the model can also explain about 86% of the whole data."
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#conclusion",
    "href": "posts/regularized_algeria_fire_pred/index.html#conclusion",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "Conclusion",
    "text": "Conclusion\nThis project compared three regularized models, and used it as an alternative of creating a model without a pca preprocessing step, as regularized models penalize the coefficient of features pushing them towards zero or making them exactly zero (lasso regression). The model helped minimize the impact of multicollinearity existing within the data."
  },
  {
    "objectID": "posts/regularized_algeria_fire_pred/index.html#reflection",
    "href": "posts/regularized_algeria_fire_pred/index.html#reflection",
    "title": "Predicting Algeria’s Forest Fire Weather Index (FWI)",
    "section": "Reflection",
    "text": "Reflection\nI tried using workflow_map() to tune the three models together combined in a workflow_set(). When metrics where collected to evaluate the models, the tuning parameters were absent and instead only results where returned. You can run the code below to confirm this.\n\n\nShow the code\nalgerian_wf_set &lt;- workflow_set(\n  preproc = list(rec = algeria_rec),\n  models = list(\n    lasso = lasso_spec,\n    ridge = ridge_spec,\n    elastic = elastic_spec\n  ),\n  cross = TRUE\n ) |&gt;\n  option_add(\n     id = \"rec_elastic\",\n     grid = tune_elastic\n  ) |&gt; \n  option_add(\n    id = \"rec_lasso\",\n    grid = tune_lasso\n  ) |&gt; \n  option_add(\n    id = \"rec_ridge\",\n    grid = tune_ridge\n  )\n\ntune_res &lt;- workflow_map(\n  algerian_wf_set,\n  resamples = algeria_bstrap,\n  verbose = TRUE,\n  seed = 123,\n  fn = \"tune_grid\",\n  grid = 20,\n  control = grid_control\n)\n\ntune_res |&gt; \n  collect_metrics()"
  },
  {
    "objectID": "posts/real-estate/index.html#the-data",
    "href": "posts/real-estate/index.html#the-data",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "The Data",
    "text": "The Data\nThis data is available in the public and is collected from UC Irvine Machine Learning Repository, for more data to practice machine learning visit UCirvine.\n\n\nShow the code\nreal_estate &lt;- readxl::read_excel(\"Real estate valuation data set.xlsx\") |&gt; \n  clean_names()\n\nhead(real_estate)\n\n\n# A tibble: 6 × 8\n     no x1_transaction_date x2_house_age x3_distance_to_the_nearest_mrt_station\n  &lt;dbl&gt;               &lt;dbl&gt;        &lt;dbl&gt;                                  &lt;dbl&gt;\n1     1               2013.         32                                     84.9\n2     2               2013.         19.5                                  307. \n3     3               2014.         13.3                                  562. \n4     4               2014.         13.3                                  562. \n5     5               2013.          5                                    391. \n6     6               2013.          7.1                                 2175. \n# ℹ 4 more variables: x4_number_of_convenience_stores &lt;dbl&gt;, x5_latitude &lt;dbl&gt;,\n#   x6_longitude &lt;dbl&gt;, y_house_price_of_unit_area &lt;dbl&gt;"
  },
  {
    "objectID": "posts/real-estate/index.html#data-definition",
    "href": "posts/real-estate/index.html#data-definition",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Data Definition",
    "text": "Data Definition\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nRole\nType\nDescription\nUnits\nMissing Values\n\n\n\n\nNo\nID\nInteger\n\n\nno\n\n\nX1\ntransaction date\nFeature\nContinuous\nfor example, 2013.250=2013 March, 2013.500=2013 June, etc.\nno\n\n\nX2\nhouse age\nFeature\nContinuous\n\nyear\n\n\nX3\ndistance to the nearest MRT station\nFeature\nContinuous\n\nmeter\n\n\nX4\nnumber of convenience stores\nFeature\nInteger\nnumber of convenience stores in the living circle on foot\ninteger\n\n\nX5\nlatitude\nFeature\nContinuous\ngeographic coordinate, latitude\ndegree\n\n\nX6\nlongitude\nFeature\nContinuous\ngeographic coordinate, longitude\ndegree\n\n\nY house price of unit area\nTarget\nContinuous\n10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared\n10000 New Taiwan Dollar/Ping\nno"
  },
  {
    "objectID": "posts/real-estate/index.html#data-preparation",
    "href": "posts/real-estate/index.html#data-preparation",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we will split the date from the Taiwan system to year and month.\n\n\nShow the code\nreal_estate &lt;- real_estate |&gt; \n  mutate(\n    year = x1_transaction_date %/% 1,\n    month = round((x1_transaction_date %% 1) * 12), # to get month from taiwanese date\n    .before = x2_house_age\n  )\n\n\nreal_estate &lt;- real_estate |&gt; \n  mutate(month = case_when(month == 0 ~ 1, TRUE ~ month)) |&gt; \n  select(!c(1, 2))\n\n\nThe names of the variables are a bit long and unclear so we will rename them to make coding easy\n\n\nShow the code\nreal_estate &lt;- real_estate |&gt; \n  rename(\n    age = x2_house_age,\n    distance_to_station = x3_distance_to_the_nearest_mrt_station,\n    number_convenience_stores = x4_number_of_convenience_stores,\n    latitude = x5_latitude,\n    longitude = x6_longitude,\n    price = y_house_price_of_unit_area\n  )\n\nreal_estate &lt;- real_estate |&gt; \n  mutate(\n    age = ceiling(age),\n    sale_date = make_date(year = as.integer(year), month = month),\n    .before = age\n  ) |&gt; \n  select(-c(year, month))\n\nnames(real_estate)\n\n\n[1] \"sale_date\"                 \"age\"                      \n[3] \"distance_to_station\"       \"number_convenience_stores\"\n[5] \"latitude\"                  \"longitude\"                \n[7] \"price\"                    \n\n\nTo get a better grasp of the pricing, the US Dollar will be used, and the size of the houses in square meter will be calculated to give an idea of how big the properties are\n\n\nShow the code\nreal_estate &lt;- real_estate |&gt; \n  mutate(\n    size_m2 = (price * 10000) / 3.9,\n    price_usd = (price * 10000) * 0.032,\n    .before = price\n  )"
  },
  {
    "objectID": "posts/real-estate/index.html#missing-values",
    "href": "posts/real-estate/index.html#missing-values",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Missing values??",
    "text": "Missing values??\nEven if the data is having no missing value when imported, it’s not a bad idea to look for missing data after the preparation which we have made.\n\n\nShow the code\nsum(is.na(real_estate))\n\n\n[1] 0\n\n\nWe can also check for duplicate data point\n\n\nShow the code\nsum(duplicated(real_estate))\n\n\n[1] 0\n\n\nThere are no duplicate data point. We can proceed with our analysis after this."
  },
  {
    "objectID": "posts/real-estate/index.html#target-variable",
    "href": "posts/real-estate/index.html#target-variable",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Target Variable",
    "text": "Target Variable\n\nUnivariate\n\n\nShow the code\nprice_median &lt;- \n  tibble(\n    med = median(real_estate$price_usd),\n    label = paste0(\"$\", med)\n  )\n\nggplot(real_estate, aes(price_usd)) +\n  geom_histogram(binwidth = 500, alpha =0.7, fill = \"wheat3\") +\n  geom_density(stat = \"bin\", binwidth = 500, col = \"brown\") +\n  geom_vline(aes(xintercept = median(price_usd)), col = \"violetred3\") +\n  geom_text(\n    data = price_median,\n    aes(x = med, y = 30, label = label),\n    hjust = -0.3,\n    col = \"red\"\n  ) +\n  labs(\n    x = \"Price\",\n    y = \"count\",\n    title = \"Long-tailed Price distribution\"\n  ) +\n  theme_igray() +\n  scale_x_continuous(label = label_dollar())\n\n\n\n\n\n\n\n\nFigure 1: House price distribution\n\n\n\n\n\nThe most house price ranges between 11000 to 14000 dollars Figure 1. The distribution shows there seems to be an outlier in our data. fig-outlier shows the outlier\n\n\nShow the code\noutlier &lt;- \n  tibble(\n    x = 1,\n    max_price = max(real_estate$price_usd),\n  )\n\n    \nggplot(real_estate, aes(price_usd, x = 1)) +\n  ggbeeswarm::geom_quasirandom(\n    col = \"darkgreen\",\n    shape = \"circle\"\n  ) + \n  geom_point(\n    data = outlier, \n    aes(x, max_price),\n    shape = \"circle filled\", stroke = 1.2, size = 3,\n    fill = \"red\",  col = \"orange\",\n  ) +\n  geom_text(\n    data = outlier,\n    aes(y = max_price, label = \"Outlier\"),\n    vjust = 1.7\n  ) +\n  scale_y_continuous(\n    label = label_dollar(),\n    breaks = seq(0, 40000, 5000)\n  ) +\n  labs(\n    x = \"\",\n    y = \"Price\",\n    title = \"Red dot shows house out that is overprized\"\n  ) +\n  coord_flip() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank()\n  ) +\n  theme_pander()\n\n\n\n\n\n\n\n\nFigure 2: Outlier point significantly overprized above 30000 usd\n\n\n\n\n\nWe need to remove the overprized house\n\n\nShow the code\nreal_estate &lt;- real_estate |&gt; filter(!price_usd &gt; 30000)\n\nrange(real_estate$price_usd)\n\n\n[1]  2432 25056\n\n\nWe will continue our EDA now that the outlier has been removed\n\n\nMultivariate\n\n\nShow the code\nggplot(real_estate, aes(factor(sale_date), price_usd)) +\n  geom_violin(fill = \"olivedrab3\") +\n  geom_jitter(aes(y = price_usd), size = 0.5, alpha = 0.5, col = \"red\") +\n  theme(axis.text.x = element_text(angle = 20)) +\n  labs(x = \"Sale Date\", y = \"Price\", \n       title = \"January and November shows large volume of sales\",\n       subtitle = \"Mid year (May/June) shows increase in house purchase, as sales in other months declines\"\n  ) +\n  scale_y_continuous(label = label_dollar()) +\n  theme_pander()\n\n\n\n\n\nMonthly price distribution of houses, there are some traces of seasonality\n\n\n\n\n\n\nShow the code\nggplot(real_estate, aes(fct_reorder(cut_number(age, 10), price_usd, .fun = sum), price_usd)) +\n  geom_col(fill = \"springgreen3\") +\n  labs(\n    x = \"Age\",\n    y = \"Price\",\n    title = str_wrap(\"New houses age 0 to 4 years fetch made more sales in dollar\n                     in general than old houses\", width = 60)\n  ) +\n  scale_y_continuous(label = label_dollar()) +\n  coord_flip() +\n  theme_igray()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ncorrelation &lt;- cor(real_estate$price_usd, real_estate$age)\n\nggplot(real_estate, aes(price_usd, age)) +\n  geom_smooth(method = \"lm\", se = F, col = \"tomato2\") +\n  expand_limits(y = c(0, 45)) +\n  labs(\n    x = \"Price\",\n    y = \"Age\",\n    title = \"House price reduces as age increases\"\n  )+\n  annotate(\n    geom = \"label\",\n    label = paste(\"correlation:\", round(correlation, 2), sep = \" \"),\n    x = 15000, y = 25, col = \"red\"\n  ) +\n  theme_clean()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 3: Correlation between age and price\n\n\n\n\n\nFigure 3 shows the relationship between house price and the age of houses\n\n\nShow the code\nggplot(real_estate, aes(price_usd, distance_to_station)) +\n  geom_point() +\n  scale_y_log10(label = label_number()) +\n  labs(\n    x = \"Price\",\n    y = \"Distance to Station (m)\",\n    title = \"Negative relationship between Price and Distance to Station\",\n    subtitle = \"Houses closer to the station are costlier\"\n  ) +\n  theme_pander()\n\n\n\n\n\n\n\n\nFigure 4: Correlation between price and distance to station\n\n\n\n\n\n\n\nShow the code\nggplot(real_estate, aes(longitude, latitude, col = price_usd)) +\n  geom_jitter() +\n  labs(\n    col = \"Price (USD)\",\n    x = \"Longitude\",\n    y = \"Latitude\",\n    title = \"The prices of houses increases as we move North East\",\n    subtitle = str_wrap(\"Prices of houses increases where there are clusters\\ of house, this\n                        may be due to the proximity to the MRT station\", width = 55)\n  ) +\n  scale_colour_gradient(low = \"gray\", high = \"red\") +\n  theme_pander() +\n  theme(legend.position = \"top\") +\n  guides(\n    color = guide_colorbar(barwidth = 15, barheight = 1/2, ticks.colour = \"black\", title.position = \"left\", title.theme = element_text(size = 8)))\n\n\n\n\n\nHouses get expensive as we move in a northeast direction,\n\n\n\n\n\n\nCorrelation with other variables\n\n\nShow the code\nggcorr(real_estate |&gt; select(!c(sale_date, price)))\n\n\n\n\n\nAll the factors shows strong relationship with the price of the building\n\n\n\n\nWhile size, number of convenience store close to the building and the position of the building, i.e., longitude and latitude are positively correlated to the price of a building, the older a building, and the farther it is from the MRT station the more likely it reduces in price."
  },
  {
    "objectID": "posts/real-estate/index.html#data-splitting",
    "href": "posts/real-estate/index.html#data-splitting",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Data Splitting",
    "text": "Data Splitting\n\n\nShow the code\nset.seed(333)\n\n\nreal_estate_split &lt;- initial_split(real_estate, prop = .8, strata = price_usd)\n\nreal_estate_train &lt;- training(real_estate_split)\nreal_estate_test &lt;- testing(real_estate_split)\n\nreal_estate_split\n\n\n&lt;Training/Testing/Total&gt;\n&lt;328/85/413&gt;"
  },
  {
    "objectID": "posts/real-estate/index.html#model-specification",
    "href": "posts/real-estate/index.html#model-specification",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Model Specification",
    "text": "Model Specification\nGiven our choice of model, XGBoost, a tree-based model, a lot of preprocessing is not required, we can going to dive right into our model specification, and tune a lot of the model hyperparameter to reduce the chances of over-fitting and under-fitting.\n\n\nShow the code\nxg_model &lt;- \n  boost_tree(\n    mtry = tune(), min_n = tune(),\n    tree_depth = tune(), trees = 1000,\n    loss_reduction = tune(),\n    sample_size = tune(),\n    learn_rate = tune(),\n  ) |&gt; \n  set_engine(\"xgboost\") |&gt; \n  set_mode(\"regression\")\n\nxg_model |&gt;  translate()\n\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n\nModel fit template:\nparsnip::xgb_train(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    colsample_bynode = tune(), nrounds = 1000, min_child_weight = tune(), \n    max_depth = tune(), eta = tune(), gamma = tune(), subsample = tune(), \n    nthread = 1, verbose = 0)"
  },
  {
    "objectID": "posts/real-estate/index.html#workflow-process",
    "href": "posts/real-estate/index.html#workflow-process",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Workflow Process",
    "text": "Workflow Process\nTo improve efficiency and streamline processes, we start a modelling workflow.\n\n\nShow the code\nxg_wf &lt;- workflow() |&gt; \n  add_formula(price_usd ~ .) |&gt; \n  add_model(xg_model)\n\nxg_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nprice_usd ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost"
  },
  {
    "objectID": "posts/real-estate/index.html#cross-validation",
    "href": "posts/real-estate/index.html#cross-validation",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Cross Validation",
    "text": "Cross Validation\nNext, we create resamples for tuning the model, Table 1.\n\n\n\nTable 1: 10 Cross Fold Resamples\n\n\n\nShow the code\nset.seed(222)\n\nreal_estate_folds &lt;- vfold_cv(real_estate_train, strata = price_usd)"
  },
  {
    "objectID": "posts/real-estate/index.html#tune-grid",
    "href": "posts/real-estate/index.html#tune-grid",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Tune Grid",
    "text": "Tune Grid\nNext, we have to set up some values for our hyperparameter, we don’t want to exhaust our computing resource, and face the risk of overfitting. We will use the Latin Hypercube grid as this approach can be more computationally efficient than a regular grid, especially when there are many hyperparameters to tune. Random selection can also introduce diversity into the search process.\n\nShow the code\nset.seed(3434)\n\n\nxgb_grid &lt;- grid_latin_hypercube(\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), real_estate_train),\n  learn_rate(),\n  size = 30\n)\n\n\nShow the code\nxgb_grid\n\n\n\n\nTable 2: XGBoost Tune Grid\n\n\n\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n\n\n# A tibble: 30 × 6\n   tree_depth min_n loss_reduction sample_size  mtry learn_rate\n        &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;\n 1          5    32       1.17e- 6       0.529     4   8.06e- 3\n 2          6    16       3.05e- 1       0.446     3   5.03e- 7\n 3         15    16       3.58e- 9       0.566     2   2.56e- 6\n 4         13    36       5.57e- 1       0.637     3   7.82e- 6\n 5         10     7       2.58e- 4       0.668     5   7.03e-10\n 6          5    20       1.11e- 2       0.491     7   5.75e- 3\n 7          8    13       3.64e-10       0.339     6   1.05e- 7\n 8         13    25       2.87e+ 0       0.937     1   2.94e-10\n 9          4    31       1.55e+ 0       0.827     2   8.75e- 7\n10          5    14       7.17e- 4       0.771     5   1.64e-10\n# ℹ 20 more rows\n\n\n\n\nSince mtry depends on the number of predictors, it had to be tuned differently Table 2.\nNOW WE TUNE. We will use our resamples, the tuneable workflow, and the Latin grid of parameters which we have to try get the best value. To also speed up the process, we will enable parallel computing\n\n\nShow the code\ndoParallel::registerDoParallel()\n\nset.seed(222)\n\nxg_tune_res &lt;- tune_grid(\n  xg_wf,\n  resamples = real_estate_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = T)\n)"
  },
  {
    "objectID": "posts/real-estate/index.html#exploring-tune-results",
    "href": "posts/real-estate/index.html#exploring-tune-results",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Exploring Tune Results",
    "text": "Exploring Tune Results\n\n\nShow the code\nxg_tune_res |&gt; \n  collect_metrics() |&gt; \n  filter(.metric == \"rmse\") |&gt; \n  select(mean, mtry:sample_size) |&gt; \n  pivot_longer(\n    mtry:sample_size,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) |&gt; \n  ggplot(aes(value, mean, color = parameter)) +\n  geom_jitter(show.legend = F, width = .4) +\n  facet_wrap(~parameter, scales = \"free_y\")\n\n\n\n\n\n\n\n\nFigure 5: Tuning result\n\n\n\n\n\nThe lower the rmse, the better the model, a simplification, but this is not always the case. We will stick to that for now.\nLet’s show the best performing set of parameter"
  },
  {
    "objectID": "posts/real-estate/index.html#best-tune",
    "href": "posts/real-estate/index.html#best-tune",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Best Tune",
    "text": "Best Tune\n\n\nShow the code\nshow_best(xg_tune_res, metric = \"rmse\")\n\n\n# A tibble: 5 × 12\n   mtry min_n tree_depth learn_rate loss_reduction sample_size .metric\n  &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n1     4     3         13    0.0145     0.0000176         0.704 rmse   \n2     8    18          6    0.0578     0.0000239         0.966 rmse   \n3     3     8          3    0.0422     0.00411           0.688 rmse   \n4     7    20          5    0.00575    0.0111            0.491 rmse   \n5     6     5         10    0.00269    0.000000326       0.893 rmse   \n# ℹ 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;,\n#   .config &lt;chr&gt;"
  },
  {
    "objectID": "posts/real-estate/index.html#select-best-parameter",
    "href": "posts/real-estate/index.html#select-best-parameter",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Select Best Parameter",
    "text": "Select Best Parameter\n\n\nShow the code\nbest_rmse &lt;- select_best(xg_tune_res, metric = \"rmse\")\nbest_rmse\n\n\n# A tibble: 1 × 7\n   mtry min_n tree_depth learn_rate loss_reduction sample_size .config          \n  &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1     4     3         13     0.0145      0.0000176       0.704 Preprocessor1_Mo…\n\n\nNow we can finalize the model\n\n\nShow the code\nfinal_boost_tree &lt;- finalize_workflow(\n  xg_wf,\n  best_rmse\n)\n\nfinal_boost_tree\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nprice_usd ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = 4\n  trees = 1000\n  min_n = 3\n  tree_depth = 13\n  learn_rate = 0.0145039211746767\n  loss_reduction = 1.76136801549921e-05\n  sample_size = 0.70429474228993\n\nComputational engine: xgboost"
  },
  {
    "objectID": "posts/real-estate/index.html#variable-importance",
    "href": "posts/real-estate/index.html#variable-importance",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Variable Importance",
    "text": "Variable Importance\nLet’s see the most important variables in the model.\n\n\nShow the code\nlibrary(vip)\n\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\nShow the code\nfinal_boost_tree |&gt; \n  fit(data = real_estate_train) |&gt; \n  pull_workflow_fit() |&gt; \n  vip(\n    geom = \"col\",\n    aesthetics = list(fill = \"springgreen3\")\n  ) +\n  theme_pander()\n\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nℹ Please use `extract_fit_parsnip()` instead.\n\n\n\n\n\nFeature importance\n\n\n\n\nThe most important predictor of the price of a house are the:\n\nSize\nDistance to the station,\nThe latitude of the buildings, and\nThe number of convenience stores."
  },
  {
    "objectID": "posts/real-estate/index.html#investigating-missing-values",
    "href": "posts/real-estate/index.html#investigating-missing-values",
    "title": "Real Estate Prediction Using Boosting Tree (XGBoost)",
    "section": "Investigating missing values",
    "text": "Investigating missing values\nEven if the data is having no missing value when imported, it’s not a bad idea to look for missing data after the preparation which we have made.\n\n\nShow the code\nsum(is.na(real_estate))\n\n\n[1] 0\n\n\nWe can also check for duplicate data point\n\n\nShow the code\nsum(duplicated(real_estate))\n\n\n[1] 0\n\n\nThere are no duplicate data point. We can proceed with our analysis after this."
  },
  {
    "objectID": "posts/wine-qual/index.html",
    "href": "posts/wine-qual/index.html",
    "title": "The Algorithmic Grape",
    "section": "",
    "text": "Wine appreciation is an art form enjoyed by many. However, beyond the subjective experience of taste, there lies a science behind wine quality. Winemakers strive to produce exceptional vintages by carefully controlling various factors during the production process. This project explores the fascinating world of wine quality, specifically focusing on the ability to predict it based on measurable chemical properties.\n\n\nThis project aims to develop a model that can estimate the quality of a wine using its chemical composition. By analyzing features such as acidity, residual sugar content, and sulfur dioxide levels, we aim to unlock valuable insights into the factors that contribute to a superior wine. This model can potentially benefit wine producers, retailers, and even consumers seeking a more informed approach to wine selection."
  },
  {
    "objectID": "posts/wine-qual/index.html#objective",
    "href": "posts/wine-qual/index.html#objective",
    "title": "The Algorithmic Grape",
    "section": "",
    "text": "This project aims to develop a model that can estimate the quality of a wine using its chemical composition. By analyzing features such as acidity, residual sugar content, and sulfur dioxide levels, we aim to unlock valuable insights into the factors that contribute to a superior wine. This model can potentially benefit wine producers, retailers, and even consumers seeking a more informed approach to wine selection."
  },
  {
    "objectID": "posts/wine-qual/index.html#data-source",
    "href": "posts/wine-qual/index.html#data-source",
    "title": "The Algorithmic Grape",
    "section": "Data Source",
    "text": "Data Source\nThe data for this project was obtained from the UCI Machine Learning Repository, a renowned resource for publicly available datasets. This repository offers a wealth of datasets for various machine learning tasks, making it a valuable resource for researchers and data scientists.\nThe specific dataset we utilized is titled Wine Quality and focuses on Portuguese Vinho Verde wines.   \nVinho Verde is a light-bodied wine known for its crisp acidity and refreshing taste. The dataset encompasses two separate datasets, one for red wines and another for white wines. For this project, we chose to focus on the white wine dataset."
  },
  {
    "objectID": "posts/wine-qual/index.html#data-definition",
    "href": "posts/wine-qual/index.html#data-definition",
    "title": "The Algorithmic Grape",
    "section": "Data Definition",
    "text": "Data Definition\nThe dataset contains more than 4500 wine samples, each characterized by 11 chemical properties measured during the winemaking process. These features include factors like:\n\nFixed Acidity: Level of acidity due to tartaric acid.\nVolatile Acidity: Level of acidity due to volatile acids like acetic acid.\nAlcohol: Percentage of alcohol content by volume\nResidual Sugar: Amount of remaining sugar after fermentation.\npH: Measure of acidity on a logarithmic scale.\nCitric Acid: Minute quantity naturally present in grapes. Winemakers may add small amounts to increase tartness and prevent haze formation.\nChlorides: Level of chloride salts that can influence a wine’s saltiness and bitterness.\nFree Sulfur Dioxide: Amount of unbound sulfur dioxide (SO2) gas, a preservative commonly used to prevent spoilage by bacteria and oxidation.\nTotal Sulfur Dioxide: The total level of SO2 gas, including both free and bound forms.\nDensity: The relative density of the wine compared to water. It can be an indicator of the wine’s alcohol content and sugar level.\nSulphates: The level of sulfate salts, which can influence a wine’s overall mouth-feel and perception of bitterness.\nQuality: This is the target variable, a score between 0 and 10 representing the perceived sensory quality of the wine according to human experts. This score serves as the target variable for our machine learning model, allowing us to predict the quality of a new wine based on its chemical makeup."
  },
  {
    "objectID": "posts/wine-qual/index.html#setting-up-the-analysis-environment",
    "href": "posts/wine-qual/index.html#setting-up-the-analysis-environment",
    "title": "The Algorithmic Grape",
    "section": "Setting up the Analysis Environment",
    "text": "Setting up the Analysis Environment\n\n\nTo begin our analysis, we’ll import essential libraries using the pacman package for efficient dependency management.\n\n\nShow the code\nlibrary(pacman)\n\np_load(tidyverse, tidymodels, gt, kableExtra, patchwork)"
  },
  {
    "objectID": "posts/wine-qual/index.html#data-inspection-a-crucial-step",
    "href": "posts/wine-qual/index.html#data-inspection-a-crucial-step",
    "title": "The Algorithmic Grape",
    "section": "Data Inspection: A Crucial Step",
    "text": "Data Inspection: A Crucial Step\nA crucial step in data analysis, after defining our goals and collecting data, is to inspect its quality and structure. This involves examining the data to identify potential issues like missing values, inconsistencies, or formatting errors. Table 1 below gives a preview of our data.\n\n\n\n\nTable 1: Data Preview\n\n\n\n\n  \n  \n\n\n\nData Preview: Wine Quality for White Wine\n\n\nAcidity\nCitric Acid\nResidual Sugar\nChlorides\nSulfur dioxide\nDensity\npH\nSulphates\nAlcohol\nQuality\n\n\nFixed\nVolatile\nFree\nTotal\n\n\n\n\n7.0\n0.27\n0.36\n20.7\n0.045\n45\n170\n1.0010\n3.00\n0.45\n8.8\n6\n\n\n6.3\n0.30\n0.34\n1.6\n0.049\n14\n132\n0.9940\n3.30\n0.49\n9.5\n6\n\n\n8.1\n0.28\n0.40\n6.9\n0.050\n30\n97\n0.9951\n3.26\n0.44\n10.1\n6\n\n\n7.2\n0.23\n0.32\n8.5\n0.058\n47\n186\n0.9956\n3.19\n0.40\n9.9\n6\n\n\n7.2\n0.23\n0.32\n8.5\n0.058\n47\n186\n0.9956\n3.19\n0.40\n9.9\n6\n\n\n8.1\n0.28\n0.40\n6.9\n0.050\n30\n97\n0.9951\n3.26\n0.44\n10.1\n6\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nglimpse(wine_tbl)\n\n\nRows: 4,898\nColumns: 12\n$ fixed_acidity        &lt;dbl&gt; 7.0, 6.3, 8.1, 7.2, 7.2, 8.1, 6.2, 7.0, 6.3, 8.1,…\n$ volatile_acidity     &lt;dbl&gt; 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.32, 0.27, 0…\n$ citric_acid          &lt;dbl&gt; 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.16, 0.36, 0…\n$ residual_sugar       &lt;dbl&gt; 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.00, 20.70,…\n$ chlorides            &lt;dbl&gt; 0.045, 0.049, 0.050, 0.058, 0.058, 0.050, 0.045, …\n$ free_sulfur_dioxide  &lt;dbl&gt; 45, 14, 30, 47, 47, 30, 30, 45, 14, 28, 11, 17, 1…\n$ total_sulfur_dioxide &lt;dbl&gt; 170, 132, 97, 186, 186, 97, 136, 170, 132, 129, 6…\n$ density              &lt;dbl&gt; 1.0010, 0.9940, 0.9951, 0.9956, 0.9956, 0.9951, 0…\n$ p_h                  &lt;dbl&gt; 3.00, 3.30, 3.26, 3.19, 3.19, 3.26, 3.18, 3.00, 3…\n$ sulphates            &lt;dbl&gt; 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.47, 0.45, 0…\n$ alcohol              &lt;dbl&gt; 8.8, 9.5, 10.1, 9.9, 9.9, 10.1, 9.6, 8.8, 9.5, 11…\n$ quality              &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7, 6…\n\n\nUsing the glimpse function we can get a high-level overview of the wine quality data and the data follows the expected structure in the data definition. We have 4898 rows and 12 columns. All the columns are numeric variables.\n\n\nShow the code\nskimr::skim(wine_tbl) |&gt; \n  select(-skim_type) |&gt; \n  gt() |&gt; \n  cols_label(\n    skim_variable = \"Variable\",\n    n_missing = \"Missing Value\",\n    complete_rate = \"Complete\",\n    numeric.mean = \"Mean\",\n    numeric.sd = \"Standard Deviation\",\n    numeric.p0 = \"Min\",\n    numeric.p25 = \"1st Percentile\",\n    numeric.p50 = \"Median\",\n    numeric.p75 = \"3rd Percentile\",\n    numeric.p100 = \"Max\",\n    numeric.hist = \"Histogram\"\n  ) |&gt; \n  tab_header(\n    title = \"White Wine Quality Descriptive Statistics\"\n  ) |&gt; \n  opt_stylize(\n    style = 3,\n    color = \"gray\"\n  ) |&gt; \n  as_raw_html()\n\n\n\n\nTable 2: Wine data summary\n\n\n\n\n  \n  \n\n\n\nWhite Wine Quality Descriptive Statistics\n\n\nVariable\nMissing Value\nComplete\nMean\nStandard Deviation\nMin\n1st Percentile\nMedian\n3rd Percentile\nMax\nHistogram\n\n\n\n\nfixed_acidity\n0\n1\n6.85478767\n0.843868228\n3.80000\n6.3000000\n6.80000\n7.3000\n14.20000\n▁▇▁▁▁\n\n\nvolatile_acidity\n0\n1\n0.27824112\n0.100794548\n0.08000\n0.2100000\n0.26000\n0.3200\n1.10000\n▇▅▁▁▁\n\n\ncitric_acid\n0\n1\n0.33419151\n0.121019804\n0.00000\n0.2700000\n0.32000\n0.3900\n1.66000\n▇▆▁▁▁\n\n\nresidual_sugar\n0\n1\n6.39141486\n5.072057784\n0.60000\n1.7000000\n5.20000\n9.9000\n65.80000\n▇▁▁▁▁\n\n\nchlorides\n0\n1\n0.04577236\n0.021847968\n0.00900\n0.0360000\n0.04300\n0.0500\n0.34600\n▇▁▁▁▁\n\n\nfree_sulfur_dioxide\n0\n1\n35.30808493\n17.007137325\n2.00000\n23.0000000\n34.00000\n46.0000\n289.00000\n▇▁▁▁▁\n\n\ntotal_sulfur_dioxide\n0\n1\n138.36065741\n42.498064554\n9.00000\n108.0000000\n134.00000\n167.0000\n440.00000\n▂▇▂▁▁\n\n\ndensity\n0\n1\n0.99402738\n0.002990907\n0.98711\n0.9917225\n0.99374\n0.9961\n1.03898\n▇▂▁▁▁\n\n\np_h\n0\n1\n3.18826664\n0.151000600\n2.72000\n3.0900000\n3.18000\n3.2800\n3.82000\n▁▇▇▂▁\n\n\nsulphates\n0\n1\n0.48984688\n0.114125834\n0.22000\n0.4100000\n0.47000\n0.5500\n1.08000\n▃▇▂▁▁\n\n\nalcohol\n0\n1\n10.51426705\n1.230620568\n8.00000\n9.5000000\n10.40000\n11.4000\n14.20000\n▃▇▆▃▁\n\n\nquality\n0\n1\n5.87790935\n0.885638575\n3.00000\n5.0000000\n6.00000\n6.0000\n9.00000\n▁▅▇▃▁\n\n\n\n\n\n\n\n\n\n\nTable 2 summarizes the key characteristics of our data. It reveals:\n\nNo Missing Values: There are no missing data points, ensuring a complete dataset for our analysis.\nDescriptive Statistics: Table 2 presents various descriptive statistics for each variable. This includes the minimum and maximum values highlighting the data’s range. Additionally, the first and third quartile provides an insight of the distribution of values in the data. Finally the mean and standard deviation indicate the average and spread of each variables relative to their mean.\nOutliers: Initial exploration of the data suggests the presence of potential outliers in several features. Specifically, the maximum values for variables fixed_acidity, volatile_acidity, citric_acid, residual_sugar, chlorides, free_sulphur_dioxide, and total_sulfur_dioxide seems deviate significantly from the rest of the distribution.\n\nThe next step is to statistically confirm these observations, we can’t rely solely on inspecting the maximum values. Instead, we’ll employ appropriate outlier detection tests. Due to the fairly large data size we have, we can use either of:\n\nGrubb’s test\nRosner’s test\n\nThese outlier detection tests help us statistically determine if the observed extreme values are likely outliers within the dataset. Both tests listed here are particularly useful for identifying outliers in a dataset that is approximately normally distributed."
  },
  {
    "objectID": "posts/wine-qual/index.html#univariate-inspection",
    "href": "posts/wine-qual/index.html#univariate-inspection",
    "title": "The Algorithmic Grape",
    "section": "Univariate Inspection",
    "text": "Univariate Inspection\n\nDealing with Outliers\nBefore diving into formal tests, it’s important we get a sense of our data. We can visualize each variables of our data to inspect their distribution.\n\nVisual Inspection\n\nShow the code\nwine_tbl_longer &lt;- wine_tbl |&gt; \n  pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  )\n\nwine_tbl_longer |&gt; \n  filter(variable != \"quality\") |&gt; \n  ggplot(aes(sample = value)) +\n  geom_qq(alpha = .2, col = \"violetred4\") +\n  geom_qq_line(col = \"gray3\") +\n  facet_wrap(~variable, scales = \"free\") +\n  labs(\n    x = \"Theoretical distribution\",\n    y = \"Sample\",\n    title = \"Variables are not normally distributed\"\n  ) +\n  theme_minimal()\n\nwine_tbl_longer |&gt; \n  filter(variable !=\"quality\") |&gt; \n  ggplot(aes(value)) +\n  facet_wrap(~variable, scales = \"free\") +\n  geom_histogram(fill = \"burlywood\", col = \"black\") +\n  geom_density(\n    stat = \"bin\",\n    col = \"violetred\"\n  ) +\n  labs(\n    x = \"Value\",\n    y = \"Frequency\",\n    title = \"Variables are fairly normal\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) QQ plot of all the variables shows they are not normally distributed\n\n\n\n\n\n\n\n\n\n\n\n(b) histogram of all variables showing tails\n\n\n\n\n\n\n\n\nshows the qqplot with variables like total and free sulful_dioxide and p_h approximately following normal distribution,(b) exposes the distribution of all variables which are mostly right-skewed.\n\n\n\n\n\nFigure 1\n\n\n\nFigure 1 shows some fairly normal data, but it won’t be bad to test for normality using Shapiro-wilk’s test. We can also use the qqPlot() function from the car package which I really like to test for the normality.\n\n\nShapiro Test\n\n\nShow the code\nnormality_test &lt;- wine_tbl |&gt; \n  map(~tidy(shapiro.test(.))) |&gt; \n  bind_rows(.id = \"variable\") |&gt; \n  select(-method) |&gt; \n  janitor::clean_names() |&gt; \n  mutate(\n    variable = str_to_title(str_replace_all(variable, \"_\", \" \"))\n  )\nactual_colnames &lt;- colnames(normality_test)\ndesired_colnames &lt;- str_to_title(names(normality_test))\nnames(desired_colnames) &lt;- actual_colnames\n  \nnormality_test |&gt; \n  filter(variable != \"Quality\") |&gt; \n  gt()  |&gt; \n  cols_label(.list = desired_colnames) |&gt; \n  tab_header(\n    title = \"Shapiro-wilk Normality Test\"\n  ) |&gt; \n  opt_stylize(\n    style = 3,\n    color = \"gray\"\n  ) |&gt; \n  as_raw_html()\n\n\n\n\nTable 3: Shapiro-wilk’s normality test: All varibles are not normally distributed\n\n\n\n\n  \n  \n\n\n\nShapiro-wilk Normality Test\n\n\nVariable\nStatistic\nP_value\n\n\n\n\nFixed Acidity\n0.9765615\n1.150151e-27\n\n\nVolatile Acidity\n0.9045497\n4.586797e-48\n\n\nCitric Acid\n0.9222473\n1.013179e-44\n\n\nResidual Sugar\n0.8845686\n2.820710e-51\n\n\nChlorides\n0.5908084\n2.140584e-75\n\n\nFree Sulfur Dioxide\n0.9420691\n3.857845e-40\n\n\nTotal Sulfur Dioxide\n0.9890146\n4.383453e-19\n\n\nDensity\n0.9548048\n1.780895e-36\n\n\nP H\n0.9880965\n6.505521e-20\n\n\nSulphates\n0.9516094\n1.821979e-37\n\n\nAlcohol\n0.9553024\n2.569014e-36\n\n\n\n\n\n\n\n\n\n\n\n\nCar’s qqPlot Test\n\nShow the code\ncar::qqPlot(\n  wine_tbl$p_h,\n  ylab = paste0(\"pH\"),\n  main = \"Quantile-quantile plot for pH variable\"\n)\ncar::qqPlot(\n  wine_tbl$alcohol,\n  ylab = paste0(\"pH\"),\n  main = \"Quantile-quantile plot for alcohol variable\"\n)\n\n\n\n\n\n\n[1] 1251 1256\n\n\n[1] 3919 4504\n\n\n\n\n\n\n\n\n\n\n(a) pH\n\n\n\n\n\n\n\n\n\n\n\n(b) alcohol\n\n\n\n\n\n\n\nFigure 2: Normality test with the car’s package qqPlot function\n\n\n\nTable 3 shows all the variables do not follow a normal distribution, Figure 2 stresses this and we can clearly see the presence of outlier.\n\n\n\nWhere Domain Knowledge Shines\nWhile our initial exploration identified potential outliers in the data, it’s important to consider them in the context of wine making expertise. Domain knowledge from experienced professionals like Cathy Howard suggests that the expected range for residual sugar in wine can vary widely, from as low as 0.3 g/L to over 35 g/L (as referenced in this post).\nThis information helps us understand that some seemingly extreme values in our data might actually be plausible for certain wine types. Similarly, consulting resources like Waterhouse’s post on “What’s in Wine”. can provide valuable insights into the expected ranges for other wine components.\nGiven the new information about our data and the supposed outliers we investigated we can proceed with our analysis thanks to domain knowledge."
  },
  {
    "objectID": "posts/wine-qual/index.html#response-variable-quality",
    "href": "posts/wine-qual/index.html#response-variable-quality",
    "title": "The Algorithmic Grape",
    "section": "Response Variable (Quality)",
    "text": "Response Variable (Quality)\nThere are two ways we can proceed with our analysis based on our response variable, we can either take quality as a categorical variable or a continuous variable, I prefer and will proceed with the later.\n\n\nShow the code\nwine_tbl |&gt; \n  ggplot(aes(quality)) +\n  geom_histogram(\n    binwidth = 1,\n    fill = \"violetred4\"\n  ) +\n  geom_density(\n    stat = \"count\",\n    col = \"orangered\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(1, 10, 1)\n  ) +\n  labs(\n    x = \"Quality\",\n    y = \"Frequency\",\n    title = \"White wine quality frequency\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = .5, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nFigure 3: Frequency of the white wine quality ratings shows that the wine is mostly rate between 5 to 7\n\n\n\n\n\nThe distribution of wine ratings in Figure 3 shows a range between 3 and 9. This is the same as what we have in Table 2. White wines rated 6 and 5 are the most occurring wine."
  },
  {
    "objectID": "posts/wine-qual/index.html#correlation-matrix-and-relationship-between-variables",
    "href": "posts/wine-qual/index.html#correlation-matrix-and-relationship-between-variables",
    "title": "The Algorithmic Grape",
    "section": "Correlation Matrix and Relationship Between Variables",
    "text": "Correlation Matrix and Relationship Between Variables\nIt is important to see how the different variables relates to wine quality.\n\n\nShow the code\nGGally::ggcorr(\n  wine_tbl,\n  method = \"pairwise\",\n  geom = \"circle\",\n  palette = \"Spectral\",\n  max_size = 10,\n  min_size = 2,\n  label = TRUE,\n  size = 3,\n  hjust = .95\n) +\n  ggtitle(\"Wine Quality Feature Correlations\") +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nFigure 4: Correlation Plot\n\n\n\n\n\nFigure 4 shows some noteworthy relationships between the variables. Here are the key observations:\n\nQuality and Alcohol: Alcohol shows a moderate positive correlation with quality of wine\nAlcohol and Density: A strong negative correlation exists between alcohol content and density. This implies that as alcohol level increases, the density of the wine decreases.\nQuality and Density: There’s moderate negative relationship between the quality and wine density.\nResidual Sugar and Density: The density of wine is largely affected by the amount of residual sugar present in the wine\nTotal Sulfur Dioxide and Free Sulfur Dioxide: A moderate positive correlation is observed between total sulfur dioxide and free sulfur dioxide. This indicates that wines with higher total sulfur dioxide content also tend to have higher levels of unbound free sulfur dioxide.\n\nOverall there’s a fair distribution of correlation across all the variables ranging from as low as ± 0.1 to ± 0.8."
  },
  {
    "objectID": "posts/wine-qual/index.html#data-splitting-strategy",
    "href": "posts/wine-qual/index.html#data-splitting-strategy",
    "title": "The Algorithmic Grape",
    "section": "Data Splitting Strategy",
    "text": "Data Splitting Strategy\nTo ensure robust model evaluation, we’ll employ a data splitting strategy. This involves dividing the dataset into two portions:\n\nTraining Set (80%): This larger portion of the data will be used to train the models. The training process involves fitting the model parameters to the data, allowing the model to learn the underlying relationships between the features (independent variables) and the target variable (wine quality).\nTest Set (20%): This smaller portion of the data will be reserved for testing the final model performance. We will not use the test set for training the models in any way. Instead, the trained model will be applied to the test set, and its predictions will be compared to the actual quality scores to assess the model’s generalizability and accuracy on unseen data.\n\nRationale for Splitting:\nThe 80/20 split is a common choice for data splitting, but depending on the size of your dataset, a 70/30 split might also be acceptable. The key idea is to have a sufficient amount of data for both training and testing purposes. The training set allows the model to learn, while the test set provides an unbiased assessment of how well the model performs on new data. To ensure balance in the distribution of the response variable (quality) between the test and the training data, we’ll execute the split taking into account the distribution of the response variable, as seen in Figure 3 the response variable is unevenly distributed.\nTo ensure reproducibility of the code, we’ll also set.seed into 34433\n\n\nShow the code\nset.seed(34433)\nwine_split &lt;- initial_split(wine_tbl, prop = .8, strata = quality)\nwine_train &lt;- training(wine_split)\nwine_test &lt;- testing(wine_split)\n\n\nWe also need to evaluate the developed models, for this, we will use a cross-fold validation.\n\n\nShow the code\nset.seed(33323)\nwine_folds &lt;- vfold_cv(wine_train, v = 10)"
  },
  {
    "objectID": "posts/wine-qual/index.html#model-specification",
    "href": "posts/wine-qual/index.html#model-specification",
    "title": "The Algorithmic Grape",
    "section": "Model Specification",
    "text": "Model Specification\nTo ensure a good modeling workflow, we specify our model engines, and prepare to tune some parameters, for example, the decision tree model specification, we do not know the tree depth that would be optimal, so we tune this parameter. Other parameters which cannot be predetermine when fitting our model are also tuned.\n\n\nShow the code\nglm_spec &lt;- linear_reg() |&gt; # OLS specification\n  set_engine(\"glm\", family = \"poisson\") |&gt; \n  set_mode(\"regression\")\n\ndec_tree_spec &lt;- decision_tree( # Decision tree specification\n  tree_depth = tune(),\n  cost_complexity = tune()\n) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\nrand_for_spec &lt;- rand_forest( # Random forest specification\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) |&gt; \n  set_engine(\"ranger\", importance = \"impurity\") |&gt; \n  set_mode(\"regression\")\n\nknn_spec &lt;- nearest_neighbor(\n  neighbors = tune(),\n  dist_power = tune(),\n  weight_func = tune()\n) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\")\n\nglm_spec\n\n\nLinear Regression Model Specification (regression)\n\nEngine-Specific Arguments:\n  family = poisson\n\nComputational engine: glm \n\n\nShow the code\ndec_tree_spec\n\n\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n\nComputational engine: rpart \n\n\nShow the code\nrand_for_spec\n\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\nShow the code\nknn_spec\n\n\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n  weight_func = tune()\n  dist_power = tune()\n\nComputational engine: kknn"
  },
  {
    "objectID": "posts/wine-qual/index.html#fine-tuning-the-ingredients-feature-engineering",
    "href": "posts/wine-qual/index.html#fine-tuning-the-ingredients-feature-engineering",
    "title": "The Algorithmic Grape",
    "section": "Fine Tuning the Ingredients: Feature Engineering",
    "text": "Fine Tuning the Ingredients: Feature Engineering\nNow that we’ve chosen our recipe testers (models), let’s take a look at the ingredients (features) we’ll be using. Ideally, we want the ingredients to be in a format that the models can easily understand and use for their predictions. This process of preparing the data is called feature engineering.\nThere are many resources available to help with feature engineering, like this one link. that provides a reference for different models and recommended pre-processing techniques.\nIn our case, we’ll start by testing the models on the features without any preprocessing. Some models, like the ones we’ve chosen, can handle data in its raw form and might not necessarily require specific transformations.\nHowever, that doesn’t mean there’s no room for improvement! We can explore additional options like:\n\nLog transformation: This can be helpful for features with skewed distributions, making them more symmetrical and easier for some models to work with.\nScaled transformations (standardization or normalization): This addresses features with differing scales and units.\n\n\nFormula Recipe\nHere, we assume no preprocessing.\n\n\nShow the code\nformula_rec &lt;- recipe(\n  quality ~ .,\n  data = wine_train\n)\nformula_rec\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\nlog_zv preproc\nFor the second preprocessing, we do the following: - Perform a log transformation on the all predictors, step_log.\n\nScale the variables so they are all on a similar scale.\n\n\n\nShow the code\nlog_sc_rec &lt;- formula_rec |&gt; \n  step_log(all_predictors()) |&gt; \n  step_zv(all_predictors())\n\nlog_sc_rec\n\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Operations \n\n\n• Log transformation on: all_predictors()\n\n\n• Zero variance filter on: all_predictors()\n\n\nA preview of the preprocess object is given below Table 4\n\n\n\n\nTable 4: Variables after log transformation and removing zero variance variables\n\n\n\n\n  \n  \n\n\n\nPreview of Preprocessed data\n\n\nAcidity\nCitric Acid\nResidual Sugar\nChlorides\nSulfur dioxide\nDensity\npH\nSulphates\nAlcohol\nQuality\n\n\nFixed\nVolatile\nFree\nTotal\n\n\n\n\n2.091864\n-1.3093333\n-0.8915981\n0.3715636\n-3.411248\n2.397895\n4.143135\n-0.009242581\n1.095273\n-0.5798185\n2.484907\n5\n\n\n2.151762\n-1.4696760\n-0.9162907\n1.4350845\n-3.352407\n2.833213\n4.691348\n-0.005314095\n1.144223\n-0.6348783\n2.272126\n5\n\n\n2.066863\n-1.7147984\n-0.9942523\n0.1823216\n-3.218876\n2.772589\n4.317488\n-0.008032172\n1.156881\n-0.4620355\n2.379546\n5\n\n\n2.116256\n-0.8675006\n-0.4780358\n2.9575111\n-3.218876\n3.713572\n5.147494\n0.000199980\n1.091923\n-0.4004776\n2.272126\n5\n\n\n1.871802\n-1.1711830\n-1.9661129\n2.0149030\n-3.123566\n3.526361\n4.890349\n-0.004510155\n1.169381\n-0.6931472\n2.251292\n5\n\n\n2.028148\n-0.4004776\n-1.9661129\n0.4054651\n-2.603690\n3.218876\n5.123964\n-0.006319929\n1.115142\n-0.6733446\n2.230014\n5\n\n\n\n\n\n\n\n\n\n\nAfter applying a log transformation, it’s common to encounter values like NaN (Not a Number), NA (missing values), or Inf (infinity). To ensure our models can process the data effectively, we should check for these potential issues before fitting the model. A quick summary of the preprocessed data can reveal their presence.\n\n\nShow the code\nlog_sc_preprocessed &lt;- prep(log_sc_rec) |&gt; \n  juice()\n\nskimr::skim(log_sc_preprocessed) |&gt; \n  gt() |&gt; \n  as_raw_html()\n\n\n\n\nTable 5: Preview of Preprocessed data\n\n\n\n\n  \n  \n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nnumeric\nfixed_acidity\n0\n1\n1.917645176\n0.12159317\n1.3350011\n1.840549633\n1.916922612\n1.987874348\n2.65324196\n▁▃▇▁▁\n\n\nnumeric\nvolatile_acidity\n0\n1\n-1.333889842\n0.33357665\n-2.5257286\n-1.560647748\n-1.347073648\n-1.139434283\n0.09531018\n▁▅▇▂▁\n\n\nnumeric\ncitric_acid\n0\n1\n-Inf\nNaN\n-Inf\n-1.309333320\n-1.139434283\n-0.941608540\n0.50681760\n▁▁▁▇▁\n\n\nnumeric\nresidual_sugar\n0\n1\n1.481001154\n0.92310358\n-0.5108256\n0.530628251\n1.648658626\n2.292534757\n4.18661984\n▅▆▇▆▁\n\n\nnumeric\nchlorides\n0\n1\n-3.148717269\n0.33152116\n-4.7105307\n-3.324236341\n-3.146555163\n-2.995732274\n-1.06131650\n▁▅▇▁▁\n\n\nnumeric\nfree_sulfur_dioxide\n0\n1\n3.437736555\n0.54690744\n0.6931472\n3.135494216\n3.526360525\n3.828641396\n5.66642669\n▁▁▇▆▁\n\n\nnumeric\ntotal_sulfur_dioxide\n0\n1\n4.880002528\n0.33468233\n2.1972246\n4.691347882\n4.897839800\n5.122471438\n6.08677473\n▁▁▁▇▁\n\n\nnumeric\ndensity\n0\n1\n-0.005988129\n0.00302027\n-0.0129738\n-0.008304386\n-0.006289739\n-0.003907625\n0.03823946\n▇▂▁▁▁\n\n\nnumeric\np_h\n0\n1\n1.157981867\n0.04671982\n1.0006319\n1.124929597\n1.156881197\n1.187843422\n1.34025042\n▁▅▇▂▁\n\n\nnumeric\nsulphates\n0\n1\n-0.739139995\n0.22328883\n-1.5141277\n-0.891598119\n-0.755022584\n-0.597837001\n0.07696104\n▁▃▇▃▁\n\n\nnumeric\nalcohol\n0\n1\n2.345977687\n0.11535902\n2.0794415\n2.251291799\n2.341805806\n2.433613355\n2.65324196\n▂▇▆▅▁\n\n\nnumeric\nquality\n0\n1\n5.882848392\n0.87445631\n3.0000000\n5.000000000\n6.000000000\n6.000000000\n9.00000000\n▁▆▇▃▁\n\n\n\n\n\n\n\n\n\n\nBased on Table 5, the citric_acid feature contains NaN and Inf values. These values can cause issues for machine learning models like KNN. To ensure our models can process the data effectively, we need to rewrite the preprocessing to resolve the issue by adding 1 to values of citric_acid.\n\n\nShow the code\nlog_sc_rec2 &lt;- formula_rec |&gt; \n  step_mutate(citric_acid = citric_acid + 1) |&gt; \n  step_log(all_predictors())\n\nlog_sc_rec2 |&gt; \n  prep() |&gt; \n  juice() |&gt; \n  skimr::skim() |&gt; \n  gt() |&gt; \n  as_raw_html()\n\n\n\n\nTable 6: Improved preprocessing\n\n\n\n\n  \n  \n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nnumeric\nfixed_acidity\n0\n1\n1.917645176\n0.12159317\n1.3350011\n1.840549633\n1.916922612\n1.987874348\n2.65324196\n▁▃▇▁▁\n\n\nnumeric\nvolatile_acidity\n0\n1\n-1.333889842\n0.33357665\n-2.5257286\n-1.560647748\n-1.347073648\n-1.139434283\n0.09531018\n▁▅▇▂▁\n\n\nnumeric\ncitric_acid\n0\n1\n0.285212528\n0.08850576\n0.0000000\n0.239016900\n0.277631737\n0.329303747\n0.97832612\n▁▇▁▁▁\n\n\nnumeric\nresidual_sugar\n0\n1\n1.481001154\n0.92310358\n-0.5108256\n0.530628251\n1.648658626\n2.292534757\n4.18661984\n▅▆▇▆▁\n\n\nnumeric\nchlorides\n0\n1\n-3.148717269\n0.33152116\n-4.7105307\n-3.324236341\n-3.146555163\n-2.995732274\n-1.06131650\n▁▅▇▁▁\n\n\nnumeric\nfree_sulfur_dioxide\n0\n1\n3.437736555\n0.54690744\n0.6931472\n3.135494216\n3.526360525\n3.828641396\n5.66642669\n▁▁▇▆▁\n\n\nnumeric\ntotal_sulfur_dioxide\n0\n1\n4.880002528\n0.33468233\n2.1972246\n4.691347882\n4.897839800\n5.122471438\n6.08677473\n▁▁▁▇▁\n\n\nnumeric\ndensity\n0\n1\n-0.005988129\n0.00302027\n-0.0129738\n-0.008304386\n-0.006289739\n-0.003907625\n0.03823946\n▇▂▁▁▁\n\n\nnumeric\np_h\n0\n1\n1.157981867\n0.04671982\n1.0006319\n1.124929597\n1.156881197\n1.187843422\n1.34025042\n▁▅▇▂▁\n\n\nnumeric\nsulphates\n0\n1\n-0.739139995\n0.22328883\n-1.5141277\n-0.891598119\n-0.755022584\n-0.597837001\n0.07696104\n▁▃▇▃▁\n\n\nnumeric\nalcohol\n0\n1\n2.345977687\n0.11535902\n2.0794415\n2.251291799\n2.341805806\n2.433613355\n2.65324196\n▂▇▆▅▁\n\n\nnumeric\nquality\n0\n1\n5.882848392\n0.87445631\n3.0000000\n5.000000000\n6.000000000\n6.000000000\n9.00000000\n▁▆▇▃▁\n\n\n\n\n\n\n\n\n\n\nOur efforts in rewriting the preprocessing step have paid off! As shown in Table 6, the citric_acid feature is now free of NA, NAN, and Inf. We can now proceed with the model training process using the preprocessed data from Table 6."
  },
  {
    "objectID": "posts/wine-qual/index.html#workflow",
    "href": "posts/wine-qual/index.html#workflow",
    "title": "The Algorithmic Grape",
    "section": "Workflow",
    "text": "Workflow\nNow that we’ve chosen our feature engineering techniques and defined the models we’ll be using, it’s time to bring them together. To capture this entire workflow, we’ll create a workflow object\nThink of this workflow object like a recipe book. It will hold all the ingredients (the recipe objects specifying the feature engineering steps) and the instructions for each dish (the model specifications). This allows us to easily apply both recipe options (feature engineering approaches) to each model we’re testing.\nThe workflow_set() function will be our handy tool for creating this comprehensive recipe book. It ensures we can seamlessly combine different feature engineering choices with various models, allowing for a more thorough analysis.\nThis approach allows us to explore different combinations and see which recipe (feature engineering) works best for each model in predicting white wine quality.\n\n\nShow the code\nwf_object &lt;- workflow_set(\n  preproc = list(\n    no_preproc = formula_rec,\n    log_zv = log_sc_rec2\n  ),\n  model = list(\n    glm = glm_spec,\n    dec_tree = dec_tree_spec,\n    rf = rand_for_spec,\n    knn = knn_spec\n  ),\n  cross = TRUE\n)"
  },
  {
    "objectID": "posts/wine-qual/index.html#finding-the-perfect-settings-parameter-tuning",
    "href": "posts/wine-qual/index.html#finding-the-perfect-settings-parameter-tuning",
    "title": "The Algorithmic Grape",
    "section": "Finding the Perfect Settings: Parameter Tuning",
    "text": "Finding the Perfect Settings: Parameter Tuning\nWe’ve chosen our models and put together our workflow, but there’s one more step before we can unleash them on the wine data. Each model has its own set of “knobs” we can adjust to fine-tune its performance. These knobs are called “hyperparameters,” and finding the right settings for them can significantly impact how well the model predicts wine quality.\nImagine you’re baking a cake. You have a recipe to follow (the model in this case), but you can adjust things like the oven temperature or the baking time (the hyperparameters) to get the perfect result. Here, we’ll use a technique called “grid search” to explore different combinations of these hyperparameters for each model.\nTo make this process faster, we’ll use a package called doParallel. This helps us leverage multiple processing cores on your computer, essentially dividing the work and speeding things up.\nThe workflow_map() function comes in handy again. It allows us to apply the grid search technique to all the models we defined in our workflow. We’ll also set verbose=TRUE during this step, which means the computer will show us the progress (although we won’t include those details in this explanation).\nBy exploring different hyperparameter combinations, we aim to find the settings that make each model perform at its best when predicting white wine quality.\n\n\nShow the code\ndoParallel::registerDoParallel(cores = 8)\n\n\nwf_tune_fit &lt;- workflow_map(\n  wf_object,\n  fn = \"tune_grid\",\n  grid = 10,\n  resamples = wine_folds,\n  seed = 11,\n  verbose = TRUE\n)\n\ndoParallel::stopImplicitCluster()"
  },
  {
    "objectID": "posts/wine-qual/index.html#tune-result",
    "href": "posts/wine-qual/index.html#tune-result",
    "title": "The Algorithmic Grape",
    "section": "Tune Result",
    "text": "Tune Result\n\n\nShow the code\nautoplot(wf_tune_fit) +\n  theme(\n    legend.position = \"bottom\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 5: Tune result\n\n\n\n\n\nFigure 5 provides a visual breakdown of how the different models performed in predicting white wine quality. We used two key metrics to evaluate their success: RMSE and R-Squared.\nThe clear winner based on these metrics is the Random Forest Model! It achieved the lowest RMSE and highest R-Squared score, indicating both high accuracy (low error) and a strong correlation between its predictions and the actual wine quality score.\nFollowing closely behind is the KNN (K-Nearest Neighbors) model. While it performs well, there’s a noticeable gap between its performance and Random Forest’s. This suggests that Random Forest might be a more robust choice for predicting wine quality in this particular dataset."
  },
  {
    "objectID": "posts/wine-qual/index.html#examining-specific-model-results",
    "href": "posts/wine-qual/index.html#examining-specific-model-results",
    "title": "The Algorithmic Grape",
    "section": "Examining Specific Model Results",
    "text": "Examining Specific Model Results\nAs Figure 5 revealed, the Random Forest model is the top performer. To gain a better understanding, let’s take a closer look at the tuning process result.\n\n\nShow the code\ntop_model &lt;- rank_results(wf_tune_fit, rank_metric = \"rmse\", select_best = TRUE)\n\ntop_model |&gt; \n  select(-c(preprocessor, n)) |&gt; \n  mutate(\n    .config = str_remove_all(.config, \"Preprocessor1_Model\"),\n    wflow_id = case_when(\n      str_detect(wflow_id, \"no_preproc\") ~ \"No Preprocessing\",\n      .default = \"Log + scale transformation\"\n    ),\n    model = str_replace_all(model, \"_\", \" \"),\n    model = str_to_title(model)\n  ) |&gt; \n  pivot_wider(\n    id_cols = c(wflow_id, model, .config, rank),\n    names_from = .metric,\n    values_from = mean\n  ) |&gt; \n  gt() |&gt; \n  cols_label(\n    wflow_id = \"Recipe\",\n    .config = \"Resample Number\",\n    model = \"Model Type\",\n    rank = \"Rank\",\n    rmse = \"RMSE\",\n    rsq = \"R-Squared\"\n  ) |&gt; \n  tab_spanner(\n    columns = rmse:rsq,\n    label = \"Evaluation Metric\"\n  ) |&gt; \n  tab_header(title = \"Top Performing Models\") |&gt; \n  opt_stylize(\n    style = 3,\n    color = \"blue\",\n    add_row_striping = TRUE\n  ) |&gt; \n  as_raw_html()\n\n\n\n\nTable 7: Ranks of the model, random forest without preprocessing is ranked highest\n\n\n\n\n  \n  \n\n\n\nTop Performing Models\n\n\nRecipe\nModel Type\nResample Number\nRank\nEvaluation Metric\n\n\nRMSE\nR-Squared\n\n\n\n\nNo Preprocessing\nRand Forest\n08\n1\n0.6038689\n0.5265060\n\n\nLog + scale transformation\nRand Forest\n08\n2\n0.6039895\n0.5262881\n\n\nNo Preprocessing\nNearest Neighbor\n05\n3\n0.6119481\n0.5115739\n\n\nLog + scale transformation\nNearest Neighbor\n05\n4\n0.6126732\n0.5109407\n\n\nNo Preprocessing\nDecision Tree\n04\n5\n0.7286573\n0.3182304\n\n\nLog + scale transformation\nDecision Tree\n04\n6\n0.7286573\n0.3182304\n\n\nLog + scale transformation\nLinear Reg\n1\n7\n0.7372858\n0.2896381\n\n\nNo Preprocessing\nLinear Reg\n1\n8\n0.7431961\n0.2783234\n\n\n\n\n\n\n\n\n\n\nTable 7 offers a comprehensive overview of each model’s performance, providing valuable insights beyond the visual representation in Figure 5. Here’s what we can learn from this table:\n\nModel Performance: It clearly shows the best performing model based on the chosen evaluation metrics (e.g., RMSE, R-squared). This allows for a quick comparison and identification of the champion model.\nPreprocessing Status: The table indicates whether each model was trained on data that underwent preprocessing (like feature scaling) or not. This information helps us understand if preprocessing played a role in the model’s performance.\nResample Selection: It specifies the resample object used to select the best performing model version during the tuning process. This provides context about the training and validation data used for each model.\n\nPreprocessing does not improve the performance of all models except the Generalized Linear Model which showed slight improvement.\nNext we get the workflow object and finalize the model"
  }
]